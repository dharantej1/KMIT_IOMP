{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./water_potability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890456</td>\n",
       "      <td>20791.31898</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.05786</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.54173</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.41744</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436525</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.98634</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ph    Hardness       Solids  Chloramines     Sulfate  Conductivity  \\\n",
       "0       NaN  204.890456  20791.31898     7.300212  368.516441    564.308654   \n",
       "1  3.716080  129.422921  18630.05786     6.635246         NaN    592.885359   \n",
       "2  8.099124  224.236259  19909.54173     9.275884         NaN    418.606213   \n",
       "3  8.316766  214.373394  22018.41744     8.059332  356.886136    363.266516   \n",
       "4  9.092223  181.101509  17978.98634     6.546600  310.135738    398.410813   \n",
       "\n",
       "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       10.379783        86.990970   2.963135           0  \n",
       "1       15.180013        56.329076   4.500656           0  \n",
       "2       16.868637        66.420093   3.055934           0  \n",
       "3       18.436525       100.341674   4.628771           0  \n",
       "4       11.558279        31.997993   4.075075           0  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104832, 10)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in different columns\n",
      "ph                 15712\n",
      "Hardness               0\n",
      "Solids                 0\n",
      "Chloramines            0\n",
      "Sulfate            24992\n",
      "Conductivity           0\n",
      "Organic_carbon         0\n",
      "Trihalomethanes     5184\n",
      "Turbidity              0\n",
      "Potability             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null values in different columns\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df['ph'].isna() == True), 'ph'] = df['ph'].mean()\n",
    "# df.loc[(df['Sulfate'].isna() == True), 'Sulfate'] = df['Sulfate'].mean()\n",
    "# df.loc[(df['Trihalomethanes'].isna() == True), 'Trihalomethanes'] = df['Trihalomethanes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64352, 10)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values after dropping null values\n",
      "ph                 0\n",
      "Hardness           0\n",
      "Solids             0\n",
      "Chloramines        0\n",
      "Sulfate            0\n",
      "Conductivity       0\n",
      "Organic_carbon     0\n",
      "Trihalomethanes    0\n",
      "Turbidity          0\n",
      "Potability         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null values after dropping null values\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "      <td>64352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.085990</td>\n",
       "      <td>195.968072</td>\n",
       "      <td>21917.441375</td>\n",
       "      <td>7.134338</td>\n",
       "      <td>333.224672</td>\n",
       "      <td>426.526409</td>\n",
       "      <td>14.357709</td>\n",
       "      <td>66.400859</td>\n",
       "      <td>3.969729</td>\n",
       "      <td>0.403282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.572958</td>\n",
       "      <td>32.627223</td>\n",
       "      <td>8640.157938</td>\n",
       "      <td>1.584438</td>\n",
       "      <td>41.195246</td>\n",
       "      <td>80.693129</td>\n",
       "      <td>3.324158</td>\n",
       "      <td>16.073237</td>\n",
       "      <td>0.780158</td>\n",
       "      <td>0.490560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.227499</td>\n",
       "      <td>73.492234</td>\n",
       "      <td>320.942611</td>\n",
       "      <td>1.390871</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>201.619737</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>8.577013</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.089431</td>\n",
       "      <td>176.736376</td>\n",
       "      <td>15613.160530</td>\n",
       "      <td>6.137757</td>\n",
       "      <td>307.621462</td>\n",
       "      <td>366.558131</td>\n",
       "      <td>12.120956</td>\n",
       "      <td>55.947322</td>\n",
       "      <td>3.442848</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.027297</td>\n",
       "      <td>197.191839</td>\n",
       "      <td>20933.512750</td>\n",
       "      <td>7.143907</td>\n",
       "      <td>332.232177</td>\n",
       "      <td>423.455906</td>\n",
       "      <td>14.322019</td>\n",
       "      <td>66.542198</td>\n",
       "      <td>3.968177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.053044</td>\n",
       "      <td>216.454108</td>\n",
       "      <td>27192.280560</td>\n",
       "      <td>8.110140</td>\n",
       "      <td>359.392567</td>\n",
       "      <td>482.451933</td>\n",
       "      <td>16.684074</td>\n",
       "      <td>77.297300</td>\n",
       "      <td>4.515150</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>317.338124</td>\n",
       "      <td>56488.672410</td>\n",
       "      <td>13.127000</td>\n",
       "      <td>481.030642</td>\n",
       "      <td>753.342620</td>\n",
       "      <td>27.006707</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>6.494749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ph      Hardness        Solids   Chloramines       Sulfate  \\\n",
       "count  64352.000000  64352.000000  64352.000000  64352.000000  64352.000000   \n",
       "mean       7.085990    195.968072  21917.441375      7.134338    333.224672   \n",
       "std        1.572958     32.627223   8640.157938      1.584438     41.195246   \n",
       "min        0.227499     73.492234    320.942611      1.390871    129.000000   \n",
       "25%        6.089431    176.736376  15613.160530      6.137757    307.621462   \n",
       "50%        7.027297    197.191839  20933.512750      7.143907    332.232177   \n",
       "75%        8.053044    216.454108  27192.280560      8.110140    359.392567   \n",
       "max       14.000000    317.338124  56488.672410     13.127000    481.030642   \n",
       "\n",
       "       Conductivity  Organic_carbon  Trihalomethanes     Turbidity  \\\n",
       "count  64352.000000    64352.000000     64352.000000  64352.000000   \n",
       "mean     426.526409       14.357709        66.400859      3.969729   \n",
       "std       80.693129        3.324158        16.073237      0.780158   \n",
       "min      201.619737        2.200000         8.577013      1.450000   \n",
       "25%      366.558131       12.120956        55.947322      3.442848   \n",
       "50%      423.455906       14.322019        66.542198      3.968177   \n",
       "75%      482.451933       16.684074        77.297300      4.515150   \n",
       "max      753.342620       27.006707       124.000000      6.494749   \n",
       "\n",
       "         Potability  \n",
       "count  64352.000000  \n",
       "mean       0.403282  \n",
       "std        0.490560  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df=df.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potability</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "      <td>25952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ph  Hardness  Solids  Chloramines  Sulfate  Conductivity  \\\n",
       "Potability                                                                \n",
       "0           38400     38400   38400        38400    38400         38400   \n",
       "1           25952     25952   25952        25952    25952         25952   \n",
       "\n",
       "            Organic_carbon  Trihalomethanes  Turbidity  \n",
       "Potability                                              \n",
       "0                    38400            38400      38400  \n",
       "1                    25952            25952      25952  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Potability\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Potability']\n",
    "X = df.drop(['Potability'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Training Set: 57916\n",
      "Size Testing Set: 6436\n"
     ]
    }
   ],
   "source": [
    "print('Size Training Set: {}'.format(len(X_train)))\n",
    "print('Size Testing Set: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",mode = 'auto',patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# model.add(layers.Dense(20,input_dim=9, activation='relu'))\n",
    "model.add(layers.Dense(20,input_dim=9, activation='elu'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2464 - acc: 0.5932 - val_loss: 0.2386 - val_acc: 0.6019\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2406 - acc: 0.5967 - val_loss: 0.2396 - val_acc: 0.6019\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2403 - acc: 0.5994 - val_loss: 0.2393 - val_acc: 0.6019\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2404 - acc: 0.6000 - val_loss: 0.2387 - val_acc: 0.6074\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2396 - acc: 0.6016 - val_loss: 0.2380 - val_acc: 0.6044\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2394 - acc: 0.6031 - val_loss: 0.2364 - val_acc: 0.6094\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2387 - acc: 0.6036 - val_loss: 0.2363 - val_acc: 0.6038\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2366 - acc: 0.6074 - val_loss: 0.2350 - val_acc: 0.6103\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2332 - acc: 0.6158 - val_loss: 0.2278 - val_acc: 0.6496\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2294 - acc: 0.6244 - val_loss: 0.2255 - val_acc: 0.6392\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2264 - acc: 0.6311 - val_loss: 0.2202 - val_acc: 0.6541\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2230 - acc: 0.6413 - val_loss: 0.2198 - val_acc: 0.6530\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2221 - acc: 0.6439 - val_loss: 0.2168 - val_acc: 0.6537\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2202 - acc: 0.6514 - val_loss: 0.2179 - val_acc: 0.6695\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2186 - acc: 0.6559 - val_loss: 0.2154 - val_acc: 0.6586\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2166 - acc: 0.6596 - val_loss: 0.2111 - val_acc: 0.6731\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2162 - acc: 0.6595 - val_loss: 0.2103 - val_acc: 0.6824\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2150 - acc: 0.6632 - val_loss: 0.2096 - val_acc: 0.6847\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2132 - acc: 0.6709 - val_loss: 0.2129 - val_acc: 0.6756\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2121 - acc: 0.6723 - val_loss: 0.2094 - val_acc: 0.6823\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2134 - acc: 0.6683 - val_loss: 0.2115 - val_acc: 0.6791\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2106 - acc: 0.6756 - val_loss: 0.2063 - val_acc: 0.6838\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2110 - acc: 0.6735 - val_loss: 0.2091 - val_acc: 0.6812\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2100 - acc: 0.6782 - val_loss: 0.2190 - val_acc: 0.6473\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2107 - acc: 0.6763 - val_loss: 0.2041 - val_acc: 0.6917\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2091 - acc: 0.6807 - val_loss: 0.2096 - val_acc: 0.6787\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2092 - acc: 0.6788 - val_loss: 0.2029 - val_acc: 0.6900\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2083 - acc: 0.6809 - val_loss: 0.2053 - val_acc: 0.6936\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2086 - acc: 0.6811 - val_loss: 0.2046 - val_acc: 0.6917\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2085 - acc: 0.6825 - val_loss: 0.2038 - val_acc: 0.6902\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2078 - acc: 0.6831 - val_loss: 0.2038 - val_acc: 0.6933\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2076 - acc: 0.6837 - val_loss: 0.2063 - val_acc: 0.6931\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2070 - acc: 0.6843 - val_loss: 0.2024 - val_acc: 0.6917\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2082 - acc: 0.6810 - val_loss: 0.2031 - val_acc: 0.6941\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2074 - acc: 0.6820 - val_loss: 0.2035 - val_acc: 0.7020\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2064 - acc: 0.6886 - val_loss: 0.2119 - val_acc: 0.6722\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2062 - acc: 0.6869 - val_loss: 0.2027 - val_acc: 0.6970\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2060 - acc: 0.6874 - val_loss: 0.2044 - val_acc: 0.6919\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6882 - val_loss: 0.2029 - val_acc: 0.6950\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2070 - acc: 0.6861 - val_loss: 0.2012 - val_acc: 0.7014\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2064 - acc: 0.6892 - val_loss: 0.2010 - val_acc: 0.7108\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2057 - acc: 0.6888 - val_loss: 0.2049 - val_acc: 0.6888\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2061 - acc: 0.6884 - val_loss: 0.2015 - val_acc: 0.7043\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2050 - acc: 0.6915 - val_loss: 0.2012 - val_acc: 0.6998\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2062 - acc: 0.6873 - val_loss: 0.2040 - val_acc: 0.6899\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2058 - acc: 0.6896 - val_loss: 0.2047 - val_acc: 0.6955\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2052 - acc: 0.6907 - val_loss: 0.2037 - val_acc: 0.6939\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2056 - acc: 0.6901 - val_loss: 0.2147 - val_acc: 0.6711\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2063 - acc: 0.6873 - val_loss: 0.2051 - val_acc: 0.6847\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2057 - acc: 0.6900 - val_loss: 0.2015 - val_acc: 0.7017\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.6890 - val_loss: 0.2259 - val_acc: 0.6551\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2053 - acc: 0.6884 - val_loss: 0.2012 - val_acc: 0.7020\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2051 - acc: 0.6890 - val_loss: 0.2019 - val_acc: 0.6966\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2046 - acc: 0.6914 - val_loss: 0.2035 - val_acc: 0.6976\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2050 - acc: 0.6892 - val_loss: 0.2182 - val_acc: 0.6569\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2049 - acc: 0.6895 - val_loss: 0.2010 - val_acc: 0.6959\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n",
    "# history = model.fit(X_train, y_train, batch_size=75, epochs=300, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 40us/sample - loss: 0.2389 - acc: 0.6005 - val_loss: 0.2274 - val_acc: 0.6231\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2196 - acc: 0.6524 - val_loss: 0.2059 - val_acc: 0.6894\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2080 - acc: 0.6807 - val_loss: 0.2048 - val_acc: 0.6849\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2047 - acc: 0.6891 - val_loss: 0.2054 - val_acc: 0.6902\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2028 - acc: 0.6911 - val_loss: 0.1990 - val_acc: 0.7025\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2012 - acc: 0.6957 - val_loss: 0.1990 - val_acc: 0.6948\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1993 - acc: 0.6964 - val_loss: 0.1965 - val_acc: 0.7062\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 3s 53us/sample - loss: 0.1987 - acc: 0.6996 - val_loss: 0.1948 - val_acc: 0.7104\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 2s 40us/sample - loss: 0.1969 - acc: 0.7027 - val_loss: 0.1929 - val_acc: 0.7138\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 2s 38us/sample - loss: 0.1959 - acc: 0.7044 - val_loss: 0.1922 - val_acc: 0.7188\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1953 - acc: 0.7055 - val_loss: 0.1932 - val_acc: 0.7037\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1947 - acc: 0.7052 - val_loss: 0.1999 - val_acc: 0.6914\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 2s 39us/sample - loss: 0.1946 - acc: 0.7062 - val_loss: 0.1915 - val_acc: 0.7247\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1934 - acc: 0.7073 - val_loss: 0.1921 - val_acc: 0.7045\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1930 - acc: 0.7092 - val_loss: 0.1970 - val_acc: 0.7073\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1931 - acc: 0.7100 - val_loss: 0.1896 - val_acc: 0.7175\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1919 - acc: 0.7112 - val_loss: 0.1940 - val_acc: 0.7154\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1922 - acc: 0.7098 - val_loss: 0.1892 - val_acc: 0.7200\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1913 - acc: 0.7123 - val_loss: 0.1909 - val_acc: 0.7129\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1915 - acc: 0.7112 - val_loss: 0.1919 - val_acc: 0.7116\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.1914 - acc: 0.7119 - val_loss: 0.1990 - val_acc: 0.6953\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1907 - acc: 0.7123 - val_loss: 0.1930 - val_acc: 0.7105\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 2s 41us/sample - loss: 0.1916 - acc: 0.7097 - val_loss: 0.1890 - val_acc: 0.7164\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1909 - acc: 0.7121 - val_loss: 0.2002 - val_acc: 0.6945\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 2s 38us/sample - loss: 0.1907 - acc: 0.7110 - val_loss: 0.1902 - val_acc: 0.7220\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1904 - acc: 0.7129 - val_loss: 0.1902 - val_acc: 0.7185\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1899 - acc: 0.7142 - val_loss: 0.1885 - val_acc: 0.7174\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1899 - acc: 0.7137 - val_loss: 0.1877 - val_acc: 0.7244\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1894 - acc: 0.7151 - val_loss: 0.1876 - val_acc: 0.7248\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1894 - acc: 0.7147 - val_loss: 0.1928 - val_acc: 0.7172\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1896 - acc: 0.7145 - val_loss: 0.1872 - val_acc: 0.7185\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 2s 38us/sample - loss: 0.1896 - acc: 0.7148 - val_loss: 0.1890 - val_acc: 0.7177\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1891 - acc: 0.7156 - val_loss: 0.1871 - val_acc: 0.7233\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1891 - acc: 0.7156 - val_loss: 0.1877 - val_acc: 0.7234\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1890 - acc: 0.7147 - val_loss: 0.1930 - val_acc: 0.7143\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 3s 46us/sample - loss: 0.1891 - acc: 0.7159 - val_loss: 0.1864 - val_acc: 0.7214\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1894 - acc: 0.7170 - val_loss: 0.1875 - val_acc: 0.7273\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1891 - acc: 0.7161 - val_loss: 0.1880 - val_acc: 0.7279\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 2s 42us/sample - loss: 0.1887 - acc: 0.7156 - val_loss: 0.1869 - val_acc: 0.7233\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1883 - acc: 0.7165 - val_loss: 0.1869 - val_acc: 0.7250\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1885 - acc: 0.7175 - val_loss: 0.1868 - val_acc: 0.7259\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1884 - acc: 0.7163 - val_loss: 0.1956 - val_acc: 0.7113\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1885 - acc: 0.7166 - val_loss: 0.1893 - val_acc: 0.7239\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1886 - acc: 0.7175 - val_loss: 0.1867 - val_acc: 0.7236\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1884 - acc: 0.7168 - val_loss: 0.1860 - val_acc: 0.7268\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1882 - acc: 0.7164 - val_loss: 0.1882 - val_acc: 0.7112\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1885 - acc: 0.7166 - val_loss: 0.1861 - val_acc: 0.7265\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1883 - acc: 0.7161 - val_loss: 0.1875 - val_acc: 0.7197\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1884 - acc: 0.7158 - val_loss: 0.1866 - val_acc: 0.7279\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1880 - acc: 0.7173 - val_loss: 0.1882 - val_acc: 0.7208\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 2s 43us/sample - loss: 0.1886 - acc: 0.7163 - val_loss: 0.1871 - val_acc: 0.7258\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1879 - acc: 0.7197 - val_loss: 0.1895 - val_acc: 0.7164\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1882 - acc: 0.7182 - val_loss: 0.1870 - val_acc: 0.7206\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1883 - acc: 0.7169 - val_loss: 0.1862 - val_acc: 0.7186\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1878 - acc: 0.7182 - val_loss: 0.1871 - val_acc: 0.7191\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1883 - acc: 0.7176 - val_loss: 0.1892 - val_acc: 0.7241\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1879 - acc: 0.7183 - val_loss: 0.1879 - val_acc: 0.7203\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1878 - acc: 0.7188 - val_loss: 0.1899 - val_acc: 0.7132\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1878 - acc: 0.7171 - val_loss: 0.1866 - val_acc: 0.7318\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 4s 64us/sample - loss: 0.1877 - acc: 0.7180 - val_loss: 0.1865 - val_acc: 0.7222\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='relu'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 4s 67us/sample - loss: 0.2735 - acc: 0.5910 - val_loss: 0.2427 - val_acc: 0.6041\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 3s 43us/sample - loss: 0.2424 - acc: 0.5929 - val_loss: 0.2492 - val_acc: 0.5333\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 2s 43us/sample - loss: 0.2417 - acc: 0.5960 - val_loss: 0.2402 - val_acc: 0.6019\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 3s 50us/sample - loss: 0.2413 - acc: 0.5957 - val_loss: 0.2394 - val_acc: 0.6019\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 3s 59us/sample - loss: 0.2409 - acc: 0.5970 - val_loss: 0.2395 - val_acc: 0.6024\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 3s 49us/sample - loss: 0.2408 - acc: 0.5963 - val_loss: 0.2391 - val_acc: 0.6019\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 3s 44us/sample - loss: 0.2408 - acc: 0.5969 - val_loss: 0.2390 - val_acc: 0.6019\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2406 - acc: 0.5964 - val_loss: 0.2396 - val_acc: 0.6030\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 3s 48us/sample - loss: 0.2409 - acc: 0.5968 - val_loss: 0.2400 - val_acc: 0.6019\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2405 - acc: 0.5966 - val_loss: 0.2399 - val_acc: 0.6022\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 2s 43us/sample - loss: 0.2405 - acc: 0.5967 - val_loss: 0.2422 - val_acc: 0.6094\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 3s 47us/sample - loss: 0.2407 - acc: 0.5964 - val_loss: 0.2393 - val_acc: 0.6019\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 3s 44us/sample - loss: 0.2405 - acc: 0.5965 - val_loss: 0.2390 - val_acc: 0.6019\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 3s 44us/sample - loss: 0.2405 - acc: 0.5967 - val_loss: 0.2390 - val_acc: 0.6019\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2403 - acc: 0.5967 - val_loss: 0.2392 - val_acc: 0.6024\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 3s 47us/sample - loss: 0.2403 - acc: 0.5968 - val_loss: 0.2388 - val_acc: 0.6024\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2401 - acc: 0.5971 - val_loss: 0.2388 - val_acc: 0.6019\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 3s 59us/sample - loss: 0.2402 - acc: 0.5971 - val_loss: 0.2393 - val_acc: 0.6019\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 2s 41us/sample - loss: 0.2401 - acc: 0.5968 - val_loss: 0.2408 - val_acc: 0.6109\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2403 - acc: 0.5974 - val_loss: 0.2395 - val_acc: 0.6035\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2402 - acc: 0.5972 - val_loss: 0.2388 - val_acc: 0.6019\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2402 - acc: 0.5975 - val_loss: 0.2389 - val_acc: 0.6024\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2401 - acc: 0.5975 - val_loss: 0.2390 - val_acc: 0.6019\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2401 - acc: 0.5979 - val_loss: 0.2387 - val_acc: 0.6019\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2402 - acc: 0.5978 - val_loss: 0.2391 - val_acc: 0.6032\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2401 - acc: 0.5973 - val_loss: 0.2390 - val_acc: 0.6044\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.5985 - val_loss: 0.2385 - val_acc: 0.6033\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2399 - acc: 0.6000 - val_loss: 0.2385 - val_acc: 0.6024\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2399 - acc: 0.5995 - val_loss: 0.2387 - val_acc: 0.6086\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2399 - acc: 0.5999 - val_loss: 0.2384 - val_acc: 0.6055\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2397 - acc: 0.5998 - val_loss: 0.2382 - val_acc: 0.6108\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2395 - acc: 0.6033 - val_loss: 0.2381 - val_acc: 0.6071\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2396 - acc: 0.6044 - val_loss: 0.2381 - val_acc: 0.6095\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2392 - acc: 0.6066 - val_loss: 0.2393 - val_acc: 0.6189\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2391 - acc: 0.6095 - val_loss: 0.2375 - val_acc: 0.6114\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2388 - acc: 0.6109 - val_loss: 0.2375 - val_acc: 0.6187\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2382 - acc: 0.6153 - val_loss: 0.2357 - val_acc: 0.6187\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2366 - acc: 0.6184 - val_loss: 0.2381 - val_acc: 0.6050\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2314 - acc: 0.6280 - val_loss: 0.2246 - val_acc: 0.6311\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2258 - acc: 0.6410 - val_loss: 0.2200 - val_acc: 0.6457\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2233 - acc: 0.6471 - val_loss: 0.2183 - val_acc: 0.6613\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2225 - acc: 0.6515 - val_loss: 0.2319 - val_acc: 0.6377\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2209 - acc: 0.6520 - val_loss: 0.2157 - val_acc: 0.6613\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2206 - acc: 0.6573 - val_loss: 0.2153 - val_acc: 0.6680\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2190 - acc: 0.6586 - val_loss: 0.2296 - val_acc: 0.6405\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2195 - acc: 0.6587 - val_loss: 0.2155 - val_acc: 0.6678\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2177 - acc: 0.6624 - val_loss: 0.2143 - val_acc: 0.6748\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2176 - acc: 0.6644 - val_loss: 0.2123 - val_acc: 0.6666\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2167 - acc: 0.6653 - val_loss: 0.2202 - val_acc: 0.6892\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2167 - acc: 0.6649 - val_loss: 0.2116 - val_acc: 0.6652\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2160 - acc: 0.6665 - val_loss: 0.2110 - val_acc: 0.6731\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2154 - acc: 0.6696 - val_loss: 0.2221 - val_acc: 0.6819\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2143 - acc: 0.6712 - val_loss: 0.2110 - val_acc: 0.6868\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2143 - acc: 0.6711 - val_loss: 0.2228 - val_acc: 0.6809\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2136 - acc: 0.6736 - val_loss: 0.2090 - val_acc: 0.6824\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2139 - acc: 0.6715 - val_loss: 0.2119 - val_acc: 0.6625\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2132 - acc: 0.6735 - val_loss: 0.2156 - val_acc: 0.6930\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2126 - acc: 0.6753 - val_loss: 0.2103 - val_acc: 0.6687\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2130 - acc: 0.6740 - val_loss: 0.2068 - val_acc: 0.6810\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2121 - acc: 0.6748 - val_loss: 0.2064 - val_acc: 0.6854\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2111 - acc: 0.6793 - val_loss: 0.2129 - val_acc: 0.6616\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2117 - acc: 0.6762 - val_loss: 0.2061 - val_acc: 0.6823\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2102 - acc: 0.6780 - val_loss: 0.2072 - val_acc: 0.6966\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2103 - acc: 0.6798 - val_loss: 0.2095 - val_acc: 0.6669\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2095 - acc: 0.6815 - val_loss: 0.2300 - val_acc: 0.6125\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2106 - acc: 0.6778 - val_loss: 0.2052 - val_acc: 0.6843\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2102 - acc: 0.6809 - val_loss: 0.2089 - val_acc: 0.7031\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2100 - acc: 0.6815 - val_loss: 0.2050 - val_acc: 0.7003\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2088 - acc: 0.6829 - val_loss: 0.2089 - val_acc: 0.6736\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2096 - acc: 0.6787 - val_loss: 0.2053 - val_acc: 0.7009\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2086 - acc: 0.6850 - val_loss: 0.2266 - val_acc: 0.6360\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2091 - acc: 0.6814 - val_loss: 0.2187 - val_acc: 0.6742\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2082 - acc: 0.6836 - val_loss: 0.2058 - val_acc: 0.6998\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2084 - acc: 0.6842 - val_loss: 0.2083 - val_acc: 0.6712\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2090 - acc: 0.6814 - val_loss: 0.2102 - val_acc: 0.6889\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2089 - acc: 0.6819 - val_loss: 0.2030 - val_acc: 0.6894\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2084 - acc: 0.6820 - val_loss: 0.2135 - val_acc: 0.6583\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2077 - acc: 0.6854 - val_loss: 0.2034 - val_acc: 0.6871\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2070 - acc: 0.6873 - val_loss: 0.2026 - val_acc: 0.6945\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2068 - acc: 0.6879 - val_loss: 0.2021 - val_acc: 0.6990\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2074 - acc: 0.6838 - val_loss: 0.2118 - val_acc: 0.6919\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2073 - acc: 0.6864 - val_loss: 0.2043 - val_acc: 0.6835\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2072 - acc: 0.685 - 1s 24us/sample - loss: 0.2071 - acc: 0.6855 - val_loss: 0.2042 - val_acc: 0.7018\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2077 - acc: 0.6850 - val_loss: 0.2072 - val_acc: 0.6708\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2066 - acc: 0.6876 - val_loss: 0.2032 - val_acc: 0.7001\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2075 - acc: 0.6851 - val_loss: 0.2052 - val_acc: 0.6955\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2064 - acc: 0.6870 - val_loss: 0.2031 - val_acc: 0.7015\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2065 - acc: 0.6876 - val_loss: 0.2015 - val_acc: 0.6997\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2071 - acc: 0.6865 - val_loss: 0.2294 - val_acc: 0.6125\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2071 - acc: 0.6847 - val_loss: 0.2285 - val_acc: 0.6145\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2059 - acc: 0.6876 - val_loss: 0.2110 - val_acc: 0.6666\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2070 - acc: 0.6861 - val_loss: 0.2062 - val_acc: 0.7026\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2065 - acc: 0.6844 - val_loss: 0.2022 - val_acc: 0.6948\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2064 - acc: 0.6874 - val_loss: 0.2154 - val_acc: 0.6690\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2056 - acc: 0.6889 - val_loss: 0.2030 - val_acc: 0.6857\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2062 - acc: 0.6865 - val_loss: 0.2014 - val_acc: 0.7003\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2058 - acc: 0.6877 - val_loss: 0.2200 - val_acc: 0.6558\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2069 - acc: 0.6857 - val_loss: 0.2105 - val_acc: 0.6658\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2054 - acc: 0.6894 - val_loss: 0.2066 - val_acc: 0.6819\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2055 - acc: 0.6896 - val_loss: 0.2112 - val_acc: 0.6835\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2068 - acc: 0.6869 - val_loss: 0.2159 - val_acc: 0.6596\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2059 - acc: 0.6881 - val_loss: 0.2015 - val_acc: 0.7021\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.6890 - val_loss: 0.2009 - val_acc: 0.6998\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2059 - acc: 0.6868 - val_loss: 0.2141 - val_acc: 0.6628\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2057 - acc: 0.6888 - val_loss: 0.2048 - val_acc: 0.7017\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6877 - val_loss: 0.2010 - val_acc: 0.6981\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2048 - acc: 0.6907 - val_loss: 0.2007 - val_acc: 0.7043\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2063 - acc: 0.6851 - val_loss: 0.2004 - val_acc: 0.7046\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2051 - acc: 0.6887 - val_loss: 0.2148 - val_acc: 0.6605\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2061 - acc: 0.6874 - val_loss: 0.2019 - val_acc: 0.7012\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2054 - acc: 0.6882 - val_loss: 0.2004 - val_acc: 0.7034\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2060 - acc: 0.6870 - val_loss: 0.2009 - val_acc: 0.6967\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2047 - acc: 0.6913 - val_loss: 0.2215 - val_acc: 0.6443\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2049 - acc: 0.6915 - val_loss: 0.2022 - val_acc: 0.6989\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2046 - acc: 0.6925 - val_loss: 0.2013 - val_acc: 0.6987\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2048 - acc: 0.6910 - val_loss: 0.2016 - val_acc: 0.6993\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.6893 - val_loss: 0.2094 - val_acc: 0.6734\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2046 - acc: 0.6915 - val_loss: 0.2011 - val_acc: 0.6992\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2048 - acc: 0.6901 - val_loss: 0.2002 - val_acc: 0.7017\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2052 - acc: 0.6909 - val_loss: 0.2003 - val_acc: 0.6964\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2042 - acc: 0.692 - 1s 24us/sample - loss: 0.2042 - acc: 0.6923 - val_loss: 0.2004 - val_acc: 0.6986\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2048 - acc: 0.6890 - val_loss: 0.2038 - val_acc: 0.7006\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2043 - acc: 0.6913 - val_loss: 0.2000 - val_acc: 0.7006\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2045 - acc: 0.6903 - val_loss: 0.2001 - val_acc: 0.7039\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2043 - acc: 0.6910 - val_loss: 0.2016 - val_acc: 0.6993\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2048 - acc: 0.6890 - val_loss: 0.2009 - val_acc: 0.6950\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2050 - acc: 0.6886 - val_loss: 0.2006 - val_acc: 0.7029\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2042 - acc: 0.6906 - val_loss: 0.2014 - val_acc: 0.6948\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2043 - acc: 0.6926 - val_loss: 0.2012 - val_acc: 0.7007\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2046 - acc: 0.6906 - val_loss: 0.2018 - val_acc: 0.7076\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2049 - acc: 0.6896 - val_loss: 0.2013 - val_acc: 0.7054\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2047 - acc: 0.6897 - val_loss: 0.2022 - val_acc: 0.7046\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2034 - acc: 0.6934 - val_loss: 0.2009 - val_acc: 0.7025\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2043 - acc: 0.6905 - val_loss: 0.2019 - val_acc: 0.7021\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2044 - acc: 0.6910 - val_loss: 0.2045 - val_acc: 0.6838\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2043 - acc: 0.6904 - val_loss: 0.2004 - val_acc: 0.6976\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2042 - acc: 0.6899 - val_loss: 0.2001 - val_acc: 0.7011\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2041 - acc: 0.6906 - val_loss: 0.1997 - val_acc: 0.6966\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2043 - acc: 0.6899 - val_loss: 0.2007 - val_acc: 0.6997\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2039 - acc: 0.6931 - val_loss: 0.1996 - val_acc: 0.7018\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2033 - acc: 0.6936 - val_loss: 0.1999 - val_acc: 0.6998\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2050 - acc: 0.6899 - val_loss: 0.2037 - val_acc: 0.6844\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2038 - acc: 0.6922 - val_loss: 0.2070 - val_acc: 0.6748\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2037 - acc: 0.6915 - val_loss: 0.2002 - val_acc: 0.6948\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2043 - acc: 0.6911 - val_loss: 0.2015 - val_acc: 0.6944\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2043 - acc: 0.6902 - val_loss: 0.1993 - val_acc: 0.7074\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2033 - acc: 0.6919 - val_loss: 0.1994 - val_acc: 0.6978\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2038 - acc: 0.6931 - val_loss: 0.1996 - val_acc: 0.7006\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2039 - acc: 0.6912 - val_loss: 0.1996 - val_acc: 0.6987\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2034 - acc: 0.6936 - val_loss: 0.2045 - val_acc: 0.6969\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2035 - acc: 0.6925 - val_loss: 0.2089 - val_acc: 0.6838\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2034 - acc: 0.6931 - val_loss: 0.1994 - val_acc: 0.7039\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - 2s 38us/sample - loss: 0.2036 - acc: 0.6914 - val_loss: 0.1996 - val_acc: 0.7059\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2032 - acc: 0.6920 - val_loss: 0.1996 - val_acc: 0.7039\n",
      "Epoch 155/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2043 - acc: 0.6916 - val_loss: 0.2028 - val_acc: 0.6986\n",
      "Epoch 156/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2037 - acc: 0.6930 - val_loss: 0.2028 - val_acc: 0.7001\n",
      "Epoch 157/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2030 - acc: 0.6936 - val_loss: 0.1996 - val_acc: 0.7043\n",
      "Epoch 158/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2030 - acc: 0.6933 - val_loss: 0.2001 - val_acc: 0.6962\n",
      "Epoch 159/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2032 - acc: 0.6934 - val_loss: 0.1995 - val_acc: 0.7101\n",
      "Epoch 160/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2034 - acc: 0.6918 - val_loss: 0.2024 - val_acc: 0.6928\n",
      "Epoch 161/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2037 - acc: 0.6918 - val_loss: 0.1992 - val_acc: 0.7059\n",
      "Epoch 162/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2033 - acc: 0.6928 - val_loss: 0.1997 - val_acc: 0.6975\n",
      "Epoch 163/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2033 - acc: 0.6925 - val_loss: 0.1991 - val_acc: 0.7037\n",
      "Epoch 164/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2034 - acc: 0.6919 - val_loss: 0.2023 - val_acc: 0.6970\n",
      "Epoch 165/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2028 - acc: 0.6949 - val_loss: 0.2080 - val_acc: 0.6875\n",
      "Epoch 166/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2033 - acc: 0.6928 - val_loss: 0.2029 - val_acc: 0.6872\n",
      "Epoch 167/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2036 - acc: 0.6932 - val_loss: 0.1992 - val_acc: 0.6993\n",
      "Epoch 168/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2035 - acc: 0.6920 - val_loss: 0.1996 - val_acc: 0.7021\n",
      "Epoch 169/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2032 - acc: 0.6928 - val_loss: 0.2004 - val_acc: 0.7039\n",
      "Epoch 170/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2028 - acc: 0.6935 - val_loss: 0.2022 - val_acc: 0.6962\n",
      "Epoch 171/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.2033 - acc: 0.6927 - val_loss: 0.1992 - val_acc: 0.7032\n",
      "Epoch 172/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2031 - acc: 0.6928 - val_loss: 0.2006 - val_acc: 0.6945\n",
      "Epoch 173/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2028 - acc: 0.6931 - val_loss: 0.2010 - val_acc: 0.6993\n",
      "Epoch 174/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2028 - acc: 0.6933 - val_loss: 0.2049 - val_acc: 0.6966\n",
      "Epoch 175/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2029 - acc: 0.6935 - val_loss: 0.1996 - val_acc: 0.7040\n",
      "Epoch 176/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2029 - acc: 0.6938 - val_loss: 0.1998 - val_acc: 0.6970\n",
      "Epoch 177/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2033 - acc: 0.6928 - val_loss: 0.2024 - val_acc: 0.6941\n",
      "Epoch 178/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2035 - acc: 0.6930 - val_loss: 0.2025 - val_acc: 0.6945\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='sigmoid'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 7s 116us/sample - loss: 0.2424 - acc: 0.5961 - val_loss: 0.2392 - val_acc: 0.6019\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2408 - acc: 0.5963 - val_loss: 0.2389 - val_acc: 0.6019\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2403 - acc: 0.5970 - val_loss: 0.2387 - val_acc: 0.6019\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2403 - acc: 0.5988 - val_loss: 0.2388 - val_acc: 0.6019\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2401 - acc: 0.5997 - val_loss: 0.2389 - val_acc: 0.6130: 0.59\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2399 - acc: 0.6022 - val_loss: 0.2387 - val_acc: 0.6030\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2391 - acc: 0.6044 - val_loss: 0.2372 - val_acc: 0.6139\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2377 - acc: 0.6082 - val_loss: 0.2351 - val_acc: 0.6131\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2331 - acc: 0.6228 - val_loss: 0.2529 - val_acc: 0.6178\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2266 - acc: 0.6462 - val_loss: 0.2214 - val_acc: 0.6521\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2236 - acc: 0.6555 - val_loss: 0.2192 - val_acc: 0.6630\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2217 - acc: 0.6586 - val_loss: 0.2188 - val_acc: 0.6736\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2202 - acc: 0.6625 - val_loss: 0.2199 - val_acc: 0.6768\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2200 - acc: 0.6619 - val_loss: 0.2177 - val_acc: 0.6787\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2197 - acc: 0.6648 - val_loss: 0.2195 - val_acc: 0.6498\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2185 - acc: 0.6654 - val_loss: 0.2161 - val_acc: 0.6563\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2169 - acc: 0.6698 - val_loss: 0.2174 - val_acc: 0.6560\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2166 - acc: 0.6705 - val_loss: 0.2158 - val_acc: 0.6941\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2162 - acc: 0.6712 - val_loss: 0.2218 - val_acc: 0.6515\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2147 - acc: 0.6757 - val_loss: 0.2135 - val_acc: 0.6953\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2149 - acc: 0.6727 - val_loss: 0.2108 - val_acc: 0.6765\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2143 - acc: 0.6753 - val_loss: 0.2215 - val_acc: 0.6495\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2142 - acc: 0.6753 - val_loss: 0.2116 - val_acc: 0.6976 0.2141 - acc: 0.6\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2129 - acc: 0.6792 - val_loss: 0.2098 - val_acc: 0.6773\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2126 - acc: 0.6781 - val_loss: 0.2151 - val_acc: 0.6981\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2125 - acc: 0.6797 - val_loss: 0.2082 - val_acc: 0.6855\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2115 - acc: 0.6821 - val_loss: 0.2080 - val_acc: 0.6798\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2109 - acc: 0.6825 - val_loss: 0.2136 - val_acc: 0.7023\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2108 - acc: 0.6856 - val_loss: 0.2074 - val_acc: 0.6987\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2100 - acc: 0.6841 - val_loss: 0.2088 - val_acc: 0.6715\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2097 - acc: 0.6856 - val_loss: 0.2055 - val_acc: 0.7000\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2089 - acc: 0.6872 - val_loss: 0.2081 - val_acc: 0.7026\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2085 - acc: 0.6861 - val_loss: 0.2042 - val_acc: 0.6961\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2079 - acc: 0.6877 - val_loss: 0.2106 - val_acc: 0.6945\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2070 - acc: 0.6892 - val_loss: 0.2099 - val_acc: 0.6956\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2071 - acc: 0.6886 - val_loss: 0.2028 - val_acc: 0.6911\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2070 - acc: 0.689 - 1s 21us/sample - loss: 0.2070 - acc: 0.6897 - val_loss: 0.2021 - val_acc: 0.6990\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2060 - acc: 0.6904 - val_loss: 0.2034 - val_acc: 0.6882\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2059 - acc: 0.6909 - val_loss: 0.2066 - val_acc: 0.6948\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2055 - acc: 0.6901 - val_loss: 0.2081 - val_acc: 0.6900\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2054 - acc: 0.6913 - val_loss: 0.2014 - val_acc: 0.6952\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2054 - acc: 0.6895 - val_loss: 0.2056 - val_acc: 0.6976\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2049 - acc: 0.6920 - val_loss: 0.2030 - val_acc: 0.7011\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2047 - acc: 0.6922 - val_loss: 0.2005 - val_acc: 0.7039\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2044 - acc: 0.6907 - val_loss: 0.2009 - val_acc: 0.7043\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2042 - acc: 0.6913 - val_loss: 0.2026 - val_acc: 0.6857\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2038 - acc: 0.6943 - val_loss: 0.2064 - val_acc: 0.6784\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2037 - acc: 0.6920 - val_loss: 0.2003 - val_acc: 0.7023\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2033 - acc: 0.6934 - val_loss: 0.2010 - val_acc: 0.6992\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2035 - acc: 0.6944 - val_loss: 0.2016 - val_acc: 0.6922\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2032 - acc: 0.6941 - val_loss: 0.2077 - val_acc: 0.6760\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2031 - acc: 0.6934 - val_loss: 0.2044 - val_acc: 0.6993\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2036 - acc: 0.6928 - val_loss: 0.1992 - val_acc: 0.7018\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2033 - acc: 0.6930 - val_loss: 0.1995 - val_acc: 0.6990\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2032 - acc: 0.6927 - val_loss: 0.2041 - val_acc: 0.7020\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2031 - acc: 0.6923 - val_loss: 0.2069 - val_acc: 0.6784 0.2032 -\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2029 - acc: 0.6944 - val_loss: 0.1988 - val_acc: 0.7048\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2027 - acc: 0.6936 - val_loss: 0.1994 - val_acc: 0.7018\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2024 - acc: 0.6949 - val_loss: 0.1995 - val_acc: 0.7049\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2021 - acc: 0.6955 - val_loss: 0.1985 - val_acc: 0.7063\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2022 - acc: 0.6946 - val_loss: 0.2019 - val_acc: 0.6993\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2022 - acc: 0.694 - 2s 28us/sample - loss: 0.2021 - acc: 0.6944 - val_loss: 0.1980 - val_acc: 0.7032\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2016 - acc: 0.6956 - val_loss: 0.1980 - val_acc: 0.7045\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2025 - acc: 0.6941 - val_loss: 0.2002 - val_acc: 0.7021\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2015 - acc: 0.6952 - val_loss: 0.1993 - val_acc: 0.7006\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2014 - acc: 0.6961 - val_loss: 0.1982 - val_acc: 0.6970\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2013 - acc: 0.6960 - val_loss: 0.1976 - val_acc: 0.7040\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2013 - acc: 0.6948 - val_loss: 0.1978 - val_acc: 0.7079\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2010 - acc: 0.6959 - val_loss: 0.2040 - val_acc: 0.6958\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2009 - acc: 0.6954 - val_loss: 0.1972 - val_acc: 0.7053\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2006 - acc: 0.6977 - val_loss: 0.2015 - val_acc: 0.7021\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2008 - acc: 0.6972 - val_loss: 0.1976 - val_acc: 0.7039\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2014 - acc: 0.6952 - val_loss: 0.1975 - val_acc: 0.7029\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2002 - acc: 0.6973 - val_loss: 0.1975 - val_acc: 0.7074\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2007 - acc: 0.6945 - val_loss: 0.1979 - val_acc: 0.7012\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2005 - acc: 0.6968 - val_loss: 0.1983 - val_acc: 0.6997\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2007 - acc: 0.6960 - val_loss: 0.1973 - val_acc: 0.7023\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2004 - acc: 0.6970 - val_loss: 0.1972 - val_acc: 0.7068\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2002 - acc: 0.6968 - val_loss: 0.1970 - val_acc: 0.7054\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1997 - acc: 0.6972 - val_loss: 0.1988 - val_acc: 0.6998\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2004 - acc: 0.6957 - val_loss: 0.1966 - val_acc: 0.7032\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1994 - acc: 0.6984 - val_loss: 0.2067 - val_acc: 0.6928\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2005 - acc: 0.6942 - val_loss: 0.1997 - val_acc: 0.6948\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1996 - acc: 0.6982 - val_loss: 0.2022 - val_acc: 0.6995\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2000 - acc: 0.6959 - val_loss: 0.1967 - val_acc: 0.7053\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1996 - acc: 0.6981 - val_loss: 0.1975 - val_acc: 0.6995\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1995 - acc: 0.6979 - val_loss: 0.1970 - val_acc: 0.7020\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1998 - acc: 0.6973 - val_loss: 0.1965 - val_acc: 0.7062\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1995 - acc: 0.6970 - val_loss: 0.1984 - val_acc: 0.7018\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.1995 - acc: 0.697 - 1s 22us/sample - loss: 0.1994 - acc: 0.6978 - val_loss: 0.1960 - val_acc: 0.7053\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1992 - acc: 0.6973 - val_loss: 0.1965 - val_acc: 0.7070\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1996 - acc: 0.6978 - val_loss: 0.1975 - val_acc: 0.7029\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1992 - acc: 0.6984 - val_loss: 0.1962 - val_acc: 0.7059\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1987 - acc: 0.6989 - val_loss: 0.2055 - val_acc: 0.6882\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1993 - acc: 0.6978 - val_loss: 0.1961 - val_acc: 0.7063\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1992 - acc: 0.6980 - val_loss: 0.1956 - val_acc: 0.7080\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1991 - acc: 0.6989 - val_loss: 0.1980 - val_acc: 0.7082\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1990 - acc: 0.6989 - val_loss: 0.1958 - val_acc: 0.7042\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1988 - acc: 0.6975 - val_loss: 0.1957 - val_acc: 0.7068\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1990 - acc: 0.6979 - val_loss: 0.1962 - val_acc: 0.7053\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1988 - acc: 0.6978 - val_loss: 0.1958 - val_acc: 0.7046\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1984 - acc: 0.6998 - val_loss: 0.1994 - val_acc: 0.7006\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1985 - acc: 0.6993 - val_loss: 0.2003 - val_acc: 0.6993\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1988 - acc: 0.6973 - val_loss: 0.1957 - val_acc: 0.7065\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1984 - acc: 0.6981 - val_loss: 0.1963 - val_acc: 0.7039\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1982 - acc: 0.6980 - val_loss: 0.1954 - val_acc: 0.7046\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1993 - acc: 0.6966 - val_loss: 0.1957 - val_acc: 0.7084\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1989 - acc: 0.6987 - val_loss: 0.1983 - val_acc: 0.7015\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1980 - acc: 0.6980 - val_loss: 0.1963 - val_acc: 0.7070\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1984 - acc: 0.6970 - val_loss: 0.1959 - val_acc: 0.7059\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1989 - acc: 0.6985 - val_loss: 0.1962 - val_acc: 0.7062\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1980 - acc: 0.6982 - val_loss: 0.1963 - val_acc: 0.7039\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1978 - acc: 0.6990 - val_loss: 0.1958 - val_acc: 0.7074\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1980 - acc: 0.6993 - val_loss: 0.1957 - val_acc: 0.7073\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1984 - acc: 0.7012 - val_loss: 0.2064 - val_acc: 0.6824\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1982 - acc: 0.7003 - val_loss: 0.1976 - val_acc: 0.7080\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1978 - acc: 0.6999 - val_loss: 0.1948 - val_acc: 0.7073\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1979 - acc: 0.6992 - val_loss: 0.1997 - val_acc: 0.6997\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1978 - acc: 0.6982 - val_loss: 0.1951 - val_acc: 0.7046\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1977 - acc: 0.6997 - val_loss: 0.2006 - val_acc: 0.7014\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1976 - acc: 0.7005 - val_loss: 0.1951 - val_acc: 0.7029\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1976 - acc: 0.6984 - val_loss: 0.2047 - val_acc: 0.6911\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1976 - acc: 0.7003 - val_loss: 0.1962 - val_acc: 0.7035\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1978 - acc: 0.7002 - val_loss: 0.1950 - val_acc: 0.7053\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1971 - acc: 0.6999 - val_loss: 0.1947 - val_acc: 0.7051\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1977 - acc: 0.6999 - val_loss: 0.1963 - val_acc: 0.6989\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1975 - acc: 0.6998 - val_loss: 0.1954 - val_acc: 0.7074\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1973 - acc: 0.7005 - val_loss: 0.1954 - val_acc: 0.7057\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1973 - acc: 0.7004 - val_loss: 0.1945 - val_acc: 0.7073\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1973 - acc: 0.6990 - val_loss: 0.1945 - val_acc: 0.7028\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1972 - acc: 0.7001 - val_loss: 0.1952 - val_acc: 0.7014\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1973 - acc: 0.6992 - val_loss: 0.1942 - val_acc: 0.7079\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1970 - acc: 0.7011 - val_loss: 0.1966 - val_acc: 0.7049\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1971 - acc: 0.7009 - val_loss: 0.1950 - val_acc: 0.7053\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1968 - acc: 0.7002 - val_loss: 0.1947 - val_acc: 0.7056\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1971 - acc: 0.6991 - val_loss: 0.1948 - val_acc: 0.7053\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1972 - acc: 0.7003 - val_loss: 0.1943 - val_acc: 0.7087\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1968 - acc: 0.7001 - val_loss: 0.1959 - val_acc: 0.7115\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1975 - acc: 0.7009 - val_loss: 0.1945 - val_acc: 0.7056\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1969 - acc: 0.7004 - val_loss: 0.1987 - val_acc: 0.7053\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1965 - acc: 0.7009 - val_loss: 0.1945 - val_acc: 0.7088\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1963 - acc: 0.7014 - val_loss: 0.1961 - val_acc: 0.7099\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1965 - acc: 0.7007 - val_loss: 0.1955 - val_acc: 0.7048\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1967 - acc: 0.6999 - val_loss: 0.1940 - val_acc: 0.7063\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1972 - acc: 0.6994 - val_loss: 0.1959 - val_acc: 0.7049\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1964 - acc: 0.7002 - val_loss: 0.1939 - val_acc: 0.7065\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1964 - acc: 0.7015 - val_loss: 0.1940 - val_acc: 0.7048\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1962 - acc: 0.7008 - val_loss: 0.1973 - val_acc: 0.7105\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1969 - acc: 0.7010 - val_loss: 0.1982 - val_acc: 0.7116\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1966 - acc: 0.7005 - val_loss: 0.1967 - val_acc: 0.7034\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1960 - acc: 0.7000 - val_loss: 0.1937 - val_acc: 0.7049: 0.1963 - acc: 0.\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1968 - acc: 0.7014 - val_loss: 0.1951 - val_acc: 0.7046\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1962 - acc: 0.7009 - val_loss: 0.1939 - val_acc: 0.7073\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1961 - acc: 0.7002 - val_loss: 0.1940 - val_acc: 0.7048\n",
      "Epoch 155/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1962 - acc: 0.7011 - val_loss: 0.1945 - val_acc: 0.7060\n",
      "Epoch 156/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1960 - acc: 0.7029 - val_loss: 0.1942 - val_acc: 0.7063\n",
      "Epoch 157/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1961 - acc: 0.7017 - val_loss: 0.1949 - val_acc: 0.7054\n",
      "Epoch 158/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1959 - acc: 0.7026 - val_loss: 0.1959 - val_acc: 0.7074\n",
      "Epoch 159/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1958 - acc: 0.7028 - val_loss: 0.1937 - val_acc: 0.7067\n",
      "Epoch 160/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1957 - acc: 0.7020 - val_loss: 0.1952 - val_acc: 0.7068\n",
      "Epoch 161/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1959 - acc: 0.7008 - val_loss: 0.1949 - val_acc: 0.7099\n",
      "Epoch 162/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1959 - acc: 0.7010 - val_loss: 0.1934 - val_acc: 0.7087\n",
      "Epoch 163/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1955 - acc: 0.7016 - val_loss: 0.1935 - val_acc: 0.7073\n",
      "Epoch 164/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1959 - acc: 0.7025 - val_loss: 0.1943 - val_acc: 0.7062\n",
      "Epoch 165/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1957 - acc: 0.7023 - val_loss: 0.1956 - val_acc: 0.7070\n",
      "Epoch 166/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1956 - acc: 0.7018 - val_loss: 0.1955 - val_acc: 0.7087\n",
      "Epoch 167/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1956 - acc: 0.7021 - val_loss: 0.1936 - val_acc: 0.7098\n",
      "Epoch 168/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1956 - acc: 0.7025 - val_loss: 0.1948 - val_acc: 0.7065\n",
      "Epoch 169/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1951 - acc: 0.7024 - val_loss: 0.1931 - val_acc: 0.7077\n",
      "Epoch 170/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1956 - acc: 0.7023 - val_loss: 0.1931 - val_acc: 0.7042\n",
      "Epoch 171/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1952 - acc: 0.7025 - val_loss: 0.1941 - val_acc: 0.7121\n",
      "Epoch 172/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1955 - acc: 0.7016 - val_loss: 0.1988 - val_acc: 0.7021\n",
      "Epoch 173/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1954 - acc: 0.7016 - val_loss: 0.1929 - val_acc: 0.7071\n",
      "Epoch 174/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1950 - acc: 0.7036 - val_loss: 0.1958 - val_acc: 0.7063\n",
      "Epoch 175/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1952 - acc: 0.7032 - val_loss: 0.1937 - val_acc: 0.7042\n",
      "Epoch 176/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1951 - acc: 0.7023 - val_loss: 0.1951 - val_acc: 0.7073\n",
      "Epoch 177/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1948 - acc: 0.7029 - val_loss: 0.1926 - val_acc: 0.7063\n",
      "Epoch 178/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1949 - acc: 0.7037 - val_loss: 0.1932 - val_acc: 0.7054\n",
      "Epoch 179/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1950 - acc: 0.7031 - val_loss: 0.1929 - val_acc: 0.7070\n",
      "Epoch 180/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1951 - acc: 0.7025 - val_loss: 0.1943 - val_acc: 0.7060\n",
      "Epoch 181/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1946 - acc: 0.7051 - val_loss: 0.1929 - val_acc: 0.7104\n",
      "Epoch 182/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1950 - acc: 0.7032 - val_loss: 0.1927 - val_acc: 0.7087\n",
      "Epoch 183/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1945 - acc: 0.7034 - val_loss: 0.1926 - val_acc: 0.7074\n",
      "Epoch 184/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1947 - acc: 0.7031 - val_loss: 0.1926 - val_acc: 0.7102\n",
      "Epoch 185/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1949 - acc: 0.7024 - val_loss: 0.1927 - val_acc: 0.7065\n",
      "Epoch 186/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1950 - acc: 0.7020 - val_loss: 0.1927 - val_acc: 0.7091\n",
      "Epoch 187/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1946 - acc: 0.7043 - val_loss: 0.1984 - val_acc: 0.7012\n",
      "Epoch 188/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1947 - acc: 0.7037 - val_loss: 0.1936 - val_acc: 0.7108\n",
      "Epoch 189/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1947 - acc: 0.7037 - val_loss: 0.1928 - val_acc: 0.7084\n",
      "Epoch 190/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1946 - acc: 0.7046 - val_loss: 0.1927 - val_acc: 0.7037\n",
      "Epoch 191/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1946 - acc: 0.7031 - val_loss: 0.1937 - val_acc: 0.7084\n",
      "Epoch 192/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1946 - acc: 0.7022 - val_loss: 0.1925 - val_acc: 0.7091\n",
      "Epoch 193/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1944 - acc: 0.7031 - val_loss: 0.1931 - val_acc: 0.7113\n",
      "Epoch 194/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1946 - acc: 0.7033 - val_loss: 0.1938 - val_acc: 0.7126\n",
      "Epoch 195/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1943 - acc: 0.7049 - val_loss: 0.1924 - val_acc: 0.7093\n",
      "Epoch 196/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1942 - acc: 0.7036 - val_loss: 0.1929 - val_acc: 0.7091\n",
      "Epoch 197/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1942 - acc: 0.7043 - val_loss: 0.1924 - val_acc: 0.7091\n",
      "Epoch 198/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1943 - acc: 0.7036 - val_loss: 0.1930 - val_acc: 0.7087\n",
      "Epoch 199/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1941 - acc: 0.7036 - val_loss: 0.1923 - val_acc: 0.7074\n",
      "Epoch 200/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1943 - acc: 0.7037 - val_loss: 0.1918 - val_acc: 0.7113\n",
      "Epoch 201/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1941 - acc: 0.7054 - val_loss: 0.1920 - val_acc: 0.7068\n",
      "Epoch 202/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1942 - acc: 0.7048 - val_loss: 0.1918 - val_acc: 0.7094\n",
      "Epoch 203/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1939 - acc: 0.7045 - val_loss: 0.1926 - val_acc: 0.7108\n",
      "Epoch 204/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1942 - acc: 0.7049 - val_loss: 0.1927 - val_acc: 0.7077\n",
      "Epoch 205/400\n",
      "57916/57916 [==============================] - 2s 40us/sample - loss: 0.1942 - acc: 0.7050 - val_loss: 0.1918 - val_acc: 0.7130\n",
      "Epoch 206/400\n",
      "57916/57916 [==============================] - 3s 48us/sample - loss: 0.1938 - acc: 0.7064 - val_loss: 0.1924 - val_acc: 0.7070\n",
      "Epoch 207/400\n",
      "57916/57916 [==============================] - 2s 39us/sample - loss: 0.1939 - acc: 0.7044 - val_loss: 0.1918 - val_acc: 0.7122\n",
      "Epoch 208/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1936 - acc: 0.7051 - val_loss: 0.1938 - val_acc: 0.7087\n",
      "Epoch 209/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1939 - acc: 0.7049 - val_loss: 0.1919 - val_acc: 0.7150\n",
      "Epoch 210/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1937 - acc: 0.7055 - val_loss: 0.1948 - val_acc: 0.7084\n",
      "Epoch 211/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1939 - acc: 0.7056 - val_loss: 0.1954 - val_acc: 0.7039\n",
      "Epoch 212/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1938 - acc: 0.7064 - val_loss: 0.1917 - val_acc: 0.7104\n",
      "Epoch 213/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1938 - acc: 0.7054 - val_loss: 0.1944 - val_acc: 0.7091\n",
      "Epoch 214/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1938 - acc: 0.7048 - val_loss: 0.1918 - val_acc: 0.7063\n",
      "Epoch 215/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1937 - acc: 0.7051 - val_loss: 0.1956 - val_acc: 0.7042\n",
      "Epoch 216/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1935 - acc: 0.7059 - val_loss: 0.1929 - val_acc: 0.7108\n",
      "Epoch 217/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1937 - acc: 0.7064 - val_loss: 0.1915 - val_acc: 0.7099\n",
      "Epoch 218/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1936 - acc: 0.7066 - val_loss: 0.1915 - val_acc: 0.7121\n",
      "Epoch 219/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1935 - acc: 0.7068 - val_loss: 0.1923 - val_acc: 0.7093\n",
      "Epoch 220/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1935 - acc: 0.7044 - val_loss: 0.1916 - val_acc: 0.7146\n",
      "Epoch 221/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1934 - acc: 0.7072 - val_loss: 0.1914 - val_acc: 0.7110\n",
      "Epoch 222/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1933 - acc: 0.7060 - val_loss: 0.1931 - val_acc: 0.7104\n",
      "Epoch 223/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1931 - acc: 0.7062 - val_loss: 0.1932 - val_acc: 0.7077\n",
      "Epoch 224/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1931 - acc: 0.7071 - val_loss: 0.1925 - val_acc: 0.7060\n",
      "Epoch 225/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1932 - acc: 0.7060 - val_loss: 0.1917 - val_acc: 0.7090\n",
      "Epoch 226/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1934 - acc: 0.7068 - val_loss: 0.1916 - val_acc: 0.7129\n",
      "Epoch 227/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1933 - acc: 0.7061 - val_loss: 0.1913 - val_acc: 0.7118\n",
      "Epoch 228/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1932 - acc: 0.7069 - val_loss: 0.1915 - val_acc: 0.7121\n",
      "Epoch 229/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1929 - acc: 0.7076 - val_loss: 0.1914 - val_acc: 0.7150\n",
      "Epoch 230/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1933 - acc: 0.7057 - val_loss: 0.1926 - val_acc: 0.7076\n",
      "Epoch 231/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1929 - acc: 0.7080 - val_loss: 0.1911 - val_acc: 0.7119\n",
      "Epoch 232/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1931 - acc: 0.7066 - val_loss: 0.1912 - val_acc: 0.7108\n",
      "Epoch 233/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1933 - acc: 0.7078 - val_loss: 0.1920 - val_acc: 0.7144\n",
      "Epoch 234/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1931 - acc: 0.7070 - val_loss: 0.1914 - val_acc: 0.7124\n",
      "Epoch 235/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1930 - acc: 0.7082 - val_loss: 0.1923 - val_acc: 0.7105\n",
      "Epoch 236/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1932 - acc: 0.7069 - val_loss: 0.1912 - val_acc: 0.7136\n",
      "Epoch 237/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1930 - acc: 0.7065 - val_loss: 0.1927 - val_acc: 0.7096\n",
      "Epoch 238/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1929 - acc: 0.7075 - val_loss: 0.1921 - val_acc: 0.7104\n",
      "Epoch 239/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1930 - acc: 0.7065 - val_loss: 0.1911 - val_acc: 0.7085\n",
      "Epoch 240/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1929 - acc: 0.7071 - val_loss: 0.1911 - val_acc: 0.7132\n",
      "Epoch 241/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1928 - acc: 0.7065 - val_loss: 0.1908 - val_acc: 0.7094\n",
      "Epoch 242/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1927 - acc: 0.7064 - val_loss: 0.1914 - val_acc: 0.7122\n",
      "Epoch 243/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1930 - acc: 0.7064 - val_loss: 0.1915 - val_acc: 0.7110\n",
      "Epoch 244/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1930 - acc: 0.7066 - val_loss: 0.1942 - val_acc: 0.7063\n",
      "Epoch 245/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1927 - acc: 0.7076 - val_loss: 0.1917 - val_acc: 0.7122\n",
      "Epoch 246/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1926 - acc: 0.7071 - val_loss: 0.1914 - val_acc: 0.7141\n",
      "Epoch 247/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1925 - acc: 0.7080 - val_loss: 0.1908 - val_acc: 0.7076\n",
      "Epoch 248/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1926 - acc: 0.7079 - val_loss: 0.1907 - val_acc: 0.7118\n",
      "Epoch 249/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1924 - acc: 0.7085 - val_loss: 0.1911 - val_acc: 0.7110\n",
      "Epoch 250/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1926 - acc: 0.7073 - val_loss: 0.1923 - val_acc: 0.7136\n",
      "Epoch 251/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1925 - acc: 0.7072 - val_loss: 0.1950 - val_acc: 0.7043\n",
      "Epoch 252/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1925 - acc: 0.7085 - val_loss: 0.1929 - val_acc: 0.7113\n",
      "Epoch 253/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1929 - acc: 0.7074 - val_loss: 0.1911 - val_acc: 0.7152\n",
      "Epoch 254/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1929 - acc: 0.7075 - val_loss: 0.1907 - val_acc: 0.7141\n",
      "Epoch 255/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1925 - acc: 0.7084 - val_loss: 0.1905 - val_acc: 0.7140\n",
      "Epoch 256/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1923 - acc: 0.7093 - val_loss: 0.1910 - val_acc: 0.7150\n",
      "Epoch 257/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1923 - acc: 0.7083 - val_loss: 0.1905 - val_acc: 0.7140\n",
      "Epoch 258/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1923 - acc: 0.7089 - val_loss: 0.1908 - val_acc: 0.7105\n",
      "Epoch 259/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1924 - acc: 0.7083 - val_loss: 0.1913 - val_acc: 0.7124\n",
      "Epoch 260/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1922 - acc: 0.7087 - val_loss: 0.1906 - val_acc: 0.7122\n",
      "Epoch 261/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1920 - acc: 0.7096 - val_loss: 0.1909 - val_acc: 0.7115\n",
      "Epoch 262/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1922 - acc: 0.7089 - val_loss: 0.1914 - val_acc: 0.7152\n",
      "Epoch 263/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1921 - acc: 0.7094 - val_loss: 0.1905 - val_acc: 0.7094\n",
      "Epoch 264/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1922 - acc: 0.7094 - val_loss: 0.1906 - val_acc: 0.7133\n",
      "Epoch 265/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1919 - acc: 0.7081 - val_loss: 0.1906 - val_acc: 0.7127\n",
      "Epoch 266/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1921 - acc: 0.7090 - val_loss: 0.1936 - val_acc: 0.7132\n",
      "Epoch 267/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1919 - acc: 0.7099 - val_loss: 0.1909 - val_acc: 0.7175\n",
      "Epoch 268/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1922 - acc: 0.7090 - val_loss: 0.1905 - val_acc: 0.7158\n",
      "Epoch 269/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1921 - acc: 0.7088 - val_loss: 0.1903 - val_acc: 0.7154\n",
      "Epoch 270/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1918 - acc: 0.7095 - val_loss: 0.1905 - val_acc: 0.7146\n",
      "Epoch 271/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.1918 - acc: 0.7105 - val_loss: 0.1901 - val_acc: 0.7119\n",
      "Epoch 272/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1921 - acc: 0.7094 - val_loss: 0.1919 - val_acc: 0.7094\n",
      "Epoch 273/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1918 - acc: 0.7103 - val_loss: 0.1926 - val_acc: 0.7167\n",
      "Epoch 274/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1918 - acc: 0.7113 - val_loss: 0.1901 - val_acc: 0.7171\n",
      "Epoch 275/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1917 - acc: 0.7106 - val_loss: 0.1902 - val_acc: 0.7130\n",
      "Epoch 276/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1917 - acc: 0.7103 - val_loss: 0.1918 - val_acc: 0.7112\n",
      "Epoch 277/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1917 - acc: 0.7105 - val_loss: 0.1912 - val_acc: 0.7163\n",
      "Epoch 278/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1916 - acc: 0.7103 - val_loss: 0.1916 - val_acc: 0.7158\n",
      "Epoch 279/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1918 - acc: 0.7106 - val_loss: 0.1899 - val_acc: 0.7146\n",
      "Epoch 280/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1915 - acc: 0.7119 - val_loss: 0.1897 - val_acc: 0.7133\n",
      "Epoch 281/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1914 - acc: 0.7111 - val_loss: 0.1924 - val_acc: 0.7101\n",
      "Epoch 282/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1913 - acc: 0.7115 - val_loss: 0.1913 - val_acc: 0.7129\n",
      "Epoch 283/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1912 - acc: 0.7117 - val_loss: 0.1902 - val_acc: 0.7164\n",
      "Epoch 284/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1913 - acc: 0.7119 - val_loss: 0.1906 - val_acc: 0.7166\n",
      "Epoch 285/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1915 - acc: 0.7105 - val_loss: 0.1897 - val_acc: 0.7144\n",
      "Epoch 286/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1914 - acc: 0.7109 - val_loss: 0.1903 - val_acc: 0.7135\n",
      "Epoch 287/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1912 - acc: 0.7122 - val_loss: 0.1902 - val_acc: 0.7108\n",
      "Epoch 288/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1914 - acc: 0.7115 - val_loss: 0.1932 - val_acc: 0.7054\n",
      "Epoch 289/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1909 - acc: 0.7125 - val_loss: 0.1905 - val_acc: 0.7164\n",
      "Epoch 290/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1911 - acc: 0.7123 - val_loss: 0.1918 - val_acc: 0.7133\n",
      "Epoch 291/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1912 - acc: 0.7128 - val_loss: 0.1905 - val_acc: 0.7132\n",
      "Epoch 292/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1908 - acc: 0.7136 - val_loss: 0.1900 - val_acc: 0.7130\n",
      "Epoch 293/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1909 - acc: 0.7115 - val_loss: 0.1898 - val_acc: 0.7216\n",
      "Epoch 294/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1910 - acc: 0.7117 - val_loss: 0.1913 - val_acc: 0.7158\n",
      "Epoch 295/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1909 - acc: 0.7136 - val_loss: 0.1904 - val_acc: 0.7158\n",
      "Epoch 296/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1908 - acc: 0.7136 - val_loss: 0.1941 - val_acc: 0.7003\n",
      "Epoch 297/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1909 - acc: 0.7113 - val_loss: 0.1895 - val_acc: 0.7174\n",
      "Epoch 298/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1907 - acc: 0.7128 - val_loss: 0.1898 - val_acc: 0.7150\n",
      "Epoch 299/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1907 - acc: 0.7128 - val_loss: 0.1911 - val_acc: 0.7147\n",
      "Epoch 300/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1910 - acc: 0.7131 - val_loss: 0.1894 - val_acc: 0.7197\n",
      "Epoch 301/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1907 - acc: 0.7129 - val_loss: 0.1892 - val_acc: 0.7186\n",
      "Epoch 302/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1904 - acc: 0.7135 - val_loss: 0.1926 - val_acc: 0.7043\n",
      "Epoch 303/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1908 - acc: 0.7122 - val_loss: 0.1892 - val_acc: 0.7143\n",
      "Epoch 304/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1906 - acc: 0.7130 - val_loss: 0.1945 - val_acc: 0.7028\n",
      "Epoch 305/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1906 - acc: 0.7136 - val_loss: 0.1895 - val_acc: 0.7180\n",
      "Epoch 306/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1906 - acc: 0.7123 - val_loss: 0.1896 - val_acc: 0.7174\n",
      "Epoch 307/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1906 - acc: 0.7123 - val_loss: 0.1895 - val_acc: 0.7199\n",
      "Epoch 308/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1902 - acc: 0.7146 - val_loss: 0.1894 - val_acc: 0.7194\n",
      "Epoch 309/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1904 - acc: 0.7135 - val_loss: 0.1900 - val_acc: 0.7124\n",
      "Epoch 310/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1904 - acc: 0.7129 - val_loss: 0.1890 - val_acc: 0.7174\n",
      "Epoch 311/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7138 - val_loss: 0.1901 - val_acc: 0.7171\n",
      "Epoch 312/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1904 - acc: 0.7135 - val_loss: 0.1912 - val_acc: 0.7126\n",
      "Epoch 313/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1901 - acc: 0.7128 - val_loss: 0.1890 - val_acc: 0.7242\n",
      "Epoch 314/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7135 - val_loss: 0.1901 - val_acc: 0.7146\n",
      "Epoch 315/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7140 - val_loss: 0.1942 - val_acc: 0.6964\n",
      "Epoch 316/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1902 - acc: 0.7136 - val_loss: 0.1893 - val_acc: 0.7160\n",
      "Epoch 317/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1904 - acc: 0.7134 - val_loss: 0.1889 - val_acc: 0.7239\n",
      "Epoch 318/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1899 - acc: 0.7150 - val_loss: 0.1889 - val_acc: 0.7175\n",
      "Epoch 319/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1902 - acc: 0.7136 - val_loss: 0.1895 - val_acc: 0.7144\n",
      "Epoch 320/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1902 - acc: 0.7137 - val_loss: 0.1887 - val_acc: 0.7167\n",
      "Epoch 321/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1901 - acc: 0.7138 - val_loss: 0.1888 - val_acc: 0.7172\n",
      "Epoch 322/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1900 - acc: 0.7148 - val_loss: 0.1891 - val_acc: 0.7194\n",
      "Epoch 323/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1899 - acc: 0.7150 - val_loss: 0.1888 - val_acc: 0.7129\n",
      "Epoch 324/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1897 - acc: 0.7154 - val_loss: 0.1885 - val_acc: 0.7216\n",
      "Epoch 325/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1900 - acc: 0.7148 - val_loss: 0.1930 - val_acc: 0.7056\n",
      "Epoch 326/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1900 - acc: 0.7146 - val_loss: 0.1885 - val_acc: 0.7152\n",
      "Epoch 327/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1897 - acc: 0.7144 - val_loss: 0.1883 - val_acc: 0.7230\n",
      "Epoch 328/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1898 - acc: 0.7151 - val_loss: 0.1891 - val_acc: 0.7213\n",
      "Epoch 329/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1895 - acc: 0.7155 - val_loss: 0.1884 - val_acc: 0.7144\n",
      "Epoch 330/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1899 - acc: 0.7143 - val_loss: 0.1885 - val_acc: 0.7163\n",
      "Epoch 331/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1896 - acc: 0.7152 - val_loss: 0.1882 - val_acc: 0.7195\n",
      "Epoch 332/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1897 - acc: 0.7142 - val_loss: 0.1883 - val_acc: 0.7202\n",
      "Epoch 333/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1895 - acc: 0.7142 - val_loss: 0.1883 - val_acc: 0.7209\n",
      "Epoch 334/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1895 - acc: 0.7160 - val_loss: 0.1907 - val_acc: 0.7090\n",
      "Epoch 335/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1900 - acc: 0.7143 - val_loss: 0.1907 - val_acc: 0.7172\n",
      "Epoch 336/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1892 - acc: 0.7156 - val_loss: 0.1883 - val_acc: 0.7149\n",
      "Epoch 337/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1895 - acc: 0.7146 - val_loss: 0.1889 - val_acc: 0.7194\n",
      "Epoch 338/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1894 - acc: 0.7159 - val_loss: 0.1884 - val_acc: 0.7174\n",
      "Epoch 339/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1893 - acc: 0.7165 - val_loss: 0.1903 - val_acc: 0.7169\n",
      "Epoch 340/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1893 - acc: 0.7151 - val_loss: 0.1880 - val_acc: 0.7202\n",
      "Epoch 341/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1898 - acc: 0.7160 - val_loss: 0.1881 - val_acc: 0.7166\n",
      "Epoch 342/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1891 - acc: 0.7157 - val_loss: 0.1879 - val_acc: 0.7211\n",
      "Epoch 343/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1891 - acc: 0.7166 - val_loss: 0.1887 - val_acc: 0.7152\n",
      "Epoch 344/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1894 - acc: 0.7160 - val_loss: 0.1892 - val_acc: 0.7104\n",
      "Epoch 345/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1891 - acc: 0.7163 - val_loss: 0.1910 - val_acc: 0.7090\n",
      "Epoch 346/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1892 - acc: 0.7167 - val_loss: 0.1881 - val_acc: 0.7209\n",
      "Epoch 347/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1891 - acc: 0.7174 - val_loss: 0.1881 - val_acc: 0.7188\n",
      "Epoch 348/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1890 - acc: 0.7172 - val_loss: 0.1901 - val_acc: 0.7143\n",
      "Epoch 349/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1890 - acc: 0.7163 - val_loss: 0.1880 - val_acc: 0.7174\n",
      "Epoch 350/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1890 - acc: 0.7160 - val_loss: 0.1880 - val_acc: 0.7211\n",
      "Epoch 351/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1888 - acc: 0.7176 - val_loss: 0.1889 - val_acc: 0.7155\n",
      "Epoch 352/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1890 - acc: 0.7165 - val_loss: 0.1893 - val_acc: 0.7141\n",
      "Epoch 353/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1892 - acc: 0.7166 - val_loss: 0.1876 - val_acc: 0.7160\n",
      "Epoch 354/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1888 - acc: 0.7171 - val_loss: 0.1878 - val_acc: 0.7231\n",
      "Epoch 355/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1889 - acc: 0.7165 - val_loss: 0.1902 - val_acc: 0.7126\n",
      "Epoch 356/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1888 - acc: 0.7175 - val_loss: 0.1879 - val_acc: 0.7250\n",
      "Epoch 357/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1886 - acc: 0.7175 - val_loss: 0.1890 - val_acc: 0.7192\n",
      "Epoch 358/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1888 - acc: 0.7182 - val_loss: 0.1934 - val_acc: 0.7091\n",
      "Epoch 359/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1887 - acc: 0.7182 - val_loss: 0.1874 - val_acc: 0.7223\n",
      "Epoch 360/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1887 - acc: 0.7171 - val_loss: 0.1879 - val_acc: 0.7236\n",
      "Epoch 361/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1889 - acc: 0.7169 - val_loss: 0.1877 - val_acc: 0.7220\n",
      "Epoch 362/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1886 - acc: 0.7184 - val_loss: 0.1876 - val_acc: 0.7216\n",
      "Epoch 363/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1886 - acc: 0.7173 - val_loss: 0.1877 - val_acc: 0.7203\n",
      "Epoch 364/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1886 - acc: 0.7177 - val_loss: 0.1909 - val_acc: 0.7080\n",
      "Epoch 365/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1886 - acc: 0.7177 - val_loss: 0.1873 - val_acc: 0.7199\n",
      "Epoch 366/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1885 - acc: 0.7175 - val_loss: 0.1880 - val_acc: 0.7178\n",
      "Epoch 367/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1884 - acc: 0.7180 - val_loss: 0.1882 - val_acc: 0.7189\n",
      "Epoch 368/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1883 - acc: 0.7184 - val_loss: 0.1879 - val_acc: 0.7219\n",
      "Epoch 369/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1885 - acc: 0.7177 - val_loss: 0.1876 - val_acc: 0.7230\n",
      "Epoch 370/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1883 - acc: 0.7183 - val_loss: 0.1873 - val_acc: 0.7186\n",
      "Epoch 371/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1885 - acc: 0.7174 - val_loss: 0.1876 - val_acc: 0.7250\n",
      "Epoch 372/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1883 - acc: 0.7184 - val_loss: 0.1902 - val_acc: 0.7101\n",
      "Epoch 373/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1882 - acc: 0.7191 - val_loss: 0.1904 - val_acc: 0.7112\n",
      "Epoch 374/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1882 - acc: 0.7180 - val_loss: 0.1873 - val_acc: 0.7211\n",
      "Epoch 375/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1884 - acc: 0.7189 - val_loss: 0.1879 - val_acc: 0.7220\n",
      "Epoch 376/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1884 - acc: 0.7180 - val_loss: 0.1872 - val_acc: 0.7189\n",
      "Epoch 377/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1879 - acc: 0.7198 - val_loss: 0.1873 - val_acc: 0.7230\n",
      "Epoch 378/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1884 - acc: 0.7178 - val_loss: 0.1887 - val_acc: 0.7195\n",
      "Epoch 379/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1881 - acc: 0.7187 - val_loss: 0.1873 - val_acc: 0.7195\n",
      "Epoch 380/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1880 - acc: 0.7183 - val_loss: 0.1904 - val_acc: 0.7107\n",
      "Epoch 381/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1880 - acc: 0.7184 - val_loss: 0.1892 - val_acc: 0.7144\n",
      "Epoch 382/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1879 - acc: 0.7190 - val_loss: 0.1876 - val_acc: 0.7175\n",
      "Epoch 383/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1880 - acc: 0.7187 - val_loss: 0.1880 - val_acc: 0.7199\n",
      "Epoch 384/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1880 - acc: 0.7194 - val_loss: 0.1870 - val_acc: 0.7208\n",
      "Epoch 385/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1877 - acc: 0.7192 - val_loss: 0.1870 - val_acc: 0.7213\n",
      "Epoch 386/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1878 - acc: 0.7209 - val_loss: 0.1905 - val_acc: 0.7164\n",
      "Epoch 387/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1878 - acc: 0.7190 - val_loss: 0.1879 - val_acc: 0.7158\n",
      "Epoch 388/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1877 - acc: 0.7196 - val_loss: 0.1878 - val_acc: 0.7174\n",
      "Epoch 389/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1876 - acc: 0.7195 - val_loss: 0.1868 - val_acc: 0.7220\n",
      "Epoch 390/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1877 - acc: 0.7203 - val_loss: 0.1870 - val_acc: 0.7214\n",
      "Epoch 391/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1877 - acc: 0.7195 - val_loss: 0.1901 - val_acc: 0.7116\n",
      "Epoch 392/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1876 - acc: 0.7208 - val_loss: 0.1869 - val_acc: 0.7171\n",
      "Epoch 393/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1876 - acc: 0.7196 - val_loss: 0.1886 - val_acc: 0.7127\n",
      "Epoch 394/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1877 - acc: 0.7190 - val_loss: 0.1873 - val_acc: 0.7234\n",
      "Epoch 395/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1877 - acc: 0.7215 - val_loss: 0.1891 - val_acc: 0.7140\n",
      "Epoch 396/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1876 - acc: 0.7187 - val_loss: 0.1876 - val_acc: 0.7183\n",
      "Epoch 397/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1877 - acc: 0.7195 - val_loss: 0.1866 - val_acc: 0.7169\n",
      "Epoch 398/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1876 - acc: 0.7199 - val_loss: 0.1887 - val_acc: 0.7152\n",
      "Epoch 399/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1877 - acc: 0.7211 - val_loss: 0.1869 - val_acc: 0.7236\n",
      "Epoch 400/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1875 - acc: 0.7205 - val_loss: 0.1865 - val_acc: 0.7289\n"
     ]
    }
   ],
   "source": [
    "#test 3\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='softmax'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2434 - acc: 0.5951 - val_loss: 0.2402 - val_acc: 0.6019\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2411 - acc: 0.5983 - val_loss: 0.2401 - val_acc: 0.6019\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2408 - acc: 0.5973 - val_loss: 0.2401 - val_acc: 0.6019\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2407 - acc: 0.5977 - val_loss: 0.2437 - val_acc: 0.6176\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2405 - acc: 0.5975 - val_loss: 0.2387 - val_acc: 0.6019\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2404 - acc: 0.5973 - val_loss: 0.2387 - val_acc: 0.6041\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2403 - acc: 0.5978 - val_loss: 0.2393 - val_acc: 0.6038\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2402 - acc: 0.5978 - val_loss: 0.2390 - val_acc: 0.6064\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2402 - acc: 0.5990 - val_loss: 0.2389 - val_acc: 0.6019\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2403 - acc: 0.5985 - val_loss: 0.2394 - val_acc: 0.6019\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2401 - acc: 0.5996 - val_loss: 0.2386 - val_acc: 0.6036\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2401 - acc: 0.5980 - val_loss: 0.2388 - val_acc: 0.6085\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.5997 - val_loss: 0.2385 - val_acc: 0.6024\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.5994 - val_loss: 0.2388 - val_acc: 0.6019\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.5991 - val_loss: 0.2396 - val_acc: 0.6172\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.6000 - val_loss: 0.2386 - val_acc: 0.6080\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2398 - acc: 0.6008 - val_loss: 0.2396 - val_acc: 0.6159\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2400 - acc: 0.6000 - val_loss: 0.2385 - val_acc: 0.6063\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2398 - acc: 0.6006 - val_loss: 0.2388 - val_acc: 0.6095\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2397 - acc: 0.6005 - val_loss: 0.2394 - val_acc: 0.6181\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2396 - acc: 0.6027 - val_loss: 0.2381 - val_acc: 0.6086\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2395 - acc: 0.6036 - val_loss: 0.2379 - val_acc: 0.6050\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2395 - acc: 0.6032 - val_loss: 0.2378 - val_acc: 0.6086\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2395 - acc: 0.6050 - val_loss: 0.2377 - val_acc: 0.6103\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2388 - acc: 0.6073 - val_loss: 0.2367 - val_acc: 0.6134\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2385 - acc: 0.6096 - val_loss: 0.2368 - val_acc: 0.6245\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2363 - acc: 0.6117 - val_loss: 0.2278 - val_acc: 0.6260\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2272 - acc: 0.6360 - val_loss: 0.2216 - val_acc: 0.6692\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2202 - acc: 0.6560 - val_loss: 0.2352 - val_acc: 0.5976\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2194 - acc: 0.6582 - val_loss: 0.2126 - val_acc: 0.6832\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2170 - acc: 0.6648 - val_loss: 0.2112 - val_acc: 0.6757\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2163 - acc: 0.6636 - val_loss: 0.2178 - val_acc: 0.6779\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2166 - acc: 0.6664 - val_loss: 0.2266 - val_acc: 0.6454\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2161 - acc: 0.6683 - val_loss: 0.2140 - val_acc: 0.6630\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2150 - acc: 0.6693 - val_loss: 0.2262 - val_acc: 0.6431\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2154 - acc: 0.6681 - val_loss: 0.2109 - val_acc: 0.6631\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2149 - acc: 0.6697 - val_loss: 0.2449 - val_acc: 0.5454\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2139 - acc: 0.6716 - val_loss: 0.2237 - val_acc: 0.6667\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2148 - acc: 0.6689 - val_loss: 0.2177 - val_acc: 0.6846\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2127 - acc: 0.6743 - val_loss: 0.2086 - val_acc: 0.6846\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2124 - acc: 0.6760 - val_loss: 0.2079 - val_acc: 0.6810\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2136 - acc: 0.6714 - val_loss: 0.2196 - val_acc: 0.6754\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2125 - acc: 0.6753 - val_loss: 0.2072 - val_acc: 0.6883\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2129 - acc: 0.6751 - val_loss: 0.2123 - val_acc: 0.6917\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2127 - acc: 0.6748 - val_loss: 0.2172 - val_acc: 0.6830\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2121 - acc: 0.6769 - val_loss: 0.2129 - val_acc: 0.6563\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2118 - acc: 0.6762 - val_loss: 0.2199 - val_acc: 0.6728\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2113 - acc: 0.6761 - val_loss: 0.2067 - val_acc: 0.6920\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2119 - acc: 0.6747 - val_loss: 0.2110 - val_acc: 0.6843\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2113 - acc: 0.6775 - val_loss: 0.2070 - val_acc: 0.6892\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2105 - acc: 0.6790 - val_loss: 0.2060 - val_acc: 0.6883\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2105 - acc: 0.6803 - val_loss: 0.2072 - val_acc: 0.6795\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2106 - acc: 0.6794 - val_loss: 0.2074 - val_acc: 0.6931\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2095 - acc: 0.6820 - val_loss: 0.2062 - val_acc: 0.6952\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2104 - acc: 0.6810 - val_loss: 0.2093 - val_acc: 0.6906\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2105 - acc: 0.6780 - val_loss: 0.2056 - val_acc: 0.6944\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2094 - acc: 0.6826 - val_loss: 0.2086 - val_acc: 0.6807\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2103 - acc: 0.680 - 1s 24us/sample - loss: 0.2104 - acc: 0.6799 - val_loss: 0.2056 - val_acc: 0.6888\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2100 - acc: 0.6797 - val_loss: 0.2122 - val_acc: 0.6944\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2097 - acc: 0.6816 - val_loss: 0.2073 - val_acc: 0.6815\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2091 - acc: 0.6818 - val_loss: 0.2061 - val_acc: 0.6855\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2097 - acc: 0.6796 - val_loss: 0.2079 - val_acc: 0.6913\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2093 - acc: 0.6808 - val_loss: 0.2046 - val_acc: 0.6886\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2086 - acc: 0.6821 - val_loss: 0.2201 - val_acc: 0.6650\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2089 - acc: 0.6840 - val_loss: 0.2213 - val_acc: 0.6555\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2097 - acc: 0.6804 - val_loss: 0.2086 - val_acc: 0.6883\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2085 - acc: 0.6831 - val_loss: 0.2043 - val_acc: 0.6928\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2086 - acc: 0.6826 - val_loss: 0.2064 - val_acc: 0.6760\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2081 - acc: 0.6845 - val_loss: 0.2081 - val_acc: 0.6948\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2078 - acc: 0.6854 - val_loss: 0.2045 - val_acc: 0.6955\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2083 - acc: 0.6829 - val_loss: 0.2041 - val_acc: 0.6950\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2079 - acc: 0.6858 - val_loss: 0.2091 - val_acc: 0.6734\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2085 - acc: 0.6831 - val_loss: 0.2149 - val_acc: 0.6589\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2088 - acc: 0.6821 - val_loss: 0.2047 - val_acc: 0.6897\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2083 - acc: 0.6831 - val_loss: 0.2045 - val_acc: 0.6933\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2081 - acc: 0.6838 - val_loss: 0.2050 - val_acc: 0.6877\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2083 - acc: 0.6836 - val_loss: 0.2045 - val_acc: 0.6969\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2084 - acc: 0.6828 - val_loss: 0.2072 - val_acc: 0.6717\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2080 - acc: 0.6836 - val_loss: 0.2121 - val_acc: 0.6711\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2084 - acc: 0.6839 - val_loss: 0.2041 - val_acc: 0.6922\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2073 - acc: 0.6863 - val_loss: 0.2067 - val_acc: 0.6765\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2082 - acc: 0.6843 - val_loss: 0.2051 - val_acc: 0.6837\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2074 - acc: 0.6872 - val_loss: 0.2079 - val_acc: 0.6757\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2073 - acc: 0.6848 - val_loss: 0.2047 - val_acc: 0.6939\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2075 - acc: 0.6853 - val_loss: 0.2063 - val_acc: 0.6934\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2083 - acc: 0.6846 - val_loss: 0.2037 - val_acc: 0.7035\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2080 - acc: 0.6842 - val_loss: 0.2055 - val_acc: 0.6961\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2067 - acc: 0.6881 - val_loss: 0.2033 - val_acc: 0.6948\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2075 - acc: 0.6864 - val_loss: 0.2048 - val_acc: 0.6821\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2071 - acc: 0.6857 - val_loss: 0.2135 - val_acc: 0.6972\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2075 - acc: 0.6859 - val_loss: 0.2043 - val_acc: 0.6958\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2067 - acc: 0.6864 - val_loss: 0.2035 - val_acc: 0.6911\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2076 - acc: 0.6859 - val_loss: 0.2048 - val_acc: 0.6832\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2070 - acc: 0.6868 - val_loss: 0.2051 - val_acc: 0.6832\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2077 - acc: 0.6844 - val_loss: 0.2039 - val_acc: 0.6987\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2069 - acc: 0.6857 - val_loss: 0.2035 - val_acc: 0.6976\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2063 - acc: 0.6873 - val_loss: 0.2065 - val_acc: 0.7023\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2069 - acc: 0.6871 - val_loss: 0.2028 - val_acc: 0.6962\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2069 - acc: 0.6874 - val_loss: 0.2072 - val_acc: 0.6992\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2065 - acc: 0.6869 - val_loss: 0.2031 - val_acc: 0.6939\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2065 - acc: 0.6878 - val_loss: 0.2057 - val_acc: 0.6816\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2068 - acc: 0.6885 - val_loss: 0.2214 - val_acc: 0.6591\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2066 - acc: 0.6874 - val_loss: 0.2031 - val_acc: 0.6956\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2065 - acc: 0.6880 - val_loss: 0.2058 - val_acc: 0.6973\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2063 - acc: 0.688 - 1s 24us/sample - loss: 0.2064 - acc: 0.6881 - val_loss: 0.2055 - val_acc: 0.6961\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2067 - acc: 0.6875 - val_loss: 0.2030 - val_acc: 0.6936\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2061 - acc: 0.6885 - val_loss: 0.2026 - val_acc: 0.6981\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2068 - acc: 0.6869 - val_loss: 0.2034 - val_acc: 0.6976\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2068 - acc: 0.6866 - val_loss: 0.2030 - val_acc: 0.6983\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2062 - acc: 0.6870 - val_loss: 0.2056 - val_acc: 0.6804\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2056 - acc: 0.6907 - val_loss: 0.2041 - val_acc: 0.7032\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2060 - acc: 0.6888 - val_loss: 0.2101 - val_acc: 0.6694\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2063 - acc: 0.6884 - val_loss: 0.2028 - val_acc: 0.6953\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2069 - acc: 0.6877 - val_loss: 0.2043 - val_acc: 0.6938\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2062 - acc: 0.6889 - val_loss: 0.2029 - val_acc: 0.6936\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2060 - acc: 0.6873 - val_loss: 0.2024 - val_acc: 0.7018\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2062 - acc: 0.6891 - val_loss: 0.2044 - val_acc: 0.6829\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2065 - acc: 0.6870 - val_loss: 0.2026 - val_acc: 0.7004\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2055 - acc: 0.6895 - val_loss: 0.2033 - val_acc: 0.7074\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2061 - acc: 0.6890 - val_loss: 0.2026 - val_acc: 0.6955\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2057 - acc: 0.6898 - val_loss: 0.2030 - val_acc: 0.6948\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6889 - val_loss: 0.2132 - val_acc: 0.6851\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2058 - acc: 0.6897 - val_loss: 0.2125 - val_acc: 0.6866\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2060 - acc: 0.6893 - val_loss: 0.2103 - val_acc: 0.6933\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2060 - acc: 0.6878 - val_loss: 0.2093 - val_acc: 0.6704\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6893 - val_loss: 0.2029 - val_acc: 0.6945\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2059 - acc: 0.6890 - val_loss: 0.2029 - val_acc: 0.7001\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6901 - val_loss: 0.2080 - val_acc: 0.6785\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2060 - acc: 0.6888 - val_loss: 0.2026 - val_acc: 0.7021\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2056 - acc: 0.6899 - val_loss: 0.2022 - val_acc: 0.6976\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2055 - acc: 0.6908 - val_loss: 0.2024 - val_acc: 0.6969\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2056 - acc: 0.6889 - val_loss: 0.2025 - val_acc: 0.7021\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2060 - acc: 0.6896 - val_loss: 0.2055 - val_acc: 0.6851\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2055 - acc: 0.6899 - val_loss: 0.2059 - val_acc: 0.7003\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2053 - acc: 0.6905 - val_loss: 0.2021 - val_acc: 0.6976\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2052 - acc: 0.6906 - val_loss: 0.2020 - val_acc: 0.6979\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2052 - acc: 0.6909 - val_loss: 0.2025 - val_acc: 0.6967\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2058 - acc: 0.6904 - val_loss: 0.2219 - val_acc: 0.6583\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2054 - acc: 0.6902 - val_loss: 0.2045 - val_acc: 0.6813\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2053 - acc: 0.6908 - val_loss: 0.2021 - val_acc: 0.6989\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2050 - acc: 0.6916 - val_loss: 0.2041 - val_acc: 0.7000\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2055 - acc: 0.6891 - val_loss: 0.2036 - val_acc: 0.7035\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2049 - acc: 0.6909 - val_loss: 0.2050 - val_acc: 0.6992\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2053 - acc: 0.6900 - val_loss: 0.2045 - val_acc: 0.6830\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2050 - acc: 0.6902 - val_loss: 0.2033 - val_acc: 0.6875\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2047 - acc: 0.6902 - val_loss: 0.2031 - val_acc: 0.6892\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2054 - acc: 0.6903 - val_loss: 0.2022 - val_acc: 0.6964\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.6916 - val_loss: 0.2054 - val_acc: 0.7029\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2051 - acc: 0.6919 - val_loss: 0.2094 - val_acc: 0.6905\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2056 - acc: 0.6891 - val_loss: 0.2044 - val_acc: 0.7031\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2051 - acc: 0.6898 - val_loss: 0.2091 - val_acc: 0.6967\n"
     ]
    }
   ],
   "source": [
    "#test 4\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='softplus'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2444 - acc: 0.5935 - val_loss: 0.2387 - val_acc: 0.6033\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2404 - acc: 0.5978 - val_loss: 0.2393 - val_acc: 0.6075\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2399 - acc: 0.6009 - val_loss: 0.2375 - val_acc: 0.6035\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2393 - acc: 0.6031 - val_loss: 0.2365 - val_acc: 0.6058\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2379 - acc: 0.6053 - val_loss: 0.2330 - val_acc: 0.6181\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2328 - acc: 0.6150 - val_loss: 0.2300 - val_acc: 0.6475\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2247 - acc: 0.6386 - val_loss: 0.2170 - val_acc: 0.6569\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2185 - acc: 0.6517 - val_loss: 0.2141 - val_acc: 0.6765\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2156 - acc: 0.6618 - val_loss: 0.2135 - val_acc: 0.6603\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2140 - acc: 0.6661 - val_loss: 0.2098 - val_acc: 0.6695\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2114 - acc: 0.6723 - val_loss: 0.2077 - val_acc: 0.6767\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2110 - acc: 0.6732 - val_loss: 0.2062 - val_acc: 0.6804\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2107 - acc: 0.6742 - val_loss: 0.2066 - val_acc: 0.6776\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2104 - acc: 0.6763 - val_loss: 0.2095 - val_acc: 0.6759\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 2s 42us/sample - loss: 0.2081 - acc: 0.6817 - val_loss: 0.2065 - val_acc: 0.6756\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2077 - acc: 0.6829 - val_loss: 0.2038 - val_acc: 0.6896\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2070 - acc: 0.6820 - val_loss: 0.2042 - val_acc: 0.6877\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2061 - acc: 0.6867 - val_loss: 0.2040 - val_acc: 0.6962\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2056 - acc: 0.6882 - val_loss: 0.2026 - val_acc: 0.6812\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2058 - acc: 0.6864 - val_loss: 0.2090 - val_acc: 0.6835\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2042 - acc: 0.6887 - val_loss: 0.2000 - val_acc: 0.6938\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2050 - acc: 0.6881 - val_loss: 0.2006 - val_acc: 0.6939\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2033 - acc: 0.6900 - val_loss: 0.2018 - val_acc: 0.6933\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2043 - acc: 0.6880 - val_loss: 0.2070 - val_acc: 0.6861\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2043 - acc: 0.6868 - val_loss: 0.2041 - val_acc: 0.6900\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2027 - acc: 0.6923 - val_loss: 0.2055 - val_acc: 0.6899\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2024 - acc: 0.6917 - val_loss: 0.2056 - val_acc: 0.6914\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2031 - acc: 0.6903 - val_loss: 0.2021 - val_acc: 0.6942\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2035 - acc: 0.6910 - val_loss: 0.1989 - val_acc: 0.6997\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2025 - acc: 0.6911 - val_loss: 0.1981 - val_acc: 0.7012\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2024 - acc: 0.6928 - val_loss: 0.2093 - val_acc: 0.6807\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2023 - acc: 0.6922 - val_loss: 0.1972 - val_acc: 0.7004\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2017 - acc: 0.6928 - val_loss: 0.2079 - val_acc: 0.6805\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2015 - acc: 0.6931 - val_loss: 0.2042 - val_acc: 0.6911\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2016 - acc: 0.6936 - val_loss: 0.1975 - val_acc: 0.6952\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2015 - acc: 0.6947 - val_loss: 0.2077 - val_acc: 0.6852\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2018 - acc: 0.6921 - val_loss: 0.2019 - val_acc: 0.6950\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1999 - acc: 0.6963 - val_loss: 0.1966 - val_acc: 0.6989\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2014 - acc: 0.6935 - val_loss: 0.1986 - val_acc: 0.6978\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2003 - acc: 0.6935 - val_loss: 0.1965 - val_acc: 0.7034\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2003 - acc: 0.6938 - val_loss: 0.1966 - val_acc: 0.6956\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1991 - acc: 0.6973 - val_loss: 0.1998 - val_acc: 0.7029\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1996 - acc: 0.6954 - val_loss: 0.1957 - val_acc: 0.7018\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2002 - acc: 0.6960 - val_loss: 0.1949 - val_acc: 0.7029\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1996 - acc: 0.6967 - val_loss: 0.1955 - val_acc: 0.7059\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1992 - acc: 0.6966 - val_loss: 0.2048 - val_acc: 0.6944\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1985 - acc: 0.6971 - val_loss: 0.1957 - val_acc: 0.7046\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1988 - acc: 0.6974 - val_loss: 0.2251 - val_acc: 0.6287\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1980 - acc: 0.6988 - val_loss: 0.1991 - val_acc: 0.7040\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1986 - acc: 0.6972 - val_loss: 0.1937 - val_acc: 0.7011\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1986 - acc: 0.6983 - val_loss: 0.2017 - val_acc: 0.6959\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1984 - acc: 0.6990 - val_loss: 0.2027 - val_acc: 0.6938\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1988 - acc: 0.6968 - val_loss: 0.1938 - val_acc: 0.7046\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1974 - acc: 0.6996 - val_loss: 0.1955 - val_acc: 0.7039\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1985 - acc: 0.6973 - val_loss: 0.1932 - val_acc: 0.7042\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1975 - acc: 0.7008 - val_loss: 0.1944 - val_acc: 0.7107\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1986 - acc: 0.6986 - val_loss: 0.1957 - val_acc: 0.7076\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1986 - acc: 0.6960 - val_loss: 0.1961 - val_acc: 0.7011\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1975 - acc: 0.6998 - val_loss: 0.1952 - val_acc: 0.7105\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1975 - acc: 0.7018 - val_loss: 0.1979 - val_acc: 0.7039\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1971 - acc: 0.7017 - val_loss: 0.2004 - val_acc: 0.7034\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1964 - acc: 0.7009 - val_loss: 0.1927 - val_acc: 0.7073\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1966 - acc: 0.7012 - val_loss: 0.1917 - val_acc: 0.7049\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1962 - acc: 0.7024 - val_loss: 0.1921 - val_acc: 0.7062\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1964 - acc: 0.7031 - val_loss: 0.1940 - val_acc: 0.7126\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1964 - acc: 0.7016 - val_loss: 0.1956 - val_acc: 0.7060\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1951 - acc: 0.7044 - val_loss: 0.1911 - val_acc: 0.7080\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1956 - acc: 0.7029 - val_loss: 0.1914 - val_acc: 0.7049\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1961 - acc: 0.7034 - val_loss: 0.1924 - val_acc: 0.7150\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1955 - acc: 0.7017 - val_loss: 0.1946 - val_acc: 0.7059\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1964 - acc: 0.7009 - val_loss: 0.1923 - val_acc: 0.7099\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1954 - acc: 0.7035 - val_loss: 0.1907 - val_acc: 0.7067\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1958 - acc: 0.7014 - val_loss: 0.1915 - val_acc: 0.7129\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1952 - acc: 0.7038 - val_loss: 0.1910 - val_acc: 0.7042\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1962 - acc: 0.7018 - val_loss: 0.1908 - val_acc: 0.7048\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1957 - acc: 0.7023 - val_loss: 0.1986 - val_acc: 0.7034\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1961 - acc: 0.7023 - val_loss: 0.1911 - val_acc: 0.7074\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1944 - acc: 0.7060 - val_loss: 0.1926 - val_acc: 0.7164\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1945 - acc: 0.7051 - val_loss: 0.1900 - val_acc: 0.7135\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1951 - acc: 0.7029 - val_loss: 0.1946 - val_acc: 0.7029\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1951 - acc: 0.7036 - val_loss: 0.1906 - val_acc: 0.7135\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1942 - acc: 0.7062 - val_loss: 0.1982 - val_acc: 0.7067\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1955 - acc: 0.7023 - val_loss: 0.1905 - val_acc: 0.7085\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1948 - acc: 0.7043 - val_loss: 0.1943 - val_acc: 0.7074\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1955 - acc: 0.7029 - val_loss: 0.1991 - val_acc: 0.6998\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1946 - acc: 0.7046 - val_loss: 0.1902 - val_acc: 0.7138\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1934 - acc: 0.7057 - val_loss: 0.2054 - val_acc: 0.6952\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1937 - acc: 0.7050 - val_loss: 0.1894 - val_acc: 0.7126\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1952 - acc: 0.7042 - val_loss: 0.1895 - val_acc: 0.7132\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1945 - acc: 0.7062 - val_loss: 0.2003 - val_acc: 0.6970\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1940 - acc: 0.7045 - val_loss: 0.1893 - val_acc: 0.7063\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1936 - acc: 0.7050 - val_loss: 0.1921 - val_acc: 0.7048\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1938 - acc: 0.7050 - val_loss: 0.1927 - val_acc: 0.7060\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1941 - acc: 0.7041 - val_loss: 0.1892 - val_acc: 0.7085\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1939 - acc: 0.7057 - val_loss: 0.1892 - val_acc: 0.7054\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1940 - acc: 0.7058 - val_loss: 0.1991 - val_acc: 0.7034\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1936 - acc: 0.7051 - val_loss: 0.1897 - val_acc: 0.7208\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1931 - acc: 0.7061 - val_loss: 0.1924 - val_acc: 0.7040\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1927 - acc: 0.7080 - val_loss: 0.1995 - val_acc: 0.7056\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1939 - acc: 0.7052 - val_loss: 0.1909 - val_acc: 0.7080\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1932 - acc: 0.7076 - val_loss: 0.1901 - val_acc: 0.7130\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1931 - acc: 0.7067 - val_loss: 0.1968 - val_acc: 0.7037\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1932 - acc: 0.7065 - val_loss: 0.1887 - val_acc: 0.7183\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1934 - acc: 0.7070 - val_loss: 0.1890 - val_acc: 0.7076\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1923 - acc: 0.7093 - val_loss: 0.1899 - val_acc: 0.7101\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1936 - acc: 0.7055 - val_loss: 0.1882 - val_acc: 0.7154\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1928 - acc: 0.7079 - val_loss: 0.1883 - val_acc: 0.7091\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1925 - acc: 0.7075 - val_loss: 0.1912 - val_acc: 0.7046\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1928 - acc: 0.7075 - val_loss: 0.1890 - val_acc: 0.7070\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1932 - acc: 0.7067 - val_loss: 0.2028 - val_acc: 0.6992\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1927 - acc: 0.7062 - val_loss: 0.1952 - val_acc: 0.7040\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1921 - acc: 0.7077 - val_loss: 0.1883 - val_acc: 0.7171\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1925 - acc: 0.7081 - val_loss: 0.1899 - val_acc: 0.7174\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1929 - acc: 0.7070 - val_loss: 0.1902 - val_acc: 0.7094\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1920 - acc: 0.7080 - val_loss: 0.1911 - val_acc: 0.7147\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1919 - acc: 0.7081 - val_loss: 0.1877 - val_acc: 0.7138\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1927 - acc: 0.7078 - val_loss: 0.1888 - val_acc: 0.7222\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1919 - acc: 0.7094 - val_loss: 0.1944 - val_acc: 0.7127\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1919 - acc: 0.7076 - val_loss: 0.1877 - val_acc: 0.7133\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1915 - acc: 0.7101 - val_loss: 0.1877 - val_acc: 0.7178\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1912 - acc: 0.7077 - val_loss: 0.1915 - val_acc: 0.7035\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1911 - acc: 0.7095 - val_loss: 0.1878 - val_acc: 0.7160\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1909 - acc: 0.7098 - val_loss: 0.1964 - val_acc: 0.6990\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1924 - acc: 0.7080 - val_loss: 0.1889 - val_acc: 0.7096\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1911 - acc: 0.7103 - val_loss: 0.1871 - val_acc: 0.7183\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1913 - acc: 0.7106 - val_loss: 0.1869 - val_acc: 0.7140\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1909 - acc: 0.7102 - val_loss: 0.1999 - val_acc: 0.6916\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1899 - acc: 0.7123 - val_loss: 0.1865 - val_acc: 0.7222\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1907 - acc: 0.7106 - val_loss: 0.1910 - val_acc: 0.7110\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1911 - acc: 0.7091 - val_loss: 0.1864 - val_acc: 0.7181\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1917 - acc: 0.7106 - val_loss: 0.1914 - val_acc: 0.7171\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1900 - acc: 0.7119 - val_loss: 0.1866 - val_acc: 0.7158\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1910 - acc: 0.7108 - val_loss: 0.1930 - val_acc: 0.7084\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1908 - acc: 0.7102 - val_loss: 0.1896 - val_acc: 0.7144\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1906 - acc: 0.7112 - val_loss: 0.1928 - val_acc: 0.7112\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1906 - acc: 0.7116 - val_loss: 0.1893 - val_acc: 0.7195\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1905 - acc: 0.7106 - val_loss: 0.1911 - val_acc: 0.7136\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1907 - acc: 0.7108 - val_loss: 0.1918 - val_acc: 0.7116\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7114 - val_loss: 0.1863 - val_acc: 0.7203\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1905 - acc: 0.7110 - val_loss: 0.1880 - val_acc: 0.7185\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1902 - acc: 0.7123 - val_loss: 0.1880 - val_acc: 0.7213\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1897 - acc: 0.7120 - val_loss: 0.1893 - val_acc: 0.7079\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1899 - acc: 0.7122 - val_loss: 0.1865 - val_acc: 0.7164\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1891 - acc: 0.7144 - val_loss: 0.1881 - val_acc: 0.7177\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1901 - acc: 0.7118 - val_loss: 0.1967 - val_acc: 0.7076\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1896 - acc: 0.7116 - val_loss: 0.1866 - val_acc: 0.7219\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1894 - acc: 0.7137 - val_loss: 0.1879 - val_acc: 0.7192\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1894 - acc: 0.7136 - val_loss: 0.1879 - val_acc: 0.7164\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1895 - acc: 0.7145 - val_loss: 0.1880 - val_acc: 0.7256\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1897 - acc: 0.7123 - val_loss: 0.1902 - val_acc: 0.7140\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1902 - acc: 0.7123 - val_loss: 0.1864 - val_acc: 0.7143\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1895 - acc: 0.7131 - val_loss: 0.1884 - val_acc: 0.7163\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1892 - acc: 0.7130 - val_loss: 0.1879 - val_acc: 0.7161\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1893 - acc: 0.7143 - val_loss: 0.1899 - val_acc: 0.7152\n"
     ]
    }
   ],
   "source": [
    "#test 5\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='softsign'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.2438 - acc: 0.5930 - val_loss: 0.2429 - val_acc: 0.5850\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2408 - acc: 0.5959 - val_loss: 0.2392 - val_acc: 0.6030\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.2407 - acc: 0.5969 - val_loss: 0.2391 - val_acc: 0.6024\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2406 - acc: 0.5967 - val_loss: 0.2394 - val_acc: 0.6024\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2405 - acc: 0.5969 - val_loss: 0.2401 - val_acc: 0.6032\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2404 - acc: 0.5973 - val_loss: 0.2389 - val_acc: 0.6019\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2406 - acc: 0.5969 - val_loss: 0.2414 - val_acc: 0.6150\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2404 - acc: 0.5975 - val_loss: 0.2399 - val_acc: 0.6046\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2402 - acc: 0.5976 - val_loss: 0.2388 - val_acc: 0.6024\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2403 - acc: 0.5974 - val_loss: 0.2387 - val_acc: 0.6027\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2402 - acc: 0.5985 - val_loss: 0.2390 - val_acc: 0.6019\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2402 - acc: 0.5967 - val_loss: 0.2387 - val_acc: 0.6032\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2402 - acc: 0.5980 - val_loss: 0.2395 - val_acc: 0.6019\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2400 - acc: 0.5978 - val_loss: 0.2394 - val_acc: 0.6122\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2399 - acc: 0.5985 - val_loss: 0.2392 - val_acc: 0.6164\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2398 - acc: 0.6004 - val_loss: 0.2382 - val_acc: 0.6032\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2397 - acc: 0.6007 - val_loss: 0.2381 - val_acc: 0.6024\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2393 - acc: 0.6015 - val_loss: 0.2380 - val_acc: 0.6019\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2383 - acc: 0.6033 - val_loss: 0.2355 - val_acc: 0.6039\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2344 - acc: 0.6078 - val_loss: 0.2321 - val_acc: 0.6266\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2284 - acc: 0.6248 - val_loss: 0.2224 - val_acc: 0.6701\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2243 - acc: 0.6389 - val_loss: 0.2337 - val_acc: 0.6243\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2218 - acc: 0.6475 - val_loss: 0.2172 - val_acc: 0.6613\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2199 - acc: 0.6545 - val_loss: 0.2301 - val_acc: 0.5897\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2189 - acc: 0.6572 - val_loss: 0.2185 - val_acc: 0.6818\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2173 - acc: 0.6617 - val_loss: 0.2127 - val_acc: 0.6714\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2160 - acc: 0.6648 - val_loss: 0.2139 - val_acc: 0.6795\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2157 - acc: 0.6647 - val_loss: 0.2125 - val_acc: 0.6857\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2144 - acc: 0.6700 - val_loss: 0.2110 - val_acc: 0.6795\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2143 - acc: 0.6702 - val_loss: 0.2141 - val_acc: 0.6851\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2134 - acc: 0.6739 - val_loss: 0.2103 - val_acc: 0.6900\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2127 - acc: 0.6759 - val_loss: 0.2088 - val_acc: 0.6905\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2114 - acc: 0.6789 - val_loss: 0.2101 - val_acc: 0.6684\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2118 - acc: 0.6770 - val_loss: 0.2084 - val_acc: 0.6771\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2112 - acc: 0.6803 - val_loss: 0.2142 - val_acc: 0.6770\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2104 - acc: 0.6841 - val_loss: 0.2066 - val_acc: 0.6849\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2101 - acc: 0.6816 - val_loss: 0.2065 - val_acc: 0.6818\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2095 - acc: 0.6835 - val_loss: 0.2204 - val_acc: 0.6628\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2094 - acc: 0.6831 - val_loss: 0.2062 - val_acc: 0.6910\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2094 - acc: 0.6846 - val_loss: 0.2057 - val_acc: 0.6829\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2087 - acc: 0.6857 - val_loss: 0.2120 - val_acc: 0.6861\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2080 - acc: 0.6873 - val_loss: 0.2057 - val_acc: 0.7057\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2074 - acc: 0.6889 - val_loss: 0.2084 - val_acc: 0.6827\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2082 - acc: 0.6848 - val_loss: 0.2040 - val_acc: 0.7014\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2076 - acc: 0.6876 - val_loss: 0.2090 - val_acc: 0.6760\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2074 - acc: 0.6879 - val_loss: 0.2041 - val_acc: 0.6992\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2068 - acc: 0.6898 - val_loss: 0.2072 - val_acc: 0.6790\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2069 - acc: 0.6884 - val_loss: 0.2065 - val_acc: 0.6993\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2069 - acc: 0.6889 - val_loss: 0.2066 - val_acc: 0.7001\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2070 - acc: 0.6876 - val_loss: 0.2236 - val_acc: 0.6297\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2067 - acc: 0.6884 - val_loss: 0.2068 - val_acc: 0.6750\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2061 - acc: 0.6899 - val_loss: 0.2025 - val_acc: 0.6987\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2060 - acc: 0.6892 - val_loss: 0.2064 - val_acc: 0.6819\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2067 - acc: 0.6885 - val_loss: 0.2056 - val_acc: 0.6952\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2062 - acc: 0.6894 - val_loss: 0.2210 - val_acc: 0.6543\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2061 - acc: 0.6894 - val_loss: 0.2076 - val_acc: 0.6773\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2056 - acc: 0.6932 - val_loss: 0.2063 - val_acc: 0.6984\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2061 - acc: 0.6890 - val_loss: 0.2047 - val_acc: 0.6816\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2055 - acc: 0.6917 - val_loss: 0.2021 - val_acc: 0.6992\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2053 - acc: 0.6917 - val_loss: 0.2204 - val_acc: 0.6414\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2053 - acc: 0.6921 - val_loss: 0.2018 - val_acc: 0.7026\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2056 - acc: 0.6904 - val_loss: 0.2030 - val_acc: 0.7017\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2052 - acc: 0.6920 - val_loss: 0.2077 - val_acc: 0.6888\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2059 - acc: 0.6890 - val_loss: 0.2024 - val_acc: 0.6983\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2050 - acc: 0.6914 - val_loss: 0.2017 - val_acc: 0.6998\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2059 - acc: 0.6888 - val_loss: 0.2101 - val_acc: 0.6830\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2049 - acc: 0.6925 - val_loss: 0.2019 - val_acc: 0.7026\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2045 - acc: 0.6948 - val_loss: 0.2010 - val_acc: 0.6958\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.6916 - val_loss: 0.2023 - val_acc: 0.6979\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2043 - acc: 0.6940 - val_loss: 0.2013 - val_acc: 0.7032\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2046 - acc: 0.6923 - val_loss: 0.2068 - val_acc: 0.6903\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2048 - acc: 0.6940 - val_loss: 0.2055 - val_acc: 0.6970\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2050 - acc: 0.6923 - val_loss: 0.2011 - val_acc: 0.6997\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2051 - acc: 0.6919 - val_loss: 0.2133 - val_acc: 0.6781\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2049 - acc: 0.6925 - val_loss: 0.2047 - val_acc: 0.6837\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2044 - acc: 0.6929 - val_loss: 0.2041 - val_acc: 0.6916\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2052 - acc: 0.6916 - val_loss: 0.2044 - val_acc: 0.6990\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2044 - acc: 0.6937 - val_loss: 0.2026 - val_acc: 0.6958\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2041 - acc: 0.6949 - val_loss: 0.2011 - val_acc: 0.7037\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2041 - acc: 0.6940 - val_loss: 0.2018 - val_acc: 0.7060\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2045 - acc: 0.6941 - val_loss: 0.2007 - val_acc: 0.7054\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2043 - acc: 0.6934 - val_loss: 0.2056 - val_acc: 0.6931\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2046 - acc: 0.6938 - val_loss: 0.2014 - val_acc: 0.6924\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2046 - acc: 0.6922 - val_loss: 0.2013 - val_acc: 0.6998\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2043 - acc: 0.6938 - val_loss: 0.2006 - val_acc: 0.7028\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2039 - acc: 0.6935 - val_loss: 0.2011 - val_acc: 0.7031\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2042 - acc: 0.6929 - val_loss: 0.2014 - val_acc: 0.7034\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2042 - acc: 0.6940 - val_loss: 0.2018 - val_acc: 0.6981\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2042 - acc: 0.6943 - val_loss: 0.2049 - val_acc: 0.6880\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2041 - acc: 0.6930 - val_loss: 0.2090 - val_acc: 0.6878\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2036 - acc: 0.6957 - val_loss: 0.2004 - val_acc: 0.7015\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2038 - acc: 0.6949 - val_loss: 0.2006 - val_acc: 0.7003\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2038 - acc: 0.6941 - val_loss: 0.2017 - val_acc: 0.6976\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2044 - acc: 0.6925 - val_loss: 0.2002 - val_acc: 0.7062\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2031 - acc: 0.6958 - val_loss: 0.2010 - val_acc: 0.7009\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2034 - acc: 0.6952 - val_loss: 0.2082 - val_acc: 0.6930\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2039 - acc: 0.6943 - val_loss: 0.2000 - val_acc: 0.7054\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2034 - acc: 0.6956 - val_loss: 0.2025 - val_acc: 0.6930\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2034 - acc: 0.6951 - val_loss: 0.1999 - val_acc: 0.7073\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2036 - acc: 0.6952 - val_loss: 0.2028 - val_acc: 0.6917\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2032 - acc: 0.6968 - val_loss: 0.1999 - val_acc: 0.7060\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2038 - acc: 0.6947 - val_loss: 0.2005 - val_acc: 0.7023\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2033 - acc: 0.6966 - val_loss: 0.1998 - val_acc: 0.7026\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2033 - acc: 0.6959 - val_loss: 0.2003 - val_acc: 0.7025\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2031 - acc: 0.6948 - val_loss: 0.1997 - val_acc: 0.7077\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2030 - acc: 0.6960 - val_loss: 0.1996 - val_acc: 0.7026\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2035 - acc: 0.6945 - val_loss: 0.2006 - val_acc: 0.6975\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2034 - acc: 0.6955 - val_loss: 0.2023 - val_acc: 0.7018\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2029 - acc: 0.6966 - val_loss: 0.2001 - val_acc: 0.7011\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2033 - acc: 0.6964 - val_loss: 0.2035 - val_acc: 0.6939\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2032 - acc: 0.6962 - val_loss: 0.1999 - val_acc: 0.7077\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2033 - acc: 0.6957 - val_loss: 0.2014 - val_acc: 0.6993\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2030 - acc: 0.6953 - val_loss: 0.1996 - val_acc: 0.7043\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2030 - acc: 0.6974 - val_loss: 0.2004 - val_acc: 0.7045\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 2s 43us/sample - loss: 0.2031 - acc: 0.6961 - val_loss: 0.2024 - val_acc: 0.6986\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2029 - acc: 0.6974 - val_loss: 0.2003 - val_acc: 0.7037\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2030 - acc: 0.6964 - val_loss: 0.2023 - val_acc: 0.6955\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2027 - acc: 0.6963 - val_loss: 0.2020 - val_acc: 0.6984\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2034 - acc: 0.6967 - val_loss: 0.2001 - val_acc: 0.7042\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2025 - acc: 0.6980 - val_loss: 0.1996 - val_acc: 0.7056\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2026 - acc: 0.6980 - val_loss: 0.2001 - val_acc: 0.7021\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2026 - acc: 0.6980 - val_loss: 0.2023 - val_acc: 0.6928\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2025 - acc: 0.6970 - val_loss: 0.2041 - val_acc: 0.6927\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2028 - acc: 0.6964 - val_loss: 0.2021 - val_acc: 0.7042\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2025 - acc: 0.6984 - val_loss: 0.2009 - val_acc: 0.7001\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2024 - acc: 0.6975 - val_loss: 0.2000 - val_acc: 0.6986\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2027 - acc: 0.6967 - val_loss: 0.1992 - val_acc: 0.7046\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2025 - acc: 0.6985 - val_loss: 0.1998 - val_acc: 0.7060\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2025 - acc: 0.6966 - val_loss: 0.1992 - val_acc: 0.7051\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2025 - acc: 0.6973 - val_loss: 0.1997 - val_acc: 0.7039\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2022 - acc: 0.6977 - val_loss: 0.1994 - val_acc: 0.7068\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2024 - acc: 0.6978 - val_loss: 0.2008 - val_acc: 0.6975\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2020 - acc: 0.6968 - val_loss: 0.1990 - val_acc: 0.7077\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2025 - acc: 0.6968 - val_loss: 0.1994 - val_acc: 0.7035\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2026 - acc: 0.6968 - val_loss: 0.2023 - val_acc: 0.6961\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2023 - acc: 0.6980 - val_loss: 0.2060 - val_acc: 0.6967\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2021 - acc: 0.6981 - val_loss: 0.2010 - val_acc: 0.7006\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2020 - acc: 0.6982 - val_loss: 0.2056 - val_acc: 0.6888\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2020 - acc: 0.6983 - val_loss: 0.2029 - val_acc: 0.6961\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2020 - acc: 0.7000 - val_loss: 0.1989 - val_acc: 0.7014\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2022 - acc: 0.6986 - val_loss: 0.2032 - val_acc: 0.6955\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2019 - acc: 0.6982 - val_loss: 0.2010 - val_acc: 0.7035\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2023 - acc: 0.6981 - val_loss: 0.1998 - val_acc: 0.7046\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2018 - acc: 0.6998 - val_loss: 0.2019 - val_acc: 0.6972\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2020 - acc: 0.6995 - val_loss: 0.1988 - val_acc: 0.7037\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2023 - acc: 0.6969 - val_loss: 0.1994 - val_acc: 0.7053\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2022 - acc: 0.6987 - val_loss: 0.1996 - val_acc: 0.7046\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2014 - acc: 0.6987 - val_loss: 0.1987 - val_acc: 0.7080\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2018 - acc: 0.6987 - val_loss: 0.2000 - val_acc: 0.7021\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2021 - acc: 0.697 - 1s 23us/sample - loss: 0.2020 - acc: 0.6978 - val_loss: 0.2007 - val_acc: 0.7034\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2018 - acc: 0.6988 - val_loss: 0.1995 - val_acc: 0.7037\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2016 - acc: 0.6986 - val_loss: 0.1991 - val_acc: 0.7060\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2015 - acc: 0.6987 - val_loss: 0.1985 - val_acc: 0.7020\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2016 - acc: 0.698 - 1s 24us/sample - loss: 0.2017 - acc: 0.6985 - val_loss: 0.2014 - val_acc: 0.6993\n",
      "Epoch 155/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2016 - acc: 0.6990 - val_loss: 0.1993 - val_acc: 0.7068\n",
      "Epoch 156/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2019 - acc: 0.6970 - val_loss: 0.1991 - val_acc: 0.7085\n",
      "Epoch 157/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2015 - acc: 0.6996 - val_loss: 0.2018 - val_acc: 0.6952\n",
      "Epoch 158/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2012 - acc: 0.6999 - val_loss: 0.1985 - val_acc: 0.7062\n",
      "Epoch 159/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2017 - acc: 0.6985 - val_loss: 0.1993 - val_acc: 0.7037\n",
      "Epoch 160/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2014 - acc: 0.6993 - val_loss: 0.2006 - val_acc: 0.7034\n",
      "Epoch 161/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2015 - acc: 0.6988 - val_loss: 0.2001 - val_acc: 0.7020\n",
      "Epoch 162/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2014 - acc: 0.6998 - val_loss: 0.1987 - val_acc: 0.7001\n",
      "Epoch 163/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2013 - acc: 0.6998 - val_loss: 0.1986 - val_acc: 0.7021\n",
      "Epoch 164/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2012 - acc: 0.6996 - val_loss: 0.1997 - val_acc: 0.7032\n",
      "Epoch 165/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2014 - acc: 0.6995 - val_loss: 0.2005 - val_acc: 0.7011\n",
      "Epoch 166/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2013 - acc: 0.6989 - val_loss: 0.1984 - val_acc: 0.7071\n",
      "Epoch 167/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2015 - acc: 0.6997 - val_loss: 0.1984 - val_acc: 0.7031\n",
      "Epoch 168/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2011 - acc: 0.6992 - val_loss: 0.2060 - val_acc: 0.6880\n",
      "Epoch 169/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2012 - acc: 0.6997 - val_loss: 0.2000 - val_acc: 0.7018\n",
      "Epoch 170/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2011 - acc: 0.6998 - val_loss: 0.2009 - val_acc: 0.7039\n",
      "Epoch 171/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2010 - acc: 0.7005 - val_loss: 0.1990 - val_acc: 0.6993\n",
      "Epoch 172/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2009 - acc: 0.7000 - val_loss: 0.1983 - val_acc: 0.7049\n",
      "Epoch 173/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2012 - acc: 0.7005 - val_loss: 0.2044 - val_acc: 0.6961\n",
      "Epoch 174/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2011 - acc: 0.7003 - val_loss: 0.1984 - val_acc: 0.7053\n",
      "Epoch 175/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2014 - acc: 0.6996 - val_loss: 0.1992 - val_acc: 0.7045\n",
      "Epoch 176/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2012 - acc: 0.6998 - val_loss: 0.1990 - val_acc: 0.7043\n",
      "Epoch 177/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2010 - acc: 0.7000 - val_loss: 0.1985 - val_acc: 0.7037\n",
      "Epoch 178/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2010 - acc: 0.6990 - val_loss: 0.2005 - val_acc: 0.7046\n",
      "Epoch 179/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2011 - acc: 0.6998 - val_loss: 0.1990 - val_acc: 0.7040\n",
      "Epoch 180/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2009 - acc: 0.7002 - val_loss: 0.1981 - val_acc: 0.7026\n",
      "Epoch 181/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2008 - acc: 0.7004 - val_loss: 0.2053 - val_acc: 0.6878\n",
      "Epoch 182/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2008 - acc: 0.7002 - val_loss: 0.1986 - val_acc: 0.6995\n",
      "Epoch 183/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2009 - acc: 0.6995 - val_loss: 0.2002 - val_acc: 0.7034\n",
      "Epoch 184/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2009 - acc: 0.7005 - val_loss: 0.1988 - val_acc: 0.7045\n",
      "Epoch 185/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2008 - acc: 0.6998 - val_loss: 0.1995 - val_acc: 0.7006\n",
      "Epoch 186/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2006 - acc: 0.7002 - val_loss: 0.1984 - val_acc: 0.7065\n",
      "Epoch 187/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2007 - acc: 0.6998 - val_loss: 0.1978 - val_acc: 0.7000\n",
      "Epoch 188/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2006 - acc: 0.7007 - val_loss: 0.1981 - val_acc: 0.7062\n",
      "Epoch 189/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2008 - acc: 0.6996 - val_loss: 0.1982 - val_acc: 0.7051\n",
      "Epoch 190/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2006 - acc: 0.7000 - val_loss: 0.1983 - val_acc: 0.7080\n",
      "Epoch 191/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2006 - acc: 0.7010 - val_loss: 0.1989 - val_acc: 0.7067\n",
      "Epoch 192/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2004 - acc: 0.6993 - val_loss: 0.1982 - val_acc: 0.7119\n",
      "Epoch 193/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2008 - acc: 0.7000 - val_loss: 0.2013 - val_acc: 0.7054\n",
      "Epoch 194/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2003 - acc: 0.7015 - val_loss: 0.1993 - val_acc: 0.6997\n",
      "Epoch 195/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2006 - acc: 0.7011 - val_loss: 0.1998 - val_acc: 0.7053\n",
      "Epoch 196/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2003 - acc: 0.7015 - val_loss: 0.1987 - val_acc: 0.7035\n",
      "Epoch 197/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2003 - acc: 0.7011 - val_loss: 0.2002 - val_acc: 0.6998\n",
      "Epoch 198/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2003 - acc: 0.7008 - val_loss: 0.1985 - val_acc: 0.7004\n",
      "Epoch 199/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2003 - acc: 0.7007 - val_loss: 0.1992 - val_acc: 0.7065\n",
      "Epoch 200/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2005 - acc: 0.7002 - val_loss: 0.1978 - val_acc: 0.7082\n",
      "Epoch 201/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2001 - acc: 0.7010 - val_loss: 0.1976 - val_acc: 0.7006\n",
      "Epoch 202/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2001 - acc: 0.7000 - val_loss: 0.2012 - val_acc: 0.7004\n",
      "Epoch 203/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2004 - acc: 0.7007 - val_loss: 0.1994 - val_acc: 0.7034\n",
      "Epoch 204/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2000 - acc: 0.7018 - val_loss: 0.1977 - val_acc: 0.7065\n",
      "Epoch 205/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2003 - acc: 0.7011 - val_loss: 0.1978 - val_acc: 0.7042\n",
      "Epoch 206/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2001 - acc: 0.7010 - val_loss: 0.1974 - val_acc: 0.7096\n",
      "Epoch 207/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2001 - acc: 0.7011 - val_loss: 0.1974 - val_acc: 0.7060\n",
      "Epoch 208/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2001 - acc: 0.7011 - val_loss: 0.1976 - val_acc: 0.7051\n",
      "Epoch 209/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1998 - acc: 0.7013 - val_loss: 0.1978 - val_acc: 0.7060\n",
      "Epoch 210/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2001 - acc: 0.7008 - val_loss: 0.1975 - val_acc: 0.7068\n",
      "Epoch 211/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1999 - acc: 0.7010 - val_loss: 0.1975 - val_acc: 0.7070\n",
      "Epoch 212/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2002 - acc: 0.7014 - val_loss: 0.1982 - val_acc: 0.7040\n",
      "Epoch 213/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1998 - acc: 0.7024 - val_loss: 0.1976 - val_acc: 0.7102\n",
      "Epoch 214/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2001 - acc: 0.7020 - val_loss: 0.2013 - val_acc: 0.7056\n",
      "Epoch 215/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1998 - acc: 0.7014 - val_loss: 0.1989 - val_acc: 0.7037\n",
      "Epoch 216/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2002 - acc: 0.6996 - val_loss: 0.2057 - val_acc: 0.6911\n",
      "Epoch 217/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.2002 - acc: 0.7012 - val_loss: 0.1974 - val_acc: 0.7087\n",
      "Epoch 218/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1998 - acc: 0.7023 - val_loss: 0.1974 - val_acc: 0.7068\n",
      "Epoch 219/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1998 - acc: 0.7019 - val_loss: 0.1982 - val_acc: 0.7028\n",
      "Epoch 220/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1997 - acc: 0.7003 - val_loss: 0.1998 - val_acc: 0.7040 0s - loss: 0.199\n",
      "Epoch 221/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1997 - acc: 0.7017 - val_loss: 0.1988 - val_acc: 0.7048\n",
      "Epoch 222/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1997 - acc: 0.7022 - val_loss: 0.1980 - val_acc: 0.7007\n",
      "Epoch 223/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1995 - acc: 0.7029 - val_loss: 0.1974 - val_acc: 0.7043\n",
      "Epoch 224/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1997 - acc: 0.7021 - val_loss: 0.1975 - val_acc: 0.7063\n",
      "Epoch 225/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1995 - acc: 0.7014 - val_loss: 0.1978 - val_acc: 0.7059\n",
      "Epoch 226/400\n",
      "57916/57916 [==============================] - 2s 38us/sample - loss: 0.1996 - acc: 0.7005 - val_loss: 0.1998 - val_acc: 0.7032\n",
      "Epoch 227/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1998 - acc: 0.7018 - val_loss: 0.1978 - val_acc: 0.7018\n",
      "Epoch 228/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1995 - acc: 0.7028 - val_loss: 0.1973 - val_acc: 0.7063\n",
      "Epoch 229/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1994 - acc: 0.7026 - val_loss: 0.1974 - val_acc: 0.7087\n",
      "Epoch 230/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1996 - acc: 0.7013 - val_loss: 0.1972 - val_acc: 0.7048\n",
      "Epoch 231/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1993 - acc: 0.7017 - val_loss: 0.1985 - val_acc: 0.7042\n",
      "Epoch 232/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1994 - acc: 0.7020 - val_loss: 0.1975 - val_acc: 0.7085\n",
      "Epoch 233/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1995 - acc: 0.7026 - val_loss: 0.1969 - val_acc: 0.7020\n",
      "Epoch 234/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1992 - acc: 0.7023 - val_loss: 0.1969 - val_acc: 0.7049\n",
      "Epoch 235/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1996 - acc: 0.7008 - val_loss: 0.1969 - val_acc: 0.7067\n",
      "Epoch 236/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1992 - acc: 0.7026 - val_loss: 0.1970 - val_acc: 0.7018\n",
      "Epoch 237/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1995 - acc: 0.7009 - val_loss: 0.1976 - val_acc: 0.7059\n",
      "Epoch 238/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1996 - acc: 0.7012 - val_loss: 0.1971 - val_acc: 0.7067\n",
      "Epoch 239/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1994 - acc: 0.7020 - val_loss: 0.1970 - val_acc: 0.7082\n",
      "Epoch 240/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1995 - acc: 0.7017 - val_loss: 0.1969 - val_acc: 0.7053\n",
      "Epoch 241/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1991 - acc: 0.7017 - val_loss: 0.1970 - val_acc: 0.7046\n",
      "Epoch 242/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1992 - acc: 0.7019 - val_loss: 0.1967 - val_acc: 0.7085\n",
      "Epoch 243/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1992 - acc: 0.7023 - val_loss: 0.1970 - val_acc: 0.7057\n",
      "Epoch 244/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1993 - acc: 0.7023 - val_loss: 0.1966 - val_acc: 0.7049\n",
      "Epoch 245/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1992 - acc: 0.7026 - val_loss: 0.1977 - val_acc: 0.7062\n",
      "Epoch 246/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1992 - acc: 0.7018 - val_loss: 0.1965 - val_acc: 0.7054\n",
      "Epoch 247/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1992 - acc: 0.7014 - val_loss: 0.1975 - val_acc: 0.7034\n",
      "Epoch 248/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1991 - acc: 0.7022 - val_loss: 0.1965 - val_acc: 0.7053\n",
      "Epoch 249/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1988 - acc: 0.7020 - val_loss: 0.1964 - val_acc: 0.7054\n",
      "Epoch 250/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1989 - acc: 0.7032 - val_loss: 0.1983 - val_acc: 0.7011\n",
      "Epoch 251/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1991 - acc: 0.7014 - val_loss: 0.1970 - val_acc: 0.7028\n",
      "Epoch 252/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1987 - acc: 0.7032 - val_loss: 0.1968 - val_acc: 0.7099\n",
      "Epoch 253/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1988 - acc: 0.7021 - val_loss: 0.1967 - val_acc: 0.7057\n",
      "Epoch 254/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1991 - acc: 0.7023 - val_loss: 0.1977 - val_acc: 0.7040\n",
      "Epoch 255/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1988 - acc: 0.7036 - val_loss: 0.1995 - val_acc: 0.7080\n",
      "Epoch 256/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1987 - acc: 0.7031 - val_loss: 0.1985 - val_acc: 0.7059\n",
      "Epoch 257/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1987 - acc: 0.7027 - val_loss: 0.1965 - val_acc: 0.7085\n",
      "Epoch 258/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1988 - acc: 0.7038 - val_loss: 0.1990 - val_acc: 0.7056\n",
      "Epoch 259/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1988 - acc: 0.7044 - val_loss: 0.1968 - val_acc: 0.7042\n",
      "Epoch 260/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1989 - acc: 0.7034 - val_loss: 0.1964 - val_acc: 0.7080\n",
      "Epoch 261/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1986 - acc: 0.7035 - val_loss: 0.1965 - val_acc: 0.7056\n",
      "Epoch 262/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1988 - acc: 0.7031 - val_loss: 0.1965 - val_acc: 0.7025\n",
      "Epoch 263/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1984 - acc: 0.7026 - val_loss: 0.1961 - val_acc: 0.7065\n",
      "Epoch 264/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1987 - acc: 0.7025 - val_loss: 0.1964 - val_acc: 0.7079\n",
      "Epoch 265/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1984 - acc: 0.7032 - val_loss: 0.1965 - val_acc: 0.7105\n",
      "Epoch 266/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1985 - acc: 0.7040 - val_loss: 0.1966 - val_acc: 0.7067\n",
      "Epoch 267/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1985 - acc: 0.7035 - val_loss: 0.1965 - val_acc: 0.7029\n",
      "Epoch 268/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1986 - acc: 0.7021 - val_loss: 0.1961 - val_acc: 0.7060\n",
      "Epoch 269/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1985 - acc: 0.7029 - val_loss: 0.1960 - val_acc: 0.7011\n",
      "Epoch 270/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1985 - acc: 0.7019 - val_loss: 0.1963 - val_acc: 0.7101\n",
      "Epoch 271/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1982 - acc: 0.7032 - val_loss: 0.1965 - val_acc: 0.7043\n",
      "Epoch 272/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1984 - acc: 0.7032 - val_loss: 0.1963 - val_acc: 0.7115\n",
      "Epoch 273/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1984 - acc: 0.7038 - val_loss: 0.1985 - val_acc: 0.7026\n",
      "Epoch 274/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1983 - acc: 0.7036 - val_loss: 0.1980 - val_acc: 0.7102\n",
      "Epoch 275/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1983 - acc: 0.7031 - val_loss: 0.1979 - val_acc: 0.7079\n",
      "Epoch 276/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1984 - acc: 0.7040 - val_loss: 0.1960 - val_acc: 0.7032\n",
      "Epoch 277/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1984 - acc: 0.7034 - val_loss: 0.1987 - val_acc: 0.7012\n",
      "Epoch 278/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1983 - acc: 0.7033 - val_loss: 0.1959 - val_acc: 0.7034\n",
      "Epoch 279/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1981 - acc: 0.7030 - val_loss: 0.1969 - val_acc: 0.7082\n",
      "Epoch 280/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1983 - acc: 0.7039 - val_loss: 0.1958 - val_acc: 0.7020\n",
      "Epoch 281/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1980 - acc: 0.7043 - val_loss: 0.2001 - val_acc: 0.7037\n",
      "Epoch 282/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1982 - acc: 0.7033 - val_loss: 0.1968 - val_acc: 0.7045\n",
      "Epoch 283/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1982 - acc: 0.7042 - val_loss: 0.1958 - val_acc: 0.7065\n",
      "Epoch 284/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1981 - acc: 0.7042 - val_loss: 0.1965 - val_acc: 0.7049\n",
      "Epoch 285/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1981 - acc: 0.7034 - val_loss: 0.1959 - val_acc: 0.7048\n",
      "Epoch 286/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1980 - acc: 0.7035 - val_loss: 0.1986 - val_acc: 0.7059\n",
      "Epoch 287/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1980 - acc: 0.7041 - val_loss: 0.1965 - val_acc: 0.7090\n",
      "Epoch 288/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1979 - acc: 0.7038 - val_loss: 0.1959 - val_acc: 0.7065\n",
      "Epoch 289/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.1979 - acc: 0.704 - 1s 23us/sample - loss: 0.1980 - acc: 0.7041 - val_loss: 0.1972 - val_acc: 0.7040\n",
      "Epoch 290/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1981 - acc: 0.7034 - val_loss: 0.1957 - val_acc: 0.7091\n",
      "Epoch 291/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1981 - acc: 0.7043 - val_loss: 0.1960 - val_acc: 0.7093\n",
      "Epoch 292/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1980 - acc: 0.7039 - val_loss: 0.1958 - val_acc: 0.7057\n",
      "Epoch 293/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1978 - acc: 0.7043 - val_loss: 0.1974 - val_acc: 0.7074\n",
      "Epoch 294/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1977 - acc: 0.7044 - val_loss: 0.1964 - val_acc: 0.7070\n",
      "Epoch 295/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1980 - acc: 0.7048 - val_loss: 0.1958 - val_acc: 0.7085\n",
      "Epoch 296/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1978 - acc: 0.7049 - val_loss: 0.1956 - val_acc: 0.7059\n",
      "Epoch 297/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1981 - acc: 0.7048 - val_loss: 0.1956 - val_acc: 0.7071\n",
      "Epoch 298/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1977 - acc: 0.7045 - val_loss: 0.1955 - val_acc: 0.7073\n",
      "Epoch 299/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1979 - acc: 0.7047 - val_loss: 0.1956 - val_acc: 0.7035\n",
      "Epoch 300/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1980 - acc: 0.7035 - val_loss: 0.1998 - val_acc: 0.7071 - loss: 0.1976 \n",
      "Epoch 301/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1978 - acc: 0.7039 - val_loss: 0.1958 - val_acc: 0.7080\n",
      "Epoch 302/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1977 - acc: 0.7048 - val_loss: 0.1956 - val_acc: 0.7068\n",
      "Epoch 303/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1976 - acc: 0.7045 - val_loss: 0.1961 - val_acc: 0.7065\n",
      "Epoch 304/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1979 - acc: 0.7041 - val_loss: 0.1982 - val_acc: 0.7116\n",
      "Epoch 305/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1976 - acc: 0.7048 - val_loss: 0.1974 - val_acc: 0.7046\n",
      "Epoch 306/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1978 - acc: 0.7033 - val_loss: 0.1955 - val_acc: 0.7067\n",
      "Epoch 307/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1976 - acc: 0.7050 - val_loss: 0.1957 - val_acc: 0.7059\n",
      "Epoch 308/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1976 - acc: 0.7040 - val_loss: 0.1954 - val_acc: 0.7034\n",
      "Epoch 309/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1976 - acc: 0.7048 - val_loss: 0.1965 - val_acc: 0.7080\n",
      "Epoch 310/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1976 - acc: 0.7045 - val_loss: 0.1969 - val_acc: 0.7015\n",
      "Epoch 311/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1975 - acc: 0.7045 - val_loss: 0.1956 - val_acc: 0.7054\n",
      "Epoch 312/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1975 - acc: 0.7050 - val_loss: 0.1957 - val_acc: 0.7020\n",
      "Epoch 313/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1975 - acc: 0.7038 - val_loss: 0.1964 - val_acc: 0.7110\n",
      "Epoch 314/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1975 - acc: 0.7050 - val_loss: 0.1953 - val_acc: 0.7079\n",
      "Epoch 315/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1977 - acc: 0.7038 - val_loss: 0.1954 - val_acc: 0.7068\n",
      "Epoch 316/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1973 - acc: 0.7046 - val_loss: 0.1954 - val_acc: 0.7067\n",
      "Epoch 317/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1973 - acc: 0.7048 - val_loss: 0.1958 - val_acc: 0.7048\n",
      "Epoch 318/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1972 - acc: 0.7051 - val_loss: 0.1956 - val_acc: 0.7090\n",
      "Epoch 319/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1973 - acc: 0.7047 - val_loss: 0.1961 - val_acc: 0.7068\n",
      "Epoch 320/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1973 - acc: 0.7048 - val_loss: 0.1957 - val_acc: 0.7076\n",
      "Epoch 321/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1973 - acc: 0.7046 - val_loss: 0.1963 - val_acc: 0.7087\n",
      "Epoch 322/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1973 - acc: 0.7055 - val_loss: 0.1960 - val_acc: 0.7034\n",
      "Epoch 323/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1975 - acc: 0.7047 - val_loss: 0.1959 - val_acc: 0.7094\n",
      "Epoch 324/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1973 - acc: 0.7043 - val_loss: 0.1957 - val_acc: 0.7107\n",
      "Epoch 325/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1973 - acc: 0.7060 - val_loss: 0.1962 - val_acc: 0.7119\n",
      "Epoch 326/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1973 - acc: 0.7043 - val_loss: 0.1965 - val_acc: 0.7073\n",
      "Epoch 327/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1972 - acc: 0.7048 - val_loss: 0.1950 - val_acc: 0.7093\n",
      "Epoch 328/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1972 - acc: 0.7042 - val_loss: 0.1951 - val_acc: 0.7090\n",
      "Epoch 329/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1971 - acc: 0.7042 - val_loss: 0.1953 - val_acc: 0.7080\n",
      "Epoch 330/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1972 - acc: 0.7056 - val_loss: 0.1951 - val_acc: 0.7067\n",
      "Epoch 331/400\n",
      "57916/57916 [==============================] - 1s 21us/sample - loss: 0.1971 - acc: 0.7044 - val_loss: 0.1952 - val_acc: 0.7074\n",
      "Epoch 332/400\n",
      "57916/57916 [==============================] - 1s 20us/sample - loss: 0.1973 - acc: 0.7034 - val_loss: 0.1966 - val_acc: 0.7113\n",
      "Epoch 333/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1971 - acc: 0.7049 - val_loss: 0.1957 - val_acc: 0.7068\n",
      "Epoch 334/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1970 - acc: 0.7049 - val_loss: 0.1959 - val_acc: 0.7012\n",
      "Epoch 335/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1970 - acc: 0.7044 - val_loss: 0.1951 - val_acc: 0.7056\n",
      "Epoch 336/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1970 - acc: 0.7052 - val_loss: 0.1968 - val_acc: 0.7057\n",
      "Epoch 337/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1970 - acc: 0.7051 - val_loss: 0.1967 - val_acc: 0.7127\n",
      "Epoch 338/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1972 - acc: 0.7040 - val_loss: 0.1951 - val_acc: 0.7032\n",
      "Epoch 339/400\n",
      "57916/57916 [==============================] - 1s 22us/sample - loss: 0.1971 - acc: 0.7047 - val_loss: 0.1957 - val_acc: 0.7053\n",
      "Epoch 340/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1969 - acc: 0.7047 - val_loss: 0.1955 - val_acc: 0.7062\n",
      "Epoch 341/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1969 - acc: 0.7053 - val_loss: 0.1957 - val_acc: 0.7105\n",
      "Epoch 342/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1969 - acc: 0.7058 - val_loss: 0.1956 - val_acc: 0.7040\n"
     ]
    }
   ],
   "source": [
    "#test 6\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='tanh'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 39us/sample - loss: 0.2450 - acc: 0.5867 - val_loss: 0.2384 - val_acc: 0.6019\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2405 - acc: 0.5980 - val_loss: 0.2415 - val_acc: 0.5923\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2402 - acc: 0.6006 - val_loss: 0.2379 - val_acc: 0.6049\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2397 - acc: 0.6011 - val_loss: 0.2372 - val_acc: 0.6038\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2385 - acc: 0.6062 - val_loss: 0.2364 - val_acc: 0.6269\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2363 - acc: 0.6104 - val_loss: 0.2316 - val_acc: 0.6027\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.2322 - acc: 0.6157 - val_loss: 0.2217 - val_acc: 0.6330\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2236 - acc: 0.6375 - val_loss: 0.2254 - val_acc: 0.6324\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2174 - acc: 0.6490 - val_loss: 0.2132 - val_acc: 0.6659\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2132 - acc: 0.6596 - val_loss: 0.2141 - val_acc: 0.6613\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2096 - acc: 0.6702 - val_loss: 0.2174 - val_acc: 0.6523\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2092 - acc: 0.6709 - val_loss: 0.2038 - val_acc: 0.6799\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2077 - acc: 0.6741 - val_loss: 0.2017 - val_acc: 0.6878\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2058 - acc: 0.6826 - val_loss: 0.2042 - val_acc: 0.6883\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2039 - acc: 0.6862 - val_loss: 0.2032 - val_acc: 0.6823\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2051 - acc: 0.6831 - val_loss: 0.2117 - val_acc: 0.6746\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2027 - acc: 0.6895 - val_loss: 0.2006 - val_acc: 0.6863\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2023 - acc: 0.6893 - val_loss: 0.1979 - val_acc: 0.6888\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2027 - acc: 0.6898 - val_loss: 0.1960 - val_acc: 0.6966\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2013 - acc: 0.6904 - val_loss: 0.1955 - val_acc: 0.7029\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1997 - acc: 0.6966 - val_loss: 0.1948 - val_acc: 0.6990\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.1998 - acc: 0.6966 - val_loss: 0.1988 - val_acc: 0.7054\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.1992 - acc: 0.6976 - val_loss: 0.2044 - val_acc: 0.6866\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.1996 - acc: 0.6980 - val_loss: 0.1937 - val_acc: 0.7026\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1985 - acc: 0.6985 - val_loss: 0.2021 - val_acc: 0.6993\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1984 - acc: 0.6984 - val_loss: 0.1990 - val_acc: 0.6962\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1975 - acc: 0.7000 - val_loss: 0.2050 - val_acc: 0.6872\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1972 - acc: 0.7021 - val_loss: 0.1940 - val_acc: 0.7012\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1967 - acc: 0.7030 - val_loss: 0.2172 - val_acc: 0.6461\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1968 - acc: 0.7016 - val_loss: 0.1960 - val_acc: 0.6983\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1969 - acc: 0.7034 - val_loss: 0.1951 - val_acc: 0.6989\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1976 - acc: 0.7008 - val_loss: 0.1919 - val_acc: 0.7070\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1966 - acc: 0.7031 - val_loss: 0.1921 - val_acc: 0.7068\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1966 - acc: 0.7029 - val_loss: 0.1937 - val_acc: 0.7177\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1964 - acc: 0.7033 - val_loss: 0.1956 - val_acc: 0.6955\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1963 - acc: 0.7045 - val_loss: 0.2004 - val_acc: 0.6888\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1979 - acc: 0.7009 - val_loss: 0.1932 - val_acc: 0.7141\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1957 - acc: 0.7054 - val_loss: 0.1974 - val_acc: 0.7150\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1955 - acc: 0.7043 - val_loss: 0.1912 - val_acc: 0.7057\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1958 - acc: 0.7063 - val_loss: 0.1925 - val_acc: 0.7034\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1969 - acc: 0.7036 - val_loss: 0.1989 - val_acc: 0.7119\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1955 - acc: 0.7045 - val_loss: 0.1936 - val_acc: 0.7225\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1952 - acc: 0.7050 - val_loss: 0.1900 - val_acc: 0.7127\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1946 - acc: 0.7083 - val_loss: 0.1956 - val_acc: 0.7115\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1947 - acc: 0.7078 - val_loss: 0.1904 - val_acc: 0.7178\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.1949 - acc: 0.7081 - val_loss: 0.1907 - val_acc: 0.7051\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 1s 23us/sample - loss: 0.1938 - acc: 0.7082 - val_loss: 0.1889 - val_acc: 0.7202\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1938 - acc: 0.7105 - val_loss: 0.1927 - val_acc: 0.7225\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1941 - acc: 0.7089 - val_loss: 0.1958 - val_acc: 0.6961\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1938 - acc: 0.7086 - val_loss: 0.1913 - val_acc: 0.7053\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1943 - acc: 0.7086 - val_loss: 0.1946 - val_acc: 0.6981\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1947 - acc: 0.7068 - val_loss: 0.1886 - val_acc: 0.7108\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1941 - acc: 0.7083 - val_loss: 0.1963 - val_acc: 0.6948\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1933 - acc: 0.7106 - val_loss: 0.1953 - val_acc: 0.6911\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1941 - acc: 0.7093 - val_loss: 0.2094 - val_acc: 0.6759\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1934 - acc: 0.7095 - val_loss: 0.1882 - val_acc: 0.7166\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1932 - acc: 0.7107 - val_loss: 0.1905 - val_acc: 0.7037\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1922 - acc: 0.7120 - val_loss: 0.2065 - val_acc: 0.6872\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1936 - acc: 0.7090 - val_loss: 0.1988 - val_acc: 0.7119\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1925 - acc: 0.7117 - val_loss: 0.1891 - val_acc: 0.7268\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1945 - acc: 0.7077 - val_loss: 0.1882 - val_acc: 0.7206\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1925 - acc: 0.7113 - val_loss: 0.1930 - val_acc: 0.6933\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1929 - acc: 0.7106 - val_loss: 0.1900 - val_acc: 0.7154\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1929 - acc: 0.7111 - val_loss: 0.1875 - val_acc: 0.7214\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1928 - acc: 0.7118 - val_loss: 0.1884 - val_acc: 0.7245\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1922 - acc: 0.7124 - val_loss: 0.1872 - val_acc: 0.7225\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1922 - acc: 0.7121 - val_loss: 0.1883 - val_acc: 0.7290\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1927 - acc: 0.7100 - val_loss: 0.1895 - val_acc: 0.7108\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1935 - acc: 0.7097 - val_loss: 0.1873 - val_acc: 0.7230\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1918 - acc: 0.7135 - val_loss: 0.1876 - val_acc: 0.7234\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1930 - acc: 0.7103 - val_loss: 0.1884 - val_acc: 0.7206\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1915 - acc: 0.7135 - val_loss: 0.1865 - val_acc: 0.7164\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1923 - acc: 0.7118 - val_loss: 0.1925 - val_acc: 0.7186\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1910 - acc: 0.7155 - val_loss: 0.1862 - val_acc: 0.7166\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1925 - acc: 0.7118 - val_loss: 0.2049 - val_acc: 0.6883\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1917 - acc: 0.7136 - val_loss: 0.1907 - val_acc: 0.7292\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1921 - acc: 0.7132 - val_loss: 0.1859 - val_acc: 0.7213\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1917 - acc: 0.7129 - val_loss: 0.1863 - val_acc: 0.7194\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1921 - acc: 0.7123 - val_loss: 0.2234 - val_acc: 0.6245\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1919 - acc: 0.7122 - val_loss: 0.1869 - val_acc: 0.7195\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1922 - acc: 0.7137 - val_loss: 0.1882 - val_acc: 0.7200\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1912 - acc: 0.7140 - val_loss: 0.1863 - val_acc: 0.7310\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1910 - acc: 0.7140 - val_loss: 0.1859 - val_acc: 0.7321\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1917 - acc: 0.7134 - val_loss: 0.1912 - val_acc: 0.7057\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1906 - acc: 0.7144 - val_loss: 0.1851 - val_acc: 0.7251\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1913 - acc: 0.7120 - val_loss: 0.1891 - val_acc: 0.7113\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1917 - acc: 0.7114 - val_loss: 0.1864 - val_acc: 0.7282\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1913 - acc: 0.7138 - val_loss: 0.1871 - val_acc: 0.7183\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1907 - acc: 0.7161 - val_loss: 0.1935 - val_acc: 0.7197\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1908 - acc: 0.7148 - val_loss: 0.1856 - val_acc: 0.7237\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1917 - acc: 0.7139 - val_loss: 0.1861 - val_acc: 0.7261\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1906 - acc: 0.7149 - val_loss: 0.1864 - val_acc: 0.7228\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1905 - acc: 0.7146 - val_loss: 0.1848 - val_acc: 0.7267\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1908 - acc: 0.7128 - val_loss: 0.1844 - val_acc: 0.7248ss: 0.1\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1910 - acc: 0.7134 - val_loss: 0.1860 - val_acc: 0.7321\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1898 - acc: 0.7162 - val_loss: 0.1865 - val_acc: 0.7237\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1910 - acc: 0.7150 - val_loss: 0.1847 - val_acc: 0.7289\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1898 - acc: 0.7184 - val_loss: 0.1859 - val_acc: 0.7286\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1906 - acc: 0.7138 - val_loss: 0.1843 - val_acc: 0.7258\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1897 - acc: 0.7172 - val_loss: 0.1889 - val_acc: 0.7074\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1898 - acc: 0.7164 - val_loss: 0.1868 - val_acc: 0.7127\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1906 - acc: 0.7155 - val_loss: 0.1904 - val_acc: 0.7053\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1907 - acc: 0.7149 - val_loss: 0.1841 - val_acc: 0.7338\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1896 - acc: 0.7173 - val_loss: 0.1868 - val_acc: 0.7282\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1908 - acc: 0.7141 - val_loss: 0.1844 - val_acc: 0.7290\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1892 - acc: 0.7170 - val_loss: 0.1845 - val_acc: 0.7245\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1907 - acc: 0.7142 - val_loss: 0.1855 - val_acc: 0.7304\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1893 - acc: 0.7174 - val_loss: 0.1841 - val_acc: 0.7262\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1897 - acc: 0.7156 - val_loss: 0.1858 - val_acc: 0.7278\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1898 - acc: 0.7162 - val_loss: 0.1890 - val_acc: 0.7234\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1893 - acc: 0.7168 - val_loss: 0.2046 - val_acc: 0.6871\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1911 - acc: 0.7120 - val_loss: 0.1849 - val_acc: 0.7203\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1897 - acc: 0.7165 - val_loss: 0.1865 - val_acc: 0.7200\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7144 - val_loss: 0.1851 - val_acc: 0.7292\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1889 - acc: 0.7172 - val_loss: 0.1843 - val_acc: 0.7203\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1902 - acc: 0.7146 - val_loss: 0.1845 - val_acc: 0.7251\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1894 - acc: 0.7157 - val_loss: 0.1843 - val_acc: 0.7275\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1886 - acc: 0.7191 - val_loss: 0.1835 - val_acc: 0.7261\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1882 - acc: 0.7169 - val_loss: 0.1928 - val_acc: 0.7183\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1894 - acc: 0.7162 - val_loss: 0.1834 - val_acc: 0.7203\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1898 - acc: 0.7155 - val_loss: 0.1838 - val_acc: 0.7181\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1887 - acc: 0.7175 - val_loss: 0.1852 - val_acc: 0.7155\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1895 - acc: 0.7176 - val_loss: 0.1853 - val_acc: 0.7230\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1881 - acc: 0.7178 - val_loss: 0.1842 - val_acc: 0.7345\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1890 - acc: 0.7164 - val_loss: 0.1904 - val_acc: 0.7048\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1875 - acc: 0.7187 - val_loss: 0.1840 - val_acc: 0.7213\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1887 - acc: 0.7177 - val_loss: 0.1844 - val_acc: 0.7357\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1885 - acc: 0.7188 - val_loss: 0.1843 - val_acc: 0.7265\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1890 - acc: 0.7175 - val_loss: 0.2070 - val_acc: 0.6835\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1885 - acc: 0.7191 - val_loss: 0.1833 - val_acc: 0.7233\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1876 - acc: 0.7198 - val_loss: 0.1927 - val_acc: 0.7157\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1896 - acc: 0.7162 - val_loss: 0.1864 - val_acc: 0.7094\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1893 - acc: 0.7156 - val_loss: 0.1849 - val_acc: 0.7340\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1881 - acc: 0.7178 - val_loss: 0.1833 - val_acc: 0.7334\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1887 - acc: 0.7169 - val_loss: 0.1916 - val_acc: 0.7015\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1877 - acc: 0.7188 - val_loss: 0.1884 - val_acc: 0.7296\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1877 - acc: 0.7186 - val_loss: 0.2020 - val_acc: 0.6920\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.1877 - acc: 0.7182 - val_loss: 0.1886 - val_acc: 0.7292\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1880 - acc: 0.7177 - val_loss: 0.1883 - val_acc: 0.7236\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1877 - acc: 0.7184 - val_loss: 0.1835 - val_acc: 0.7304\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1871 - acc: 0.7201 - val_loss: 0.1834 - val_acc: 0.7225\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1880 - acc: 0.7173 - val_loss: 0.1835 - val_acc: 0.7195\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1881 - acc: 0.7182 - val_loss: 0.1825 - val_acc: 0.7270\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1880 - acc: 0.7176 - val_loss: 0.1846 - val_acc: 0.7175\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1885 - acc: 0.7163 - val_loss: 0.1858 - val_acc: 0.7328\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1870 - acc: 0.7200 - val_loss: 0.1837 - val_acc: 0.7216\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1879 - acc: 0.7182 - val_loss: 0.1829 - val_acc: 0.7281\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1877 - acc: 0.7202 - val_loss: 0.1845 - val_acc: 0.7180\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1884 - acc: 0.7162 - val_loss: 0.2248 - val_acc: 0.6215\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.1880 - acc: 0.7193 - val_loss: 0.1871 - val_acc: 0.7275\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1873 - acc: 0.7194 - val_loss: 0.1837 - val_acc: 0.7166\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1877 - acc: 0.7195 - val_loss: 0.1864 - val_acc: 0.7084\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1880 - acc: 0.7179 - val_loss: 0.1832 - val_acc: 0.7323\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1891 - acc: 0.7165 - val_loss: 0.1832 - val_acc: 0.7223\n",
      "Epoch 155/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1886 - acc: 0.7173 - val_loss: 0.1954 - val_acc: 0.6966\n",
      "Epoch 156/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.1881 - acc: 0.7169 - val_loss: 0.1981 - val_acc: 0.6928\n",
      "Epoch 157/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1878 - acc: 0.7186 - val_loss: 0.1852 - val_acc: 0.7328\n",
      "Epoch 158/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.1881 - acc: 0.7187 - val_loss: 0.1834 - val_acc: 0.7393\n"
     ]
    }
   ],
   "source": [
    "#test 7\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='selu'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57916 samples, validate on 6436 samples\n",
      "Epoch 1/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2694 - acc: 0.5665 - val_loss: 0.3448 - val_acc: 0.3981\n",
      "Epoch 2/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2471 - acc: 0.5749 - val_loss: 0.2410 - val_acc: 0.6019\n",
      "Epoch 3/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2433 - acc: 0.5876 - val_loss: 0.2426 - val_acc: 0.6085\n",
      "Epoch 4/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2417 - acc: 0.5964 - val_loss: 0.2417 - val_acc: 0.6109\n",
      "Epoch 5/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2417 - acc: 0.5945 - val_loss: 0.2388 - val_acc: 0.6052\n",
      "Epoch 6/400\n",
      "57916/57916 [==============================] - 2s 34us/sample - loss: 0.2411 - acc: 0.5968 - val_loss: 0.2386 - val_acc: 0.6038\n",
      "Epoch 7/400\n",
      "57916/57916 [==============================] - 2s 33us/sample - loss: 0.2407 - acc: 0.5987 - val_loss: 0.2388 - val_acc: 0.6080\n",
      "Epoch 8/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2406 - acc: 0.6001 - val_loss: 0.2384 - val_acc: 0.6046\n",
      "Epoch 9/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2402 - acc: 0.6015 - val_loss: 0.2381 - val_acc: 0.6035\n",
      "Epoch 10/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2399 - acc: 0.6009 - val_loss: 0.2382 - val_acc: 0.6043\n",
      "Epoch 11/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2396 - acc: 0.6030 - val_loss: 0.2372 - val_acc: 0.6071\n",
      "Epoch 12/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2391 - acc: 0.6047 - val_loss: 0.2425 - val_acc: 0.5994\n",
      "Epoch 13/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2381 - acc: 0.6065 - val_loss: 0.2345 - val_acc: 0.6085\n",
      "Epoch 14/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2360 - acc: 0.6138 - val_loss: 0.2312 - val_acc: 0.6133\n",
      "Epoch 15/400\n",
      "57916/57916 [==============================] - 1s 24us/sample - loss: 0.2324 - acc: 0.6251 - val_loss: 0.2266 - val_acc: 0.6484\n",
      "Epoch 16/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2281 - acc: 0.6382 - val_loss: 0.2257 - val_acc: 0.6316\n",
      "Epoch 17/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2258 - acc: 0.6452 - val_loss: 0.2217 - val_acc: 0.6715\n",
      "Epoch 18/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2243 - acc: 0.6488 - val_loss: 0.2199 - val_acc: 0.6611\n",
      "Epoch 19/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2220 - acc: 0.6555 - val_loss: 0.2188 - val_acc: 0.6579\n",
      "Epoch 20/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2213 - acc: 0.6576 - val_loss: 0.2227 - val_acc: 0.6443\n",
      "Epoch 21/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2202 - acc: 0.6598 - val_loss: 0.2164 - val_acc: 0.6608\n",
      "Epoch 22/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.2195 - acc: 0.6599 - val_loss: 0.2251 - val_acc: 0.6680\n",
      "Epoch 23/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2186 - acc: 0.6639 - val_loss: 0.2145 - val_acc: 0.6703\n",
      "Epoch 24/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2169 - acc: 0.6665 - val_loss: 0.2133 - val_acc: 0.6759\n",
      "Epoch 25/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2171 - acc: 0.6670 - val_loss: 0.2145 - val_acc: 0.6832\n",
      "Epoch 26/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2171 - acc: 0.6643 - val_loss: 0.2129 - val_acc: 0.6686\n",
      "Epoch 27/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2159 - acc: 0.6670 - val_loss: 0.2120 - val_acc: 0.6796\n",
      "Epoch 28/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2159 - acc: 0.6695 - val_loss: 0.2139 - val_acc: 0.6607\n",
      "Epoch 29/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2155 - acc: 0.6689 - val_loss: 0.2168 - val_acc: 0.6866\n",
      "Epoch 30/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2155 - acc: 0.6684 - val_loss: 0.2219 - val_acc: 0.6624\n",
      "Epoch 31/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2144 - acc: 0.6722 - val_loss: 0.2132 - val_acc: 0.6950\n",
      "Epoch 32/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2136 - acc: 0.6755 - val_loss: 0.2109 - val_acc: 0.6686\n",
      "Epoch 33/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2144 - acc: 0.6718 - val_loss: 0.2120 - val_acc: 0.6975\n",
      "Epoch 34/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2126 - acc: 0.6781 - val_loss: 0.2120 - val_acc: 0.6958\n",
      "Epoch 35/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2128 - acc: 0.6764 - val_loss: 0.2114 - val_acc: 0.6607\n",
      "Epoch 36/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2131 - acc: 0.6736 - val_loss: 0.2083 - val_acc: 0.6863\n",
      "Epoch 37/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2126 - acc: 0.677 - 1s 25us/sample - loss: 0.2126 - acc: 0.6770 - val_loss: 0.2099 - val_acc: 0.6701\n",
      "Epoch 38/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2129 - acc: 0.6748 - val_loss: 0.2094 - val_acc: 0.6947\n",
      "Epoch 39/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2112 - acc: 0.6795 - val_loss: 0.2081 - val_acc: 0.6933\n",
      "Epoch 40/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2115 - acc: 0.6802 - val_loss: 0.2077 - val_acc: 0.6860\n",
      "Epoch 41/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2112 - acc: 0.6814 - val_loss: 0.2095 - val_acc: 0.7018\n",
      "Epoch 42/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2110 - acc: 0.6794 - val_loss: 0.2070 - val_acc: 0.6896\n",
      "Epoch 43/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2106 - acc: 0.6840 - val_loss: 0.2108 - val_acc: 0.6649\n",
      "Epoch 44/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2104 - acc: 0.6831 - val_loss: 0.2103 - val_acc: 0.6948\n",
      "Epoch 45/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2109 - acc: 0.6810 - val_loss: 0.2140 - val_acc: 0.6558\n",
      "Epoch 46/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2110 - acc: 0.6797 - val_loss: 0.2074 - val_acc: 0.6757\n",
      "Epoch 47/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2101 - acc: 0.6818 - val_loss: 0.2095 - val_acc: 0.6751\n",
      "Epoch 48/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2099 - acc: 0.6813 - val_loss: 0.2059 - val_acc: 0.6945\n",
      "Epoch 49/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2093 - acc: 0.6847 - val_loss: 0.2072 - val_acc: 0.6997\n",
      "Epoch 50/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2095 - acc: 0.6827 - val_loss: 0.2053 - val_acc: 0.6941\n",
      "Epoch 51/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2095 - acc: 0.6827 - val_loss: 0.2057 - val_acc: 0.6916\n",
      "Epoch 52/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2097 - acc: 0.6836 - val_loss: 0.2059 - val_acc: 0.7004\n",
      "Epoch 53/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2088 - acc: 0.6860 - val_loss: 0.2056 - val_acc: 0.7048\n",
      "Epoch 54/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2098 - acc: 0.6835 - val_loss: 0.2144 - val_acc: 0.6877\n",
      "Epoch 55/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2087 - acc: 0.6847 - val_loss: 0.2055 - val_acc: 0.6889\n",
      "Epoch 56/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2097 - acc: 0.6827 - val_loss: 0.2062 - val_acc: 0.6857\n",
      "Epoch 57/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2085 - acc: 0.6863 - val_loss: 0.2049 - val_acc: 0.6959\n",
      "Epoch 58/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2088 - acc: 0.6854 - val_loss: 0.2091 - val_acc: 0.6917\n",
      "Epoch 59/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2086 - acc: 0.6871 - val_loss: 0.2125 - val_acc: 0.6642\n",
      "Epoch 60/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2087 - acc: 0.6867 - val_loss: 0.2052 - val_acc: 0.6897\n",
      "Epoch 61/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2082 - acc: 0.6876 - val_loss: 0.2046 - val_acc: 0.6905\n",
      "Epoch 62/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2080 - acc: 0.6880 - val_loss: 0.2080 - val_acc: 0.7004\n",
      "Epoch 63/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2087 - acc: 0.6859 - val_loss: 0.2060 - val_acc: 0.6819\n",
      "Epoch 64/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2076 - acc: 0.6890 - val_loss: 0.2057 - val_acc: 0.7048\n",
      "Epoch 65/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2079 - acc: 0.6869 - val_loss: 0.2094 - val_acc: 0.6964\n",
      "Epoch 66/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2082 - acc: 0.6878 - val_loss: 0.2048 - val_acc: 0.7063 loss: 0.2093 - a\n",
      "Epoch 67/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2076 - acc: 0.6893 - val_loss: 0.2141 - val_acc: 0.6619\n",
      "Epoch 68/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2077 - acc: 0.6888 - val_loss: 0.2088 - val_acc: 0.6941\n",
      "Epoch 69/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2079 - acc: 0.6875 - val_loss: 0.2043 - val_acc: 0.6936\n",
      "Epoch 70/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2079 - acc: 0.6865 - val_loss: 0.2058 - val_acc: 0.7003\n",
      "Epoch 71/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2076 - acc: 0.6874 - val_loss: 0.2042 - val_acc: 0.6942\n",
      "Epoch 72/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2076 - acc: 0.6885 - val_loss: 0.2054 - val_acc: 0.6827\n",
      "Epoch 73/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2077 - acc: 0.6863 - val_loss: 0.2048 - val_acc: 0.6899\n",
      "Epoch 74/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2070 - acc: 0.6889 - val_loss: 0.2045 - val_acc: 0.6958\n",
      "Epoch 75/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2070 - acc: 0.6904 - val_loss: 0.2038 - val_acc: 0.6952\n",
      "Epoch 76/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2071 - acc: 0.6889 - val_loss: 0.2037 - val_acc: 0.6919\n",
      "Epoch 77/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2071 - acc: 0.6887 - val_loss: 0.2078 - val_acc: 0.7018\n",
      "Epoch 78/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2067 - acc: 0.6906 - val_loss: 0.2032 - val_acc: 0.6978\n",
      "Epoch 79/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2064 - acc: 0.6912 - val_loss: 0.2039 - val_acc: 0.6916\n",
      "Epoch 80/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2066 - acc: 0.6905 - val_loss: 0.2051 - val_acc: 0.6931\n",
      "Epoch 81/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2061 - acc: 0.6899 - val_loss: 0.2035 - val_acc: 0.7009\n",
      "Epoch 82/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2069 - acc: 0.6887 - val_loss: 0.2038 - val_acc: 0.7011\n",
      "Epoch 83/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2077 - acc: 0.6860 - val_loss: 0.2032 - val_acc: 0.7073\n",
      "Epoch 84/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2066 - acc: 0.6896 - val_loss: 0.2071 - val_acc: 0.6987\n",
      "Epoch 85/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2075 - acc: 0.6874 - val_loss: 0.2030 - val_acc: 0.6970\n",
      "Epoch 86/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2064 - acc: 0.6911 - val_loss: 0.2033 - val_acc: 0.7007\n",
      "Epoch 87/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2066 - acc: 0.6883 - val_loss: 0.2032 - val_acc: 0.7026\n",
      "Epoch 88/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2073 - acc: 0.6883 - val_loss: 0.2038 - val_acc: 0.6967\n",
      "Epoch 89/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2063 - acc: 0.6920 - val_loss: 0.2037 - val_acc: 0.6936\n",
      "Epoch 90/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2065 - acc: 0.6892 - val_loss: 0.2027 - val_acc: 0.7031\n",
      "Epoch 91/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2060 - acc: 0.6921 - val_loss: 0.2027 - val_acc: 0.6998\n",
      "Epoch 92/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2059 - acc: 0.6904 - val_loss: 0.2045 - val_acc: 0.7004\n",
      "Epoch 93/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2057 - acc: 0.6903 - val_loss: 0.2032 - val_acc: 0.6942\n",
      "Epoch 94/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2068 - acc: 0.6878 - val_loss: 0.2064 - val_acc: 0.6986\n",
      "Epoch 95/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2061 - acc: 0.6896 - val_loss: 0.2040 - val_acc: 0.7054\n",
      "Epoch 96/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2062 - acc: 0.6899 - val_loss: 0.2084 - val_acc: 0.7039\n",
      "Epoch 97/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2061 - acc: 0.6901 - val_loss: 0.2125 - val_acc: 0.6894\n",
      "Epoch 98/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2065 - acc: 0.6871 - val_loss: 0.2041 - val_acc: 0.6868\n",
      "Epoch 99/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2055 - acc: 0.6922 - val_loss: 0.2025 - val_acc: 0.6966\n",
      "Epoch 100/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2054 - acc: 0.6921 - val_loss: 0.2023 - val_acc: 0.6992\n",
      "Epoch 101/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2055 - acc: 0.6906 - val_loss: 0.2037 - val_acc: 0.6902\n",
      "Epoch 102/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2056 - acc: 0.6903 - val_loss: 0.2031 - val_acc: 0.6914\n",
      "Epoch 103/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2059 - acc: 0.6909 - val_loss: 0.2036 - val_acc: 0.6892\n",
      "Epoch 104/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2063 - acc: 0.6903 - val_loss: 0.2027 - val_acc: 0.7074\n",
      "Epoch 105/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2058 - acc: 0.6898 - val_loss: 0.2025 - val_acc: 0.6978\n",
      "Epoch 106/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2056 - acc: 0.6909 - val_loss: 0.2121 - val_acc: 0.6883\n",
      "Epoch 107/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2052 - acc: 0.6917 - val_loss: 0.2025 - val_acc: 0.6993\n",
      "Epoch 108/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2055 - acc: 0.6924 - val_loss: 0.2060 - val_acc: 0.7007\n",
      "Epoch 109/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2053 - acc: 0.6910 - val_loss: 0.2022 - val_acc: 0.6984\n",
      "Epoch 110/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2053 - acc: 0.6909 - val_loss: 0.2104 - val_acc: 0.6681\n",
      "Epoch 111/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2058 - acc: 0.6904 - val_loss: 0.2032 - val_acc: 0.6900\n",
      "Epoch 112/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2051 - acc: 0.6930 - val_loss: 0.2027 - val_acc: 0.6944\n",
      "Epoch 113/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2056 - acc: 0.6913 - val_loss: 0.2077 - val_acc: 0.7009\n",
      "Epoch 114/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2053 - acc: 0.6900 - val_loss: 0.2081 - val_acc: 0.6731\n",
      "Epoch 115/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2046 - acc: 0.6927 - val_loss: 0.2019 - val_acc: 0.6975\n",
      "Epoch 116/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2047 - acc: 0.6920 - val_loss: 0.2031 - val_acc: 0.6916\n",
      "Epoch 117/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2053 - acc: 0.6913 - val_loss: 0.2083 - val_acc: 0.6920\n",
      "Epoch 118/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2048 - acc: 0.6909 - val_loss: 0.2027 - val_acc: 0.6922\n",
      "Epoch 119/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2050 - acc: 0.6922 - val_loss: 0.2039 - val_acc: 0.7094\n",
      "Epoch 120/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2052 - acc: 0.6915 - val_loss: 0.2024 - val_acc: 0.7032\n",
      "Epoch 121/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2047 - acc: 0.6909 - val_loss: 0.2023 - val_acc: 0.7000\n",
      "Epoch 122/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2047 - acc: 0.6928 - val_loss: 0.2131 - val_acc: 0.6781\n",
      "Epoch 123/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2051 - acc: 0.6899 - val_loss: 0.2224 - val_acc: 0.6254\n",
      "Epoch 124/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2047 - acc: 0.6916 - val_loss: 0.2026 - val_acc: 0.7023\n",
      "Epoch 125/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2046 - acc: 0.6911 - val_loss: 0.2107 - val_acc: 0.6840\n",
      "Epoch 126/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2047 - acc: 0.6905 - val_loss: 0.2017 - val_acc: 0.6955\n",
      "Epoch 127/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2046 - acc: 0.6918 - val_loss: 0.2025 - val_acc: 0.6886\n",
      "Epoch 128/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2044 - acc: 0.6936 - val_loss: 0.2019 - val_acc: 0.7042\n",
      "Epoch 129/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2043 - acc: 0.6931 - val_loss: 0.2033 - val_acc: 0.6865\n",
      "Epoch 130/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2044 - acc: 0.6913 - val_loss: 0.2026 - val_acc: 0.6990\n",
      "Epoch 131/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2047 - acc: 0.6911 - val_loss: 0.2022 - val_acc: 0.6990\n",
      "Epoch 132/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2049 - acc: 0.6908 - val_loss: 0.2026 - val_acc: 0.7011\n",
      "Epoch 133/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2045 - acc: 0.6928 - val_loss: 0.2017 - val_acc: 0.6941\n",
      "Epoch 134/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2050 - acc: 0.6903 - val_loss: 0.2032 - val_acc: 0.7006\n",
      "Epoch 135/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2043 - acc: 0.6924 - val_loss: 0.2048 - val_acc: 0.7045\n",
      "Epoch 136/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2042 - acc: 0.6921 - val_loss: 0.2015 - val_acc: 0.6961\n",
      "Epoch 137/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2045 - acc: 0.6916 - val_loss: 0.2014 - val_acc: 0.6979\n",
      "Epoch 138/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2043 - acc: 0.6926 - val_loss: 0.2036 - val_acc: 0.7045\n",
      "Epoch 139/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2039 - acc: 0.6921 - val_loss: 0.2055 - val_acc: 0.6815\n",
      "Epoch 140/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2044 - acc: 0.6906 - val_loss: 0.2012 - val_acc: 0.7007\n",
      "Epoch 141/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2046 - acc: 0.6913 - val_loss: 0.2062 - val_acc: 0.6952\n",
      "Epoch 142/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2043 - acc: 0.6919 - val_loss: 0.2013 - val_acc: 0.7025\n",
      "Epoch 143/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2042 - acc: 0.6915 - val_loss: 0.2018 - val_acc: 0.6956\n",
      "Epoch 144/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2040 - acc: 0.6932 - val_loss: 0.2017 - val_acc: 0.6978\n",
      "Epoch 145/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2050 - acc: 0.6888 - val_loss: 0.2044 - val_acc: 0.6823\n",
      "Epoch 146/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2039 - acc: 0.6926 - val_loss: 0.2017 - val_acc: 0.7017\n",
      "Epoch 147/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2042 - acc: 0.6918 - val_loss: 0.2017 - val_acc: 0.7015\n",
      "Epoch 148/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2039 - acc: 0.6926 - val_loss: 0.2022 - val_acc: 0.6919\n",
      "Epoch 149/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2043 - acc: 0.6913 - val_loss: 0.2016 - val_acc: 0.7007\n",
      "Epoch 150/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2042 - acc: 0.6905 - val_loss: 0.2017 - val_acc: 0.6948\n",
      "Epoch 151/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2039 - acc: 0.6932 - val_loss: 0.2012 - val_acc: 0.6962\n",
      "Epoch 152/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2039 - acc: 0.6927 - val_loss: 0.2023 - val_acc: 0.6880\n",
      "Epoch 153/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2036 - acc: 0.693 - 1s 25us/sample - loss: 0.2036 - acc: 0.6934 - val_loss: 0.2062 - val_acc: 0.7021\n",
      "Epoch 154/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2039 - acc: 0.6921 - val_loss: 0.2013 - val_acc: 0.6955\n",
      "Epoch 155/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2036 - acc: 0.6951 - val_loss: 0.2066 - val_acc: 0.7025\n",
      "Epoch 156/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2041 - acc: 0.6911 - val_loss: 0.2017 - val_acc: 0.7057\n",
      "Epoch 157/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2040 - acc: 0.6920 - val_loss: 0.2021 - val_acc: 0.6934\n",
      "Epoch 158/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2035 - acc: 0.6935 - val_loss: 0.2038 - val_acc: 0.7001\n",
      "Epoch 159/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2036 - acc: 0.6931 - val_loss: 0.2050 - val_acc: 0.7017\n",
      "Epoch 160/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2033 - acc: 0.6942 - val_loss: 0.2017 - val_acc: 0.6978\n",
      "Epoch 161/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2040 - acc: 0.6913 - val_loss: 0.2014 - val_acc: 0.6964\n",
      "Epoch 162/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2035 - acc: 0.6946 - val_loss: 0.2013 - val_acc: 0.7011\n",
      "Epoch 163/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2040 - acc: 0.6920 - val_loss: 0.2019 - val_acc: 0.7090\n",
      "Epoch 164/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2035 - acc: 0.6925 - val_loss: 0.2010 - val_acc: 0.7001\n",
      "Epoch 165/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2035 - acc: 0.6937 - val_loss: 0.2011 - val_acc: 0.7037\n",
      "Epoch 166/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2038 - acc: 0.6922 - val_loss: 0.2024 - val_acc: 0.7018\n",
      "Epoch 167/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2037 - acc: 0.6928 - val_loss: 0.2017 - val_acc: 0.6891\n",
      "Epoch 168/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2035 - acc: 0.6926 - val_loss: 0.2015 - val_acc: 0.6927\n",
      "Epoch 169/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2038 - acc: 0.6924 - val_loss: 0.2023 - val_acc: 0.6910\n",
      "Epoch 170/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2037 - acc: 0.6935 - val_loss: 0.2008 - val_acc: 0.7004\n",
      "Epoch 171/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2035 - acc: 0.6923 - val_loss: 0.2033 - val_acc: 0.6837\n",
      "Epoch 172/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2034 - acc: 0.6931 - val_loss: 0.2016 - val_acc: 0.7040\n",
      "Epoch 173/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2032 - acc: 0.6930 - val_loss: 0.2007 - val_acc: 0.6992\n",
      "Epoch 174/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2034 - acc: 0.6927 - val_loss: 0.2016 - val_acc: 0.7045\n",
      "Epoch 175/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2032 - acc: 0.6940 - val_loss: 0.2015 - val_acc: 0.6913\n",
      "Epoch 176/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2034 - acc: 0.6918 - val_loss: 0.2010 - val_acc: 0.6952\n",
      "Epoch 177/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2035 - acc: 0.6914 - val_loss: 0.2022 - val_acc: 0.7018\n",
      "Epoch 178/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2034 - acc: 0.6916 - val_loss: 0.2027 - val_acc: 0.6986\n",
      "Epoch 179/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2033 - acc: 0.6930 - val_loss: 0.2006 - val_acc: 0.7051\n",
      "Epoch 180/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2033 - acc: 0.6938 - val_loss: 0.2011 - val_acc: 0.7014\n",
      "Epoch 181/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2036 - acc: 0.6920 - val_loss: 0.2038 - val_acc: 0.7018\n",
      "Epoch 182/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2032 - acc: 0.6936 - val_loss: 0.2005 - val_acc: 0.6981\n",
      "Epoch 183/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2031 - acc: 0.6934 - val_loss: 0.2010 - val_acc: 0.6947\n",
      "Epoch 184/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2032 - acc: 0.6922 - val_loss: 0.2029 - val_acc: 0.6987\n",
      "Epoch 185/400\n",
      "57916/57916 [==============================] - 2s 36us/sample - loss: 0.2035 - acc: 0.6918 - val_loss: 0.2004 - val_acc: 0.6958\n",
      "Epoch 186/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2032 - acc: 0.6926 - val_loss: 0.2041 - val_acc: 0.6986\n",
      "Epoch 187/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2033 - acc: 0.6926 - val_loss: 0.2021 - val_acc: 0.6911\n",
      "Epoch 188/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2031 - acc: 0.6929 - val_loss: 0.2004 - val_acc: 0.6997\n",
      "Epoch 189/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2030 - acc: 0.6930 - val_loss: 0.2022 - val_acc: 0.6983\n",
      "Epoch 190/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2028 - acc: 0.6936 - val_loss: 0.2029 - val_acc: 0.6961\n",
      "Epoch 191/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2033 - acc: 0.6914 - val_loss: 0.2005 - val_acc: 0.6978\n",
      "Epoch 192/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2026 - acc: 0.6927 - val_loss: 0.2004 - val_acc: 0.7034\n",
      "Epoch 193/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2029 - acc: 0.6954 - val_loss: 0.2006 - val_acc: 0.6939\n",
      "Epoch 194/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2028 - acc: 0.6919 - val_loss: 0.2028 - val_acc: 0.6981\n",
      "Epoch 195/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2031 - acc: 0.6924 - val_loss: 0.2004 - val_acc: 0.6970\n",
      "Epoch 196/400\n",
      "57916/57916 [==============================] - ETA: 0s - loss: 0.2030 - acc: 0.693 - 2s 26us/sample - loss: 0.2031 - acc: 0.6936 - val_loss: 0.2011 - val_acc: 0.6948\n",
      "Epoch 197/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2032 - acc: 0.6922 - val_loss: 0.2003 - val_acc: 0.7029\n",
      "Epoch 198/400\n",
      "57916/57916 [==============================] - 2s 31us/sample - loss: 0.2029 - acc: 0.6936 - val_loss: 0.2055 - val_acc: 0.6771\n",
      "Epoch 199/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2028 - acc: 0.6925 - val_loss: 0.2001 - val_acc: 0.6983\n",
      "Epoch 200/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2028 - acc: 0.6925 - val_loss: 0.2059 - val_acc: 0.6978\n",
      "Epoch 201/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2031 - acc: 0.6930 - val_loss: 0.2007 - val_acc: 0.6941\n",
      "Epoch 202/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2025 - acc: 0.6932 - val_loss: 0.2001 - val_acc: 0.6998\n",
      "Epoch 203/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2026 - acc: 0.6927 - val_loss: 0.2005 - val_acc: 0.7028\n",
      "Epoch 204/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2028 - acc: 0.6931 - val_loss: 0.2013 - val_acc: 0.6925\n",
      "Epoch 205/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2031 - acc: 0.6920 - val_loss: 0.2002 - val_acc: 0.7018\n",
      "Epoch 206/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2026 - acc: 0.6937 - val_loss: 0.2124 - val_acc: 0.6728\n",
      "Epoch 207/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2031 - acc: 0.6931 - val_loss: 0.2011 - val_acc: 0.6993\n",
      "Epoch 208/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2030 - acc: 0.6922 - val_loss: 0.2003 - val_acc: 0.7015\n",
      "Epoch 209/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2025 - acc: 0.6944 - val_loss: 0.2015 - val_acc: 0.6875\n",
      "Epoch 210/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2030 - acc: 0.6921 - val_loss: 0.2016 - val_acc: 0.7000\n",
      "Epoch 211/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2026 - acc: 0.6929 - val_loss: 0.2005 - val_acc: 0.6992\n",
      "Epoch 212/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2025 - acc: 0.6933 - val_loss: 0.2000 - val_acc: 0.7025\n",
      "Epoch 213/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2026 - acc: 0.6939 - val_loss: 0.2002 - val_acc: 0.6970\n",
      "Epoch 214/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2027 - acc: 0.6925 - val_loss: 0.2001 - val_acc: 0.7071\n",
      "Epoch 215/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2022 - acc: 0.6935 - val_loss: 0.2007 - val_acc: 0.7004\n",
      "Epoch 216/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2025 - acc: 0.6929 - val_loss: 0.1998 - val_acc: 0.6997\n",
      "Epoch 217/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2026 - acc: 0.6931 - val_loss: 0.2006 - val_acc: 0.6900\n",
      "Epoch 218/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2027 - acc: 0.6938 - val_loss: 0.2042 - val_acc: 0.6958\n",
      "Epoch 219/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2021 - acc: 0.6929 - val_loss: 0.1999 - val_acc: 0.7065\n",
      "Epoch 220/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2024 - acc: 0.6934 - val_loss: 0.1998 - val_acc: 0.7020\n",
      "Epoch 221/400\n",
      "57916/57916 [==============================] - 2s 36us/sample - loss: 0.2023 - acc: 0.6946 - val_loss: 0.2016 - val_acc: 0.6995\n",
      "Epoch 222/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2028 - acc: 0.6935 - val_loss: 0.1999 - val_acc: 0.7032\n",
      "Epoch 223/400\n",
      "57916/57916 [==============================] - 3s 45us/sample - loss: 0.2021 - acc: 0.6940 - val_loss: 0.1998 - val_acc: 0.7046\n",
      "Epoch 224/400\n",
      "57916/57916 [==============================] - 2s 37us/sample - loss: 0.2022 - acc: 0.6947 - val_loss: 0.1997 - val_acc: 0.7012\n",
      "Epoch 225/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2021 - acc: 0.6937 - val_loss: 0.2001 - val_acc: 0.6987\n",
      "Epoch 226/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2021 - acc: 0.6946 - val_loss: 0.2000 - val_acc: 0.7035\n",
      "Epoch 227/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2024 - acc: 0.6949 - val_loss: 0.2006 - val_acc: 0.6978\n",
      "Epoch 228/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2021 - acc: 0.6949 - val_loss: 0.2007 - val_acc: 0.6979\n",
      "Epoch 229/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2021 - acc: 0.6938 - val_loss: 0.1998 - val_acc: 0.6962\n",
      "Epoch 230/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2019 - acc: 0.6952 - val_loss: 0.2064 - val_acc: 0.6958\n",
      "Epoch 231/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2022 - acc: 0.6940 - val_loss: 0.2003 - val_acc: 0.7053\n",
      "Epoch 232/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2018 - acc: 0.6947 - val_loss: 0.2006 - val_acc: 0.6986\n",
      "Epoch 233/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2024 - acc: 0.6932 - val_loss: 0.1998 - val_acc: 0.7021\n",
      "Epoch 234/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2018 - acc: 0.6949 - val_loss: 0.2011 - val_acc: 0.6990\n",
      "Epoch 235/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2021 - acc: 0.6941 - val_loss: 0.2009 - val_acc: 0.6979\n",
      "Epoch 236/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2024 - acc: 0.6931 - val_loss: 0.2000 - val_acc: 0.6966\n",
      "Epoch 237/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2017 - acc: 0.6961 - val_loss: 0.1997 - val_acc: 0.7039\n",
      "Epoch 238/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2015 - acc: 0.6969 - val_loss: 0.2007 - val_acc: 0.7006\n",
      "Epoch 239/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2018 - acc: 0.6955 - val_loss: 0.1996 - val_acc: 0.6995\n",
      "Epoch 240/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2020 - acc: 0.6943 - val_loss: 0.1998 - val_acc: 0.6933\n",
      "Epoch 241/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2019 - acc: 0.6955 - val_loss: 0.2015 - val_acc: 0.6861\n",
      "Epoch 242/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2018 - acc: 0.6950 - val_loss: 0.1995 - val_acc: 0.7039\n",
      "Epoch 243/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2018 - acc: 0.6950 - val_loss: 0.2002 - val_acc: 0.7025\n",
      "Epoch 244/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2018 - acc: 0.6942 - val_loss: 0.2013 - val_acc: 0.7004\n",
      "Epoch 245/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2020 - acc: 0.6944 - val_loss: 0.2001 - val_acc: 0.6997\n",
      "Epoch 246/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2017 - acc: 0.6943 - val_loss: 0.2022 - val_acc: 0.6984\n",
      "Epoch 247/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2018 - acc: 0.6944 - val_loss: 0.2057 - val_acc: 0.7060\n",
      "Epoch 248/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2016 - acc: 0.6956 - val_loss: 0.2003 - val_acc: 0.6933\n",
      "Epoch 249/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2015 - acc: 0.6954 - val_loss: 0.1996 - val_acc: 0.6987\n",
      "Epoch 250/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2013 - acc: 0.6936 - val_loss: 0.1995 - val_acc: 0.7029\n",
      "Epoch 251/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2020 - acc: 0.6938 - val_loss: 0.2005 - val_acc: 0.6986\n",
      "Epoch 252/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2014 - acc: 0.6950 - val_loss: 0.1994 - val_acc: 0.6972\n",
      "Epoch 253/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2013 - acc: 0.6955 - val_loss: 0.1998 - val_acc: 0.6964\n",
      "Epoch 254/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2021 - acc: 0.6928 - val_loss: 0.1999 - val_acc: 0.6990\n",
      "Epoch 255/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2017 - acc: 0.6939 - val_loss: 0.1998 - val_acc: 0.6987\n",
      "Epoch 256/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2017 - acc: 0.6955 - val_loss: 0.1995 - val_acc: 0.7034\n",
      "Epoch 257/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2013 - acc: 0.6942 - val_loss: 0.1995 - val_acc: 0.6966\n",
      "Epoch 258/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2014 - acc: 0.6946 - val_loss: 0.1993 - val_acc: 0.7012\n",
      "Epoch 259/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2014 - acc: 0.6948 - val_loss: 0.1997 - val_acc: 0.7053\n",
      "Epoch 260/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2012 - acc: 0.6947 - val_loss: 0.1992 - val_acc: 0.7025\n",
      "Epoch 261/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2014 - acc: 0.6969 - val_loss: 0.1998 - val_acc: 0.6993\n",
      "Epoch 262/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2014 - acc: 0.6955 - val_loss: 0.1991 - val_acc: 0.6990\n",
      "Epoch 263/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2017 - acc: 0.6940 - val_loss: 0.1994 - val_acc: 0.7003\n",
      "Epoch 264/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2012 - acc: 0.6953 - val_loss: 0.2011 - val_acc: 0.7000\n",
      "Epoch 265/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2009 - acc: 0.6962 - val_loss: 0.2005 - val_acc: 0.7031\n",
      "Epoch 266/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2013 - acc: 0.6960 - val_loss: 0.1999 - val_acc: 0.6956\n",
      "Epoch 267/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2012 - acc: 0.6959 - val_loss: 0.2036 - val_acc: 0.6812\n",
      "Epoch 268/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2013 - acc: 0.6952 - val_loss: 0.1997 - val_acc: 0.6962\n",
      "Epoch 269/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2011 - acc: 0.6953 - val_loss: 0.1996 - val_acc: 0.6989\n",
      "Epoch 270/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2013 - acc: 0.6961 - val_loss: 0.2003 - val_acc: 0.7023\n",
      "Epoch 271/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2010 - acc: 0.6965 - val_loss: 0.2075 - val_acc: 0.6936\n",
      "Epoch 272/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2012 - acc: 0.6960 - val_loss: 0.1990 - val_acc: 0.7080\n",
      "Epoch 273/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2012 - acc: 0.6950 - val_loss: 0.1994 - val_acc: 0.7021\n",
      "Epoch 274/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2012 - acc: 0.6959 - val_loss: 0.1995 - val_acc: 0.7054\n",
      "Epoch 275/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2012 - acc: 0.6957 - val_loss: 0.1989 - val_acc: 0.7051\n",
      "Epoch 276/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2008 - acc: 0.6974 - val_loss: 0.2012 - val_acc: 0.6970\n",
      "Epoch 277/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2013 - acc: 0.6961 - val_loss: 0.2005 - val_acc: 0.7020\n",
      "Epoch 278/400\n",
      "57916/57916 [==============================] - 2s 35us/sample - loss: 0.2011 - acc: 0.6973 - val_loss: 0.1998 - val_acc: 0.6986\n",
      "Epoch 279/400\n",
      "57916/57916 [==============================] - 2s 36us/sample - loss: 0.2010 - acc: 0.6973 - val_loss: 0.1996 - val_acc: 0.6981\n",
      "Epoch 280/400\n",
      "57916/57916 [==============================] - 2s 32us/sample - loss: 0.2010 - acc: 0.6955 - val_loss: 0.1989 - val_acc: 0.7034\n",
      "Epoch 281/400\n",
      "57916/57916 [==============================] - 2s 30us/sample - loss: 0.2008 - acc: 0.6959 - val_loss: 0.2004 - val_acc: 0.7063\n",
      "Epoch 282/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2008 - acc: 0.6961 - val_loss: 0.1989 - val_acc: 0.7051\n",
      "Epoch 283/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2010 - acc: 0.6952 - val_loss: 0.2125 - val_acc: 0.6690\n",
      "Epoch 284/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2009 - acc: 0.6956 - val_loss: 0.2049 - val_acc: 0.7003\n",
      "Epoch 285/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2012 - acc: 0.6964 - val_loss: 0.2004 - val_acc: 0.6997\n",
      "Epoch 286/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2007 - acc: 0.6968 - val_loss: 0.1986 - val_acc: 0.7023\n",
      "Epoch 287/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2009 - acc: 0.6964 - val_loss: 0.1989 - val_acc: 0.7035\n",
      "Epoch 288/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2006 - acc: 0.6962 - val_loss: 0.2000 - val_acc: 0.6986\n",
      "Epoch 289/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2008 - acc: 0.6960 - val_loss: 0.1989 - val_acc: 0.6997\n",
      "Epoch 290/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2009 - acc: 0.6966 - val_loss: 0.1988 - val_acc: 0.7057\n",
      "Epoch 291/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2007 - acc: 0.6965 - val_loss: 0.1987 - val_acc: 0.6993\n",
      "Epoch 292/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2004 - acc: 0.6984 - val_loss: 0.2011 - val_acc: 0.6995\n",
      "Epoch 293/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2006 - acc: 0.6969 - val_loss: 0.2006 - val_acc: 0.7004\n",
      "Epoch 294/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2009 - acc: 0.6969 - val_loss: 0.1998 - val_acc: 0.6952\n",
      "Epoch 295/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2006 - acc: 0.6953 - val_loss: 0.1990 - val_acc: 0.7070\n",
      "Epoch 296/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2005 - acc: 0.6956 - val_loss: 0.1988 - val_acc: 0.6986\n",
      "Epoch 297/400\n",
      "57916/57916 [==============================] - 2s 29us/sample - loss: 0.2006 - acc: 0.6959 - val_loss: 0.1985 - val_acc: 0.6975\n",
      "Epoch 298/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2007 - acc: 0.6970 - val_loss: 0.1992 - val_acc: 0.7068\n",
      "Epoch 299/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2008 - acc: 0.6969 - val_loss: 0.1988 - val_acc: 0.7043\n",
      "Epoch 300/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2007 - acc: 0.6960 - val_loss: 0.1986 - val_acc: 0.7000\n",
      "Epoch 301/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2007 - acc: 0.6978 - val_loss: 0.1993 - val_acc: 0.6979\n",
      "Epoch 302/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2008 - acc: 0.6967 - val_loss: 0.1989 - val_acc: 0.7029\n",
      "Epoch 303/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2005 - acc: 0.6980 - val_loss: 0.1985 - val_acc: 0.7025\n",
      "Epoch 304/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2006 - acc: 0.6965 - val_loss: 0.1992 - val_acc: 0.6989 - loss: 0.2012 - acc - ETA: 0s - loss: 0.2007 - acc: 0.69\n",
      "Epoch 305/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2005 - acc: 0.6965 - val_loss: 0.1985 - val_acc: 0.7048\n",
      "Epoch 306/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2008 - acc: 0.6970 - val_loss: 0.1992 - val_acc: 0.7014\n",
      "Epoch 307/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2003 - acc: 0.6978 - val_loss: 0.1991 - val_acc: 0.7029\n",
      "Epoch 308/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2005 - acc: 0.6972 - val_loss: 0.2066 - val_acc: 0.6952\n",
      "Epoch 309/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2005 - acc: 0.6976 - val_loss: 0.1987 - val_acc: 0.7065\n",
      "Epoch 310/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2003 - acc: 0.6980 - val_loss: 0.1999 - val_acc: 0.7006\n",
      "Epoch 311/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2002 - acc: 0.6983 - val_loss: 0.1988 - val_acc: 0.7053\n",
      "Epoch 312/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2007 - acc: 0.6964 - val_loss: 0.1986 - val_acc: 0.6975\n",
      "Epoch 313/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2001 - acc: 0.6972 - val_loss: 0.1992 - val_acc: 0.6997\n",
      "Epoch 314/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2004 - acc: 0.6974 - val_loss: 0.1983 - val_acc: 0.7015\n",
      "Epoch 315/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2002 - acc: 0.6975 - val_loss: 0.2024 - val_acc: 0.6961\n",
      "Epoch 316/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2006 - acc: 0.6967 - val_loss: 0.2049 - val_acc: 0.6976\n",
      "Epoch 317/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2004 - acc: 0.6981 - val_loss: 0.2007 - val_acc: 0.7049\n",
      "Epoch 318/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2004 - acc: 0.6967 - val_loss: 0.1985 - val_acc: 0.7054\n",
      "Epoch 319/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.2003 - acc: 0.6984 - val_loss: 0.1982 - val_acc: 0.7003\n",
      "Epoch 320/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2003 - acc: 0.6967 - val_loss: 0.1995 - val_acc: 0.7079\n",
      "Epoch 321/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2006 - acc: 0.6970 - val_loss: 0.1990 - val_acc: 0.7048\n",
      "Epoch 322/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2003 - acc: 0.6967 - val_loss: 0.1992 - val_acc: 0.6942\n",
      "Epoch 323/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2002 - acc: 0.6979 - val_loss: 0.1990 - val_acc: 0.7021\n",
      "Epoch 324/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2001 - acc: 0.6962 - val_loss: 0.1987 - val_acc: 0.6979\n",
      "Epoch 325/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2002 - acc: 0.6967 - val_loss: 0.2003 - val_acc: 0.6944\n",
      "Epoch 326/400\n",
      "57916/57916 [==============================] - 2s 28us/sample - loss: 0.2006 - acc: 0.6964 - val_loss: 0.1982 - val_acc: 0.7029\n",
      "Epoch 327/400\n",
      "57916/57916 [==============================] - 1s 26us/sample - loss: 0.1999 - acc: 0.6983 - val_loss: 0.1990 - val_acc: 0.7056\n",
      "Epoch 328/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2001 - acc: 0.6973 - val_loss: 0.2008 - val_acc: 0.7020\n",
      "Epoch 329/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2003 - acc: 0.6955 - val_loss: 0.2001 - val_acc: 0.7053\n",
      "Epoch 330/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1999 - acc: 0.6992 - val_loss: 0.1987 - val_acc: 0.7009\n",
      "Epoch 331/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2000 - acc: 0.6974 - val_loss: 0.2015 - val_acc: 0.7025\n",
      "Epoch 332/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2002 - acc: 0.6976 - val_loss: 0.1999 - val_acc: 0.6952\n",
      "Epoch 333/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2004 - acc: 0.6970 - val_loss: 0.1980 - val_acc: 0.6981\n",
      "Epoch 334/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1998 - acc: 0.6987 - val_loss: 0.1981 - val_acc: 0.7040\n",
      "Epoch 335/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1998 - acc: 0.6986 - val_loss: 0.1989 - val_acc: 0.7017\n",
      "Epoch 336/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1999 - acc: 0.6984 - val_loss: 0.1996 - val_acc: 0.7011\n",
      "Epoch 337/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2002 - acc: 0.6991 - val_loss: 0.2032 - val_acc: 0.7025\n",
      "Epoch 338/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1999 - acc: 0.6990 - val_loss: 0.1981 - val_acc: 0.7023\n",
      "Epoch 339/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.1996 - acc: 0.6971 - val_loss: 0.1981 - val_acc: 0.7046\n",
      "Epoch 340/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2003 - acc: 0.6965 - val_loss: 0.1990 - val_acc: 0.7065\n",
      "Epoch 341/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2000 - acc: 0.6977 - val_loss: 0.1987 - val_acc: 0.6993\n",
      "Epoch 342/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.1997 - acc: 0.6989 - val_loss: 0.1982 - val_acc: 0.7006\n",
      "Epoch 343/400\n",
      "57916/57916 [==============================] - 2s 26us/sample - loss: 0.2002 - acc: 0.6951 - val_loss: 0.1984 - val_acc: 0.7042\n",
      "Epoch 344/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1998 - acc: 0.6987 - val_loss: 0.1982 - val_acc: 0.7060\n",
      "Epoch 345/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.2000 - acc: 0.6958 - val_loss: 0.1981 - val_acc: 0.7062\n",
      "Epoch 346/400\n",
      "57916/57916 [==============================] - 1s 25us/sample - loss: 0.2001 - acc: 0.6976 - val_loss: 0.1998 - val_acc: 0.6939\n",
      "Epoch 347/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1996 - acc: 0.6988 - val_loss: 0.2009 - val_acc: 0.7006\n",
      "Epoch 348/400\n",
      "57916/57916 [==============================] - 2s 27us/sample - loss: 0.1999 - acc: 0.6981 - val_loss: 0.1983 - val_acc: 0.6992\n"
     ]
    }
   ],
   "source": [
    "#test 8\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20,input_dim=9, activation='exponential'))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(15))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test),callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1acd9c700b8>"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[20,8])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy', size=25, pad=20)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1acdcfb0cc0>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss', size=25, pad=20)\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_keras = model.predict(X_test).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 15.0, 'Predicted label')"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9783cdf2a37e2a41721e81ed5e12f4266c666f19eac87f25458d24597dc339e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
