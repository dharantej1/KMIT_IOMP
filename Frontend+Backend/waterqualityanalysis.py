# -*- coding: utf-8 -*-
"""dharananna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19EKsydbrR1PLi2rWTfSJBhsgOdkUHEN-
"""

import pandas as pd
import numpy as np
#from IPython.display import Image
import matplotlib.pyplot as plt

df = pd.read_csv("./water_potability.csv")

df.head()

np.shape(df)

print("Number of null values in different columns")
print(df.isna().sum())

print("Number of null values in different columns")
print(df.isna().sum())

df=df.dropna()
# df["ph"].fillna(value = df["ph"].mean(),inplace=True)
# df["Sulfate"].fillna(value = df["Sulfate"].mean(),inplace=True)
# df["Trihalomethanes"].fillna(value = df["Trihalomethanes"].mean(),inplace=True)

df.shape

x = pd.DataFrame(df,columns = ['ph', 'Hardness','Solids', 'Chloramines','Sulfate', 'Conductivity','Organic_carbon', 'Trihalomethanes','Turbidity'])
y = pd.DataFrame(df,columns = ['Potability'])

df.describe()

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest  


bestfeatures = SelectKBest(score_func = chi2, k=5)
fit = bestfeatures.fit(x,y)
fit.scores_
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)
featurescore = pd.concat([dfcolumns,dfscores],axis=1)
featurescore.columns = ['feature' , 'relevance']
featurescore.sort_values(by='relevance',ascending=False)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier().fit(x,y)
model.feature_importances_

plt.figure(figsize=(20,10)) 
featuresimp = pd.DataFrame(model.feature_importances_, index = x.columns, columns=['importance'])
featureimp = featuresimp.sort_values(by='importance',ascending=False)
plt.bar(featuresimp.index, featuresimp['importance'])

import seaborn as sns

plt.figure(figsize=(20,10)) 
datacor = df.corr()
sns.heatmap(datacor,annot=True)

df.reset_index(inplace=True)
df=df.drop(['index'],axis=1)
# df=df.drop(['Hardness'],axis=1)
# df=df.drop(['Sulfate'],axis=1)
df.groupby("Potability").count()

y = df['Potability']
X = df.drop(['Potability'],axis=1)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# from sklearn.decomposition import PCA
# pca = PCA(n_components=5) 
# X = pca.fit_transform(X1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)
print('Size Training Set: {}'.format(len(X_train)))
print('Size Testing Set: {}'.format(len(X_test)))
X_test_at_end = X_test[:100]
y_test_at_end = y_test[:100]
X_test = X_test[100:]
y_test = y_test[100:]
print('Size Training Set: {}'.format(len(X_train)))
print('Size Testing Set: {}'.format(len(X_test)))

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from keras.layers import Dense, Dropout, Activation
early_stopping = EarlyStopping(monitor="val_loss",mode = 'auto',patience=15)
#test 1

model = keras.Sequential()
model.add(layers.Dense(256,input_dim=9, activation='relu'))
model.add(Dropout(0.2))
model.add(layers.Dense(128,activation='relu'))
model.add(Dropout(0.2))
model.add(layers.Dense(128,activation='relu'))
model.add(Dropout(0.1))
model.add(layers.Dense(32,activation='relu'))
model.add(Dropout(0.2))
model.add(layers.Dense(1))

opt = keras.optimizers.Adamax()
model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])

history = model.fit(X_train, y_train, batch_size=75, epochs=50, validation_data=(X_test, y_test),callbacks=[early_stopping])
# history = model.fit(X_train, y_train, batch_size=75, epochs=400, validation_data=(X_test, y_test))

results = model.evaluate(X_test, y_test, batch_size=64)
print("test_losses, test accdrng:", results)
results = model.evaluate(X_test_at_end, y_test_at_end, batch_size=64)
print("test_losses, test accdrng on unseen data:", results)


import warnings
import pickle
warnings.filterwarnings("ignore")
#import math


pickle.dump(model,open("model.pkl","wb"))
model=pickle.load(open("model.pkl","rb"))




























# data_to_be_predicted = [[-0.47658809,  0.55487565, -0.84464553, -0.34708111, -0.41820391,-0.69065654, -0.64326531,  1.07434204, -0.23191052]]
# final_result = model.predict(data_to_be_predicted)
# if abs(final_result) < 0.5:
#   print("Not potable")
# else:
#   print("Potable")

# data_to_be_predicted = [[ 1.31924393,-2.33322826, 0.54404732, -0.64929837,0.24431558, -0.57999474,
#  -1.04020633 ,-0.73539734,  1.17711959]]
# final_result = model.predict(data_to_be_predicted)
# if abs(final_result) < 0.5:
#   print("Not potable")
# else:
#   print("Potable")